{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize) #show full matrixes\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from random import randint, choice\n",
    "tf.random.set_seed(7)\n",
    "from statistics import mode\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET ALL FILES IN A FOLDER\n",
    "folder_path = r'C:\\Users\\catar\\OneDrive\\Ambiente de Trabalho\\champalimaud\\finalfiles'\n",
    "\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "csv_files = [file for file in file_list if file.endswith('_normalized.csv') ] #File restrictions\n",
    "\n",
    "#Create the data frame\n",
    "dataframes = []\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET THE COLUMNS WE WANT \n",
    "coordinates = []\n",
    "for data in dataframes:\n",
    "    df = data[['FrameIndex','VisualStim', 'PixelChange', 'Centroid.X', 'Centroid.Y', 'Head.Position.X', 'Head.Position.Y', 'Thorax.Position.X', 'Thorax.Position.Y', 'Abdomen.Position.X','Abdomen.Position.Y', 'LeftWing.Position.X', 'LeftWing.Position.Y', 'RightWing.Position.X', 'RightWing.Position.Y' ]]\n",
    "    coordinates.append(np.nan_to_num(df.values, nan=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous len: 1564\n",
      "36030\n",
      "36029\n",
      "36028\n",
      "36028\n",
      "33024\n",
      "33024\n",
      "33024\n",
      "33024\n",
      "4795\n",
      "4795\n",
      "4795\n",
      "4795\n",
      "33612\n",
      "33612\n",
      "33612\n",
      "33612\n",
      "4911\n",
      "4911\n",
      "4911\n",
      "4911\n",
      "5882\n",
      "5886\n",
      "5895\n",
      "5895\n",
      "afterwards len: 1540\n"
     ]
    }
   ],
   "source": [
    "#DELETE FILES THAT WENT WRONG\n",
    "print(\"previous len:\", len(coordinates))\n",
    "n = 0\n",
    "while n < len(coordinates): \n",
    "    if coordinates[n].shape[0] != 36599: #If the file has a different size than we want it to have (error)\n",
    "        print(coordinates[n].shape[0])\n",
    "        del coordinates[n]\n",
    "        n = n - 1 \n",
    "    n = n + 1\n",
    "    \n",
    "    \n",
    "print(\"afterwards len:\", len(coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TAKE OUT FREEZING \n",
    "#Create a column of 0s and add 1s when freezing \n",
    "#Freezing: when pixel change is 0 for more than 30 frames \n",
    "#Break from freezing: when pixel change is different than 0 for more than 3 frames\n",
    "\n",
    "freezing_files = []  \n",
    "pixel_change = 2 #Column of pixel change\n",
    "n = 0 \n",
    "while n < len(coordinates): \n",
    "    new_column = np.zeros((coordinates[n].shape[0], 1)) #one new column for each file\n",
    "    i = 0\n",
    "    while i < coordinates[n].shape[0]: #i < 36599 \n",
    "        if i + 30 < coordinates[n].shape[0]:\n",
    "            if coordinates[n][i, pixel_change] == 0 and coordinates[n][i + 30, pixel_change] == 0 : \n",
    "                if not np.any(np.convolve(coordinates[n][i : (i + 30), pixel_change] != 0, np.ones(3), mode='valid') == 3): #if theres no intervals where there's more than 3 pixels diferent from 0\n",
    "                    new_column[i : (i+30)] = 1\n",
    "                    i = i + 30\n",
    "                    while i < len(new_column):  \n",
    "                        if (i + 3) < coordinates[n].shape[0]:\n",
    "                            if coordinates[n][i, pixel_change] == 0: \n",
    "                                new_column[i] = 1\n",
    "                            elif coordinates[n][i, pixel_change] != 0 and coordinates[n][i + 1, pixel_change] != 0 and coordinates[n][i + 2, pixel_change] != 0 : #break from freezing\n",
    "                                i = i + 3 \n",
    "                                break\n",
    "                            elif coordinates[n][i, pixel_change] != 0  and coordinates[n][i + 2, pixel_change] == 0 : #no break \n",
    "                                new_column[i] = 1\n",
    "                                \n",
    "                            elif coordinates[n][i, pixel_change] != 0 and coordinates[n][i + 1, pixel_change] == 0 and coordinates[n][i + 2, pixel_change] != 0 : #no break \n",
    "                                new_column[i] = 1\n",
    "                        else:\n",
    "                            break\n",
    "                        \n",
    "                        i = i + 1   \n",
    "                else:\n",
    "                    i = i + 1\n",
    "            else:\n",
    "                i = i + 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "        \n",
    "    freezing_files.append( np.hstack((coordinates[n], new_column)) )   \n",
    "    n = n + 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE A TABLE THAT RECORDS THE ACCURACY SCORES \n",
    "\n",
    "#RUN TO RESET TABLE\n",
    "results_df = pd.DataFrame(columns=['Time_Window','Nº of Looms' ,'Freezing', 'Accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 66, 13)\n",
      "(9216,)\n",
      "(6451, 66, 13)\n",
      "(2765, 66, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_32 (Flatten)        (None, 858)               0         \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 858)               3432      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 256)               219904    \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273002 (1.04 MB)\n",
      "Trainable params: 271286 (1.03 MB)\n",
      "Non-trainable params: 1716 (6.70 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 3s 10ms/step - loss: 0.7250 - accuracy: 0.5336 - val_loss: 0.6964 - val_accuracy: 0.5599\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6942 - accuracy: 0.5401 - val_loss: 0.6888 - val_accuracy: 0.5537\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6893 - accuracy: 0.5501 - val_loss: 0.6886 - val_accuracy: 0.5606\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6886 - accuracy: 0.5567 - val_loss: 0.6899 - val_accuracy: 0.5523\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6875 - accuracy: 0.5537 - val_loss: 0.6845 - val_accuracy: 0.5595\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6882 - accuracy: 0.5534 - val_loss: 0.6856 - val_accuracy: 0.5599\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6861 - accuracy: 0.5539 - val_loss: 0.6865 - val_accuracy: 0.5515\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6863 - accuracy: 0.5599 - val_loss: 0.6872 - val_accuracy: 0.5580\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6830 - accuracy: 0.5588 - val_loss: 0.6835 - val_accuracy: 0.5508\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6849 - accuracy: 0.5587 - val_loss: 0.6792 - val_accuracy: 0.5678\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6834 - accuracy: 0.5644 - val_loss: 0.6811 - val_accuracy: 0.5562\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6773 - accuracy: 0.5732 - val_loss: 0.6849 - val_accuracy: 0.5580\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6778 - accuracy: 0.5687 - val_loss: 0.6782 - val_accuracy: 0.5664\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6733 - accuracy: 0.5711 - val_loss: 0.6770 - val_accuracy: 0.5700\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6745 - accuracy: 0.5762 - val_loss: 0.6796 - val_accuracy: 0.5693\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6725 - accuracy: 0.5726 - val_loss: 0.6762 - val_accuracy: 0.5627\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6722 - accuracy: 0.5875 - val_loss: 0.6779 - val_accuracy: 0.5707\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6691 - accuracy: 0.5787 - val_loss: 0.6783 - val_accuracy: 0.5718\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6700 - accuracy: 0.5762 - val_loss: 0.6778 - val_accuracy: 0.5790\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 2s 7ms/step - loss: 0.6691 - accuracy: 0.5878 - val_loss: 0.6787 - val_accuracy: 0.5631\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6686 - accuracy: 0.5881 - val_loss: 0.6726 - val_accuracy: 0.5765\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6685 - accuracy: 0.5867 - val_loss: 0.6760 - val_accuracy: 0.5750\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6691 - accuracy: 0.5825 - val_loss: 0.6761 - val_accuracy: 0.5787\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6647 - accuracy: 0.5909 - val_loss: 0.6730 - val_accuracy: 0.5769\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6659 - accuracy: 0.5863 - val_loss: 0.6730 - val_accuracy: 0.5805\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6622 - accuracy: 0.5905 - val_loss: 0.6809 - val_accuracy: 0.5703\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6653 - accuracy: 0.5928 - val_loss: 0.6775 - val_accuracy: 0.5624\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6623 - accuracy: 0.5917 - val_loss: 0.6751 - val_accuracy: 0.5664\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6626 - accuracy: 0.5892 - val_loss: 0.6739 - val_accuracy: 0.5779\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6626 - accuracy: 0.5870 - val_loss: 0.6776 - val_accuracy: 0.5743\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.5743218806509945\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 72, 13)\n",
      "(9216,)\n",
      "(6451, 72, 13)\n",
      "(2765, 72, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_33 (Flatten)        (None, 936)               0         \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 936)               3744      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 256)               239872    \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_100 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 293282 (1.12 MB)\n",
      "Trainable params: 291410 (1.11 MB)\n",
      "Non-trainable params: 1872 (7.31 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 3s 10ms/step - loss: 0.7257 - accuracy: 0.5298 - val_loss: 0.6954 - val_accuracy: 0.5552\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6926 - accuracy: 0.5457 - val_loss: 0.6879 - val_accuracy: 0.5570\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6875 - accuracy: 0.5515 - val_loss: 0.6866 - val_accuracy: 0.5638\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6853 - accuracy: 0.5613 - val_loss: 0.6848 - val_accuracy: 0.5638\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6825 - accuracy: 0.5618 - val_loss: 0.6830 - val_accuracy: 0.5627\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6809 - accuracy: 0.5622 - val_loss: 0.6799 - val_accuracy: 0.5703\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6750 - accuracy: 0.5697 - val_loss: 0.6739 - val_accuracy: 0.5656\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6719 - accuracy: 0.5717 - val_loss: 0.6691 - val_accuracy: 0.5591\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6681 - accuracy: 0.5861 - val_loss: 0.6704 - val_accuracy: 0.5787\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6676 - accuracy: 0.5819 - val_loss: 0.6693 - val_accuracy: 0.5776\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6668 - accuracy: 0.5832 - val_loss: 0.6750 - val_accuracy: 0.5714\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6672 - accuracy: 0.5849 - val_loss: 0.6720 - val_accuracy: 0.5805\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6643 - accuracy: 0.5829 - val_loss: 0.6733 - val_accuracy: 0.5797\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6650 - accuracy: 0.5897 - val_loss: 0.6690 - val_accuracy: 0.5722\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6632 - accuracy: 0.5849 - val_loss: 0.6708 - val_accuracy: 0.5769\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6627 - accuracy: 0.5838 - val_loss: 0.6685 - val_accuracy: 0.5722\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6641 - accuracy: 0.5915 - val_loss: 0.6695 - val_accuracy: 0.5761\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6602 - accuracy: 0.5973 - val_loss: 0.6730 - val_accuracy: 0.5772\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6623 - accuracy: 0.5807 - val_loss: 0.6690 - val_accuracy: 0.5805\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6608 - accuracy: 0.5926 - val_loss: 0.6709 - val_accuracy: 0.5703\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6600 - accuracy: 0.5932 - val_loss: 0.6727 - val_accuracy: 0.5819\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6598 - accuracy: 0.5979 - val_loss: 0.6688 - val_accuracy: 0.5794\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6607 - accuracy: 0.5951 - val_loss: 0.6696 - val_accuracy: 0.5747\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6558 - accuracy: 0.5994 - val_loss: 0.6661 - val_accuracy: 0.5750\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6572 - accuracy: 0.5911 - val_loss: 0.6670 - val_accuracy: 0.5732\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6562 - accuracy: 0.5998 - val_loss: 0.6771 - val_accuracy: 0.5732\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6561 - accuracy: 0.6007 - val_loss: 0.6710 - val_accuracy: 0.5747\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6536 - accuracy: 0.5999 - val_loss: 0.6683 - val_accuracy: 0.5732\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6511 - accuracy: 0.5993 - val_loss: 0.6690 - val_accuracy: 0.5790\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6549 - accuracy: 0.5973 - val_loss: 0.6752 - val_accuracy: 0.5772\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.5772151898734177\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 78, 13)\n",
      "(9216,)\n",
      "(6451, 78, 13)\n",
      "(2765, 78, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_34 (Flatten)        (None, 1014)              0         \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 1014)              4056      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 256)               259840    \n",
      "                                                                 \n",
      " dropout_102 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_103 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_104 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 313562 (1.20 MB)\n",
      "Trainable params: 311534 (1.19 MB)\n",
      "Non-trainable params: 2028 (7.92 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 3s 9ms/step - loss: 0.7247 - accuracy: 0.5337 - val_loss: 0.6960 - val_accuracy: 0.5559\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6936 - accuracy: 0.5460 - val_loss: 0.6869 - val_accuracy: 0.5588\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6866 - accuracy: 0.5559 - val_loss: 0.6772 - val_accuracy: 0.5801\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6764 - accuracy: 0.5709 - val_loss: 0.6693 - val_accuracy: 0.5823\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6696 - accuracy: 0.5830 - val_loss: 0.6813 - val_accuracy: 0.5779\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6669 - accuracy: 0.5767 - val_loss: 0.6611 - val_accuracy: 0.5928\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6628 - accuracy: 0.5894 - val_loss: 0.6717 - val_accuracy: 0.5693\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6618 - accuracy: 0.5914 - val_loss: 0.6618 - val_accuracy: 0.5888\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6593 - accuracy: 0.5855 - val_loss: 0.6632 - val_accuracy: 0.5881\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6572 - accuracy: 0.5940 - val_loss: 0.6598 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6579 - accuracy: 0.5948 - val_loss: 0.6700 - val_accuracy: 0.5787\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6559 - accuracy: 0.5908 - val_loss: 0.6596 - val_accuracy: 0.5837\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6530 - accuracy: 0.5982 - val_loss: 0.6629 - val_accuracy: 0.5884\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6523 - accuracy: 0.6024 - val_loss: 0.6600 - val_accuracy: 0.5866\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6544 - accuracy: 0.5900 - val_loss: 0.6618 - val_accuracy: 0.5863\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6523 - accuracy: 0.6013 - val_loss: 0.6651 - val_accuracy: 0.5830\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6498 - accuracy: 0.6075 - val_loss: 0.6579 - val_accuracy: 0.5902\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6485 - accuracy: 0.6129 - val_loss: 0.6604 - val_accuracy: 0.5899\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6517 - accuracy: 0.6044 - val_loss: 0.6609 - val_accuracy: 0.5939\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6483 - accuracy: 0.6058 - val_loss: 0.6655 - val_accuracy: 0.5808\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6482 - accuracy: 0.6063 - val_loss: 0.6663 - val_accuracy: 0.5939\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6468 - accuracy: 0.6029 - val_loss: 0.6647 - val_accuracy: 0.5884\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6479 - accuracy: 0.5988 - val_loss: 0.6633 - val_accuracy: 0.5783\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6430 - accuracy: 0.6114 - val_loss: 0.6640 - val_accuracy: 0.5967\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6430 - accuracy: 0.6087 - val_loss: 0.6680 - val_accuracy: 0.5830\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6432 - accuracy: 0.6039 - val_loss: 0.6728 - val_accuracy: 0.5837\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6426 - accuracy: 0.6070 - val_loss: 0.6783 - val_accuracy: 0.5714\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6429 - accuracy: 0.6112 - val_loss: 0.6641 - val_accuracy: 0.5732\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 2s 9ms/step - loss: 0.6388 - accuracy: 0.6100 - val_loss: 0.6656 - val_accuracy: 0.5837\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6385 - accuracy: 0.6149 - val_loss: 0.6606 - val_accuracy: 0.5808\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.5808318264014467\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 84, 13)\n",
      "(9216,)\n",
      "(6451, 84, 13)\n",
      "(2765, 84, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_35 (Flatten)        (None, 1092)              0         \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 1092)              4368      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 256)               279808    \n",
      "                                                                 \n",
      " dropout_105 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_106 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_107 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 333842 (1.27 MB)\n",
      "Trainable params: 331658 (1.27 MB)\n",
      "Non-trainable params: 2184 (8.53 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 3s 11ms/step - loss: 0.7299 - accuracy: 0.5385 - val_loss: 0.6995 - val_accuracy: 0.5580\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.6916 - accuracy: 0.5528 - val_loss: 0.6686 - val_accuracy: 0.5906\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.6652 - accuracy: 0.5832 - val_loss: 0.6626 - val_accuracy: 0.5769\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6512 - accuracy: 0.6137 - val_loss: 0.6653 - val_accuracy: 0.6069\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6481 - accuracy: 0.6019 - val_loss: 0.6558 - val_accuracy: 0.6033\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6470 - accuracy: 0.6075 - val_loss: 0.6417 - val_accuracy: 0.6177\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6406 - accuracy: 0.6145 - val_loss: 0.6467 - val_accuracy: 0.5993\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 0.6118 - val_loss: 0.6436 - val_accuracy: 0.6000\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6364 - accuracy: 0.6162 - val_loss: 0.6462 - val_accuracy: 0.6054\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6322 - accuracy: 0.6207 - val_loss: 0.6489 - val_accuracy: 0.6033\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6321 - accuracy: 0.6227 - val_loss: 0.6447 - val_accuracy: 0.6101\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6303 - accuracy: 0.6211 - val_loss: 0.6375 - val_accuracy: 0.6188\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6271 - accuracy: 0.6252 - val_loss: 0.6366 - val_accuracy: 0.6213\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6272 - accuracy: 0.6295 - val_loss: 0.6412 - val_accuracy: 0.6108\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6255 - accuracy: 0.6345 - val_loss: 0.6431 - val_accuracy: 0.6076\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6234 - accuracy: 0.6337 - val_loss: 0.6350 - val_accuracy: 0.6076\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6154 - accuracy: 0.6407 - val_loss: 0.6307 - val_accuracy: 0.6217\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6180 - accuracy: 0.6366 - val_loss: 0.6408 - val_accuracy: 0.6174\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6125 - accuracy: 0.6402 - val_loss: 0.6285 - val_accuracy: 0.6235\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6114 - accuracy: 0.6469 - val_loss: 0.6298 - val_accuracy: 0.6199\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6124 - accuracy: 0.6504 - val_loss: 0.6263 - val_accuracy: 0.6260\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6105 - accuracy: 0.6411 - val_loss: 0.6321 - val_accuracy: 0.6203\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6107 - accuracy: 0.6483 - val_loss: 0.6298 - val_accuracy: 0.6213\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6039 - accuracy: 0.6495 - val_loss: 0.6251 - val_accuracy: 0.6297\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6050 - accuracy: 0.6486 - val_loss: 0.6220 - val_accuracy: 0.6289\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5966 - accuracy: 0.6532 - val_loss: 0.6442 - val_accuracy: 0.5993\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6098 - accuracy: 0.6490 - val_loss: 0.6329 - val_accuracy: 0.6166\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6005 - accuracy: 0.6525 - val_loss: 0.6223 - val_accuracy: 0.6297\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5993 - accuracy: 0.6565 - val_loss: 0.6275 - val_accuracy: 0.6188\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.5976 - accuracy: 0.6583 - val_loss: 0.6311 - val_accuracy: 0.6203\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.620253164556962\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 90, 13)\n",
      "(9216,)\n",
      "(6451, 90, 13)\n",
      "(2765, 90, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_36 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_108 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_110 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 4s 11ms/step - loss: 0.7262 - accuracy: 0.5309 - val_loss: 0.6971 - val_accuracy: 0.5642\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6826 - accuracy: 0.5722 - val_loss: 0.6484 - val_accuracy: 0.6083\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6408 - accuracy: 0.6180 - val_loss: 0.6292 - val_accuracy: 0.6282\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6128 - accuracy: 0.6542 - val_loss: 0.6213 - val_accuracy: 0.6731\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5916 - accuracy: 0.6715 - val_loss: 0.5834 - val_accuracy: 0.6723\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5754 - accuracy: 0.6890 - val_loss: 0.5605 - val_accuracy: 0.6919\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5602 - accuracy: 0.7041 - val_loss: 0.5501 - val_accuracy: 0.7099\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5433 - accuracy: 0.7227 - val_loss: 0.5304 - val_accuracy: 0.7230\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5359 - accuracy: 0.7269 - val_loss: 0.5353 - val_accuracy: 0.7222\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5283 - accuracy: 0.7304 - val_loss: 0.5237 - val_accuracy: 0.7371\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.5184 - accuracy: 0.7404 - val_loss: 0.5243 - val_accuracy: 0.7309\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5138 - accuracy: 0.7385 - val_loss: 0.5139 - val_accuracy: 0.7418\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5149 - accuracy: 0.7393 - val_loss: 0.4993 - val_accuracy: 0.7519\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5065 - accuracy: 0.7400 - val_loss: 0.5274 - val_accuracy: 0.7298\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5070 - accuracy: 0.7464 - val_loss: 0.5149 - val_accuracy: 0.7382\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5027 - accuracy: 0.7510 - val_loss: 0.5025 - val_accuracy: 0.7396\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4909 - accuracy: 0.7560 - val_loss: 0.4985 - val_accuracy: 0.7472\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4989 - accuracy: 0.7559 - val_loss: 0.5000 - val_accuracy: 0.7443\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4861 - accuracy: 0.7577 - val_loss: 0.4970 - val_accuracy: 0.7508\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4882 - accuracy: 0.7569 - val_loss: 0.5036 - val_accuracy: 0.7501\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4850 - accuracy: 0.7557 - val_loss: 0.4924 - val_accuracy: 0.7508\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4849 - accuracy: 0.7565 - val_loss: 0.5050 - val_accuracy: 0.7450\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4716 - accuracy: 0.7634 - val_loss: 0.4979 - val_accuracy: 0.7497\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.4723 - accuracy: 0.7597 - val_loss: 0.4935 - val_accuracy: 0.7523\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4767 - accuracy: 0.7605 - val_loss: 0.4861 - val_accuracy: 0.7667\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4670 - accuracy: 0.7684 - val_loss: 0.5169 - val_accuracy: 0.7421\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4724 - accuracy: 0.7664 - val_loss: 0.4936 - val_accuracy: 0.7530\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4665 - accuracy: 0.7670 - val_loss: 0.4851 - val_accuracy: 0.7609\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4645 - accuracy: 0.7653 - val_loss: 0.5032 - val_accuracy: 0.7537\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4668 - accuracy: 0.7638 - val_loss: 0.4976 - val_accuracy: 0.7483\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.7482820976491863\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 96, 13)\n",
      "(9216,)\n",
      "(6451, 96, 13)\n",
      "(2765, 96, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_37 (Flatten)        (None, 1248)              0         \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 1248)              4992      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 256)               319744    \n",
      "                                                                 \n",
      " dropout_111 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_112 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_113 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 374402 (1.43 MB)\n",
      "Trainable params: 371906 (1.42 MB)\n",
      "Non-trainable params: 2496 (9.75 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 4s 12ms/step - loss: 0.7268 - accuracy: 0.5320 - val_loss: 0.6957 - val_accuracy: 0.5635\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6613 - accuracy: 0.5959 - val_loss: 0.6194 - val_accuracy: 0.6333\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.6084 - accuracy: 0.6498 - val_loss: 0.5863 - val_accuracy: 0.6821\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5649 - accuracy: 0.7100 - val_loss: 0.5442 - val_accuracy: 0.7071\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5350 - accuracy: 0.7252 - val_loss: 0.5344 - val_accuracy: 0.7081\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5252 - accuracy: 0.7369 - val_loss: 0.5168 - val_accuracy: 0.7313\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5148 - accuracy: 0.7472 - val_loss: 0.5213 - val_accuracy: 0.7306\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5086 - accuracy: 0.7413 - val_loss: 0.4931 - val_accuracy: 0.7490\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4960 - accuracy: 0.7526 - val_loss: 0.5060 - val_accuracy: 0.7353\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4895 - accuracy: 0.7541 - val_loss: 0.4848 - val_accuracy: 0.7526\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4801 - accuracy: 0.7582 - val_loss: 0.4989 - val_accuracy: 0.7447\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4840 - accuracy: 0.7565 - val_loss: 0.4780 - val_accuracy: 0.7577\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4783 - accuracy: 0.7608 - val_loss: 0.4680 - val_accuracy: 0.7664\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4665 - accuracy: 0.7672 - val_loss: 0.4823 - val_accuracy: 0.7552\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4636 - accuracy: 0.7706 - val_loss: 0.4733 - val_accuracy: 0.7526\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4635 - accuracy: 0.7718 - val_loss: 0.4641 - val_accuracy: 0.7714\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4561 - accuracy: 0.7718 - val_loss: 0.4695 - val_accuracy: 0.7635\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4637 - accuracy: 0.7701 - val_loss: 0.4668 - val_accuracy: 0.7635\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4513 - accuracy: 0.7726 - val_loss: 0.4614 - val_accuracy: 0.7667\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4416 - accuracy: 0.7838 - val_loss: 0.4548 - val_accuracy: 0.7678\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 2s 12ms/step - loss: 0.4440 - accuracy: 0.7763 - val_loss: 0.4566 - val_accuracy: 0.7693\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4436 - accuracy: 0.7805 - val_loss: 0.4583 - val_accuracy: 0.7743\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4353 - accuracy: 0.7842 - val_loss: 0.4488 - val_accuracy: 0.7855\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 2s 12ms/step - loss: 0.4288 - accuracy: 0.7841 - val_loss: 0.4552 - val_accuracy: 0.7758\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4317 - accuracy: 0.7838 - val_loss: 0.4448 - val_accuracy: 0.7844\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 2s 12ms/step - loss: 0.4245 - accuracy: 0.7830 - val_loss: 0.4666 - val_accuracy: 0.7624\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 2s 12ms/step - loss: 0.4318 - accuracy: 0.7827 - val_loss: 0.4548 - val_accuracy: 0.7787\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4186 - accuracy: 0.7914 - val_loss: 0.4645 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4212 - accuracy: 0.7858 - val_loss: 0.4549 - val_accuracy: 0.7671\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4137 - accuracy: 0.7912 - val_loss: 0.4632 - val_accuracy: 0.7700\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.7699819168173598\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 102, 13)\n",
      "(9216,)\n",
      "(6451, 102, 13)\n",
      "(2765, 102, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_38 (Flatten)        (None, 1326)              0         \n",
      "                                                                 \n",
      " batch_normalization_38 (Ba  (None, 1326)              5304      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 256)               339712    \n",
      "                                                                 \n",
      " dropout_114 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_115 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_116 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 394682 (1.51 MB)\n",
      "Trainable params: 392030 (1.50 MB)\n",
      "Non-trainable params: 2652 (10.36 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 4s 13ms/step - loss: 0.7238 - accuracy: 0.5427 - val_loss: 0.6848 - val_accuracy: 0.5848\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.6493 - accuracy: 0.6156 - val_loss: 0.6338 - val_accuracy: 0.6116\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.5817 - accuracy: 0.6810 - val_loss: 0.5904 - val_accuracy: 0.6821\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.5369 - accuracy: 0.7199 - val_loss: 0.5275 - val_accuracy: 0.7193\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.5119 - accuracy: 0.7464 - val_loss: 0.5234 - val_accuracy: 0.7146\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.5068 - accuracy: 0.7461 - val_loss: 0.5040 - val_accuracy: 0.7302\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5000 - accuracy: 0.7484 - val_loss: 0.5045 - val_accuracy: 0.7385\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4846 - accuracy: 0.7577 - val_loss: 0.4789 - val_accuracy: 0.7602\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4741 - accuracy: 0.7642 - val_loss: 0.4867 - val_accuracy: 0.7501\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4733 - accuracy: 0.7641 - val_loss: 0.4754 - val_accuracy: 0.7530\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4527 - accuracy: 0.7701 - val_loss: 0.4726 - val_accuracy: 0.7599\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4599 - accuracy: 0.7683 - val_loss: 0.4601 - val_accuracy: 0.7649\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4544 - accuracy: 0.7700 - val_loss: 0.4521 - val_accuracy: 0.7740\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.4430 - accuracy: 0.7794 - val_loss: 0.4569 - val_accuracy: 0.7664\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4411 - accuracy: 0.7870 - val_loss: 0.4543 - val_accuracy: 0.7783\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4414 - accuracy: 0.7814 - val_loss: 0.4487 - val_accuracy: 0.7732\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4320 - accuracy: 0.7920 - val_loss: 0.4590 - val_accuracy: 0.7667\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4369 - accuracy: 0.7882 - val_loss: 0.4493 - val_accuracy: 0.7758\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4273 - accuracy: 0.7892 - val_loss: 0.4431 - val_accuracy: 0.7978\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4225 - accuracy: 0.7927 - val_loss: 0.4378 - val_accuracy: 0.7779\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4199 - accuracy: 0.7926 - val_loss: 0.4308 - val_accuracy: 0.7877\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4188 - accuracy: 0.7951 - val_loss: 0.4453 - val_accuracy: 0.7844\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4163 - accuracy: 0.7931 - val_loss: 0.4478 - val_accuracy: 0.7870\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4103 - accuracy: 0.7965 - val_loss: 0.4477 - val_accuracy: 0.7826\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4080 - accuracy: 0.7980 - val_loss: 0.4394 - val_accuracy: 0.7906\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4066 - accuracy: 0.7955 - val_loss: 0.4460 - val_accuracy: 0.7823\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4034 - accuracy: 0.8045 - val_loss: 0.4429 - val_accuracy: 0.7787\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4039 - accuracy: 0.8014 - val_loss: 0.4377 - val_accuracy: 0.7841\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4044 - accuracy: 0.7948 - val_loss: 0.4571 - val_accuracy: 0.7729\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.3997 - accuracy: 0.7976 - val_loss: 0.4356 - val_accuracy: 0.7939\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.7938517179023508\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 108, 13)\n",
      "(9216,)\n",
      "(6451, 108, 13)\n",
      "(2765, 108, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_39 (Flatten)        (None, 1404)              0         \n",
      "                                                                 \n",
      " batch_normalization_39 (Ba  (None, 1404)              5616      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 256)               359680    \n",
      "                                                                 \n",
      " dropout_117 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_118 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_119 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 414962 (1.58 MB)\n",
      "Trainable params: 412154 (1.57 MB)\n",
      "Non-trainable params: 2808 (10.97 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 16ms/step - loss: 0.7253 - accuracy: 0.5416 - val_loss: 0.6789 - val_accuracy: 0.5848\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.6411 - accuracy: 0.6252 - val_loss: 0.6011 - val_accuracy: 0.6604\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.5593 - accuracy: 0.7069 - val_loss: 0.5434 - val_accuracy: 0.7186\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.5252 - accuracy: 0.7286 - val_loss: 0.5542 - val_accuracy: 0.7071\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.5056 - accuracy: 0.7436 - val_loss: 0.4991 - val_accuracy: 0.7421\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4993 - accuracy: 0.7490 - val_loss: 0.5058 - val_accuracy: 0.7277\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4890 - accuracy: 0.7566 - val_loss: 0.5220 - val_accuracy: 0.7439\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4770 - accuracy: 0.7608 - val_loss: 0.4870 - val_accuracy: 0.7526\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4698 - accuracy: 0.7661 - val_loss: 0.4841 - val_accuracy: 0.7664\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4631 - accuracy: 0.7701 - val_loss: 0.4715 - val_accuracy: 0.7649\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4509 - accuracy: 0.7783 - val_loss: 0.4731 - val_accuracy: 0.7664\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4580 - accuracy: 0.7755 - val_loss: 0.4672 - val_accuracy: 0.7642\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4546 - accuracy: 0.7780 - val_loss: 0.4445 - val_accuracy: 0.7783\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4350 - accuracy: 0.7794 - val_loss: 0.4515 - val_accuracy: 0.7758\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4360 - accuracy: 0.7796 - val_loss: 0.4494 - val_accuracy: 0.7787\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4398 - accuracy: 0.7842 - val_loss: 0.4643 - val_accuracy: 0.7693\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4322 - accuracy: 0.7904 - val_loss: 0.4508 - val_accuracy: 0.7722\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4296 - accuracy: 0.7878 - val_loss: 0.4497 - val_accuracy: 0.7808\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4269 - accuracy: 0.7825 - val_loss: 0.4469 - val_accuracy: 0.7881\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4231 - accuracy: 0.7859 - val_loss: 0.4375 - val_accuracy: 0.7783\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4138 - accuracy: 0.7926 - val_loss: 0.4367 - val_accuracy: 0.7877\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4189 - accuracy: 0.7937 - val_loss: 0.4490 - val_accuracy: 0.7765\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4118 - accuracy: 0.7948 - val_loss: 0.4490 - val_accuracy: 0.7866\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4135 - accuracy: 0.7929 - val_loss: 0.4455 - val_accuracy: 0.7863\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4098 - accuracy: 0.7980 - val_loss: 0.4407 - val_accuracy: 0.7859\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.3994 - accuracy: 0.7993 - val_loss: 0.4452 - val_accuracy: 0.7808\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4027 - accuracy: 0.7940 - val_loss: 0.4288 - val_accuracy: 0.7816\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4007 - accuracy: 0.8016 - val_loss: 0.4309 - val_accuracy: 0.7863\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4009 - accuracy: 0.7982 - val_loss: 0.4411 - val_accuracy: 0.7801\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4005 - accuracy: 0.7962 - val_loss: 0.4365 - val_accuracy: 0.7913\n",
      "87/87 [==============================] - 0s 4ms/step\n",
      "accuracy 0.7913200723327306\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 114, 13)\n",
      "(9216,)\n",
      "(6451, 114, 13)\n",
      "(2765, 114, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_40 (Flatten)        (None, 1482)              0         \n",
      "                                                                 \n",
      " batch_normalization_40 (Ba  (None, 1482)              5928      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 256)               379648    \n",
      "                                                                 \n",
      " dropout_120 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_121 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_122 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 435242 (1.66 MB)\n",
      "Trainable params: 432278 (1.65 MB)\n",
      "Non-trainable params: 2964 (11.58 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 17ms/step - loss: 0.7238 - accuracy: 0.5503 - val_loss: 0.6845 - val_accuracy: 0.5450\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.6420 - accuracy: 0.6290 - val_loss: 0.5900 - val_accuracy: 0.6629\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.5571 - accuracy: 0.7052 - val_loss: 0.5473 - val_accuracy: 0.7146\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.5217 - accuracy: 0.7242 - val_loss: 0.5742 - val_accuracy: 0.7002\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5031 - accuracy: 0.7450 - val_loss: 0.5232 - val_accuracy: 0.7143\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4969 - accuracy: 0.7450 - val_loss: 0.4986 - val_accuracy: 0.7450\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4783 - accuracy: 0.7673 - val_loss: 0.4913 - val_accuracy: 0.7490\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4762 - accuracy: 0.7645 - val_loss: 0.4673 - val_accuracy: 0.7736\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4631 - accuracy: 0.7704 - val_loss: 0.4580 - val_accuracy: 0.7725\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4558 - accuracy: 0.7729 - val_loss: 0.4691 - val_accuracy: 0.7573\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4530 - accuracy: 0.7737 - val_loss: 0.4699 - val_accuracy: 0.7627\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4547 - accuracy: 0.7749 - val_loss: 0.4670 - val_accuracy: 0.7617\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4463 - accuracy: 0.7774 - val_loss: 0.4465 - val_accuracy: 0.7736\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4346 - accuracy: 0.7802 - val_loss: 0.4480 - val_accuracy: 0.7750\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4312 - accuracy: 0.7924 - val_loss: 0.4439 - val_accuracy: 0.7783\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4345 - accuracy: 0.7807 - val_loss: 0.4490 - val_accuracy: 0.7718\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4236 - accuracy: 0.7914 - val_loss: 0.4388 - val_accuracy: 0.7819\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4266 - accuracy: 0.7838 - val_loss: 0.4423 - val_accuracy: 0.7884\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4214 - accuracy: 0.7869 - val_loss: 0.4436 - val_accuracy: 0.7859\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4204 - accuracy: 0.7946 - val_loss: 0.4346 - val_accuracy: 0.7754\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4110 - accuracy: 0.7931 - val_loss: 0.4379 - val_accuracy: 0.7888\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4189 - accuracy: 0.7976 - val_loss: 0.4398 - val_accuracy: 0.7783\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4116 - accuracy: 0.7989 - val_loss: 0.4292 - val_accuracy: 0.7935\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4059 - accuracy: 0.7985 - val_loss: 0.4450 - val_accuracy: 0.7834\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4058 - accuracy: 0.7991 - val_loss: 0.4325 - val_accuracy: 0.7863\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4012 - accuracy: 0.7988 - val_loss: 0.4442 - val_accuracy: 0.7859\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.3982 - accuracy: 0.7996 - val_loss: 0.4269 - val_accuracy: 0.7892\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.3912 - accuracy: 0.8036 - val_loss: 0.4522 - val_accuracy: 0.7866\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4020 - accuracy: 0.7979 - val_loss: 0.4443 - val_accuracy: 0.7808\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.3958 - accuracy: 0.8002 - val_loss: 0.4438 - val_accuracy: 0.7797\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.779746835443038\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 120, 13)\n",
      "(9216,)\n",
      "(6451, 120, 13)\n",
      "(2765, 120, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_41 (Flatten)        (None, 1560)              0         \n",
      "                                                                 \n",
      " batch_normalization_41 (Ba  (None, 1560)              6240      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 256)               399616    \n",
      "                                                                 \n",
      " dropout_123 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_124 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_125 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 455522 (1.74 MB)\n",
      "Trainable params: 452402 (1.73 MB)\n",
      "Non-trainable params: 3120 (12.19 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 15ms/step - loss: 0.7204 - accuracy: 0.5565 - val_loss: 0.6532 - val_accuracy: 0.5960\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.6140 - accuracy: 0.6638 - val_loss: 0.5807 - val_accuracy: 0.6600\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.5425 - accuracy: 0.7162 - val_loss: 0.5497 - val_accuracy: 0.7110\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.5183 - accuracy: 0.7292 - val_loss: 0.5415 - val_accuracy: 0.7197\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.5038 - accuracy: 0.7472 - val_loss: 0.5054 - val_accuracy: 0.7233\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4938 - accuracy: 0.7528 - val_loss: 0.4858 - val_accuracy: 0.7458\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4782 - accuracy: 0.7557 - val_loss: 0.5012 - val_accuracy: 0.7494\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4704 - accuracy: 0.7594 - val_loss: 0.4546 - val_accuracy: 0.7783\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4650 - accuracy: 0.7695 - val_loss: 0.4708 - val_accuracy: 0.7548\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4547 - accuracy: 0.7743 - val_loss: 0.4712 - val_accuracy: 0.7609\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4532 - accuracy: 0.7724 - val_loss: 0.4677 - val_accuracy: 0.7667\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4540 - accuracy: 0.7741 - val_loss: 0.4543 - val_accuracy: 0.7678\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4477 - accuracy: 0.7766 - val_loss: 0.4517 - val_accuracy: 0.7646\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4366 - accuracy: 0.7855 - val_loss: 0.4408 - val_accuracy: 0.7844\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4355 - accuracy: 0.7859 - val_loss: 0.4511 - val_accuracy: 0.7740\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4341 - accuracy: 0.7825 - val_loss: 0.4434 - val_accuracy: 0.7765\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4237 - accuracy: 0.7929 - val_loss: 0.4407 - val_accuracy: 0.7787\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4318 - accuracy: 0.7876 - val_loss: 0.4304 - val_accuracy: 0.7859\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4259 - accuracy: 0.7876 - val_loss: 0.4357 - val_accuracy: 0.7913\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4197 - accuracy: 0.7918 - val_loss: 0.4388 - val_accuracy: 0.7736\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4149 - accuracy: 0.7881 - val_loss: 0.4379 - val_accuracy: 0.7816\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4150 - accuracy: 0.7958 - val_loss: 0.4299 - val_accuracy: 0.7899\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4141 - accuracy: 0.7989 - val_loss: 0.4371 - val_accuracy: 0.7902\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4097 - accuracy: 0.7983 - val_loss: 0.4385 - val_accuracy: 0.7837\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4055 - accuracy: 0.8014 - val_loss: 0.4374 - val_accuracy: 0.7819\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4027 - accuracy: 0.7983 - val_loss: 0.4475 - val_accuracy: 0.7837\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.4026 - accuracy: 0.7971 - val_loss: 0.4277 - val_accuracy: 0.7859\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.3932 - accuracy: 0.8025 - val_loss: 0.4381 - val_accuracy: 0.7841\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 3s 14ms/step - loss: 0.3967 - accuracy: 0.8008 - val_loss: 0.4425 - val_accuracy: 0.7787\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4008 - accuracy: 0.7948 - val_loss: 0.4346 - val_accuracy: 0.7859\n",
      "87/87 [==============================] - 0s 4ms/step\n",
      "accuracy 0.7858951175406872\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 126, 13)\n",
      "(9216,)\n",
      "(6451, 126, 13)\n",
      "(2765, 126, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_42 (Flatten)        (None, 1638)              0         \n",
      "                                                                 \n",
      " batch_normalization_42 (Ba  (None, 1638)              6552      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 256)               419584    \n",
      "                                                                 \n",
      " dropout_126 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_127 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_128 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 475802 (1.82 MB)\n",
      "Trainable params: 472526 (1.80 MB)\n",
      "Non-trainable params: 3276 (12.80 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 16ms/step - loss: 0.7244 - accuracy: 0.5517 - val_loss: 0.6369 - val_accuracy: 0.6452\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.6054 - accuracy: 0.6663 - val_loss: 0.5646 - val_accuracy: 0.6741\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.5361 - accuracy: 0.7224 - val_loss: 0.5160 - val_accuracy: 0.7367\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.5177 - accuracy: 0.7301 - val_loss: 0.5256 - val_accuracy: 0.7186\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4980 - accuracy: 0.7478 - val_loss: 0.4798 - val_accuracy: 0.7324\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4865 - accuracy: 0.7563 - val_loss: 0.4961 - val_accuracy: 0.7396\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4793 - accuracy: 0.7611 - val_loss: 0.4972 - val_accuracy: 0.7490\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4735 - accuracy: 0.7642 - val_loss: 0.4633 - val_accuracy: 0.7526\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.4627 - accuracy: 0.7661 - val_loss: 0.4665 - val_accuracy: 0.7667\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.4541 - accuracy: 0.7746 - val_loss: 0.4620 - val_accuracy: 0.7671\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4505 - accuracy: 0.7754 - val_loss: 0.4667 - val_accuracy: 0.7722\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4475 - accuracy: 0.7727 - val_loss: 0.4482 - val_accuracy: 0.7797\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4467 - accuracy: 0.7758 - val_loss: 0.4462 - val_accuracy: 0.7855\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4373 - accuracy: 0.7825 - val_loss: 0.4375 - val_accuracy: 0.7801\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4349 - accuracy: 0.7855 - val_loss: 0.4353 - val_accuracy: 0.7852\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4347 - accuracy: 0.7870 - val_loss: 0.4463 - val_accuracy: 0.7743\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4229 - accuracy: 0.7907 - val_loss: 0.4532 - val_accuracy: 0.7696\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4316 - accuracy: 0.7850 - val_loss: 0.4356 - val_accuracy: 0.7841\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4234 - accuracy: 0.7881 - val_loss: 0.4351 - val_accuracy: 0.7924\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4192 - accuracy: 0.7889 - val_loss: 0.4293 - val_accuracy: 0.7906\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.4147 - accuracy: 0.7906 - val_loss: 0.4416 - val_accuracy: 0.7931\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4228 - accuracy: 0.7917 - val_loss: 0.4312 - val_accuracy: 0.7892\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4081 - accuracy: 0.7965 - val_loss: 0.4340 - val_accuracy: 0.7906\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4045 - accuracy: 0.7974 - val_loss: 0.4463 - val_accuracy: 0.7841\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4064 - accuracy: 0.7954 - val_loss: 0.4319 - val_accuracy: 0.7873\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.3959 - accuracy: 0.8008 - val_loss: 0.4444 - val_accuracy: 0.7837\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4004 - accuracy: 0.7988 - val_loss: 0.4262 - val_accuracy: 0.7906\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3939 - accuracy: 0.7988 - val_loss: 0.4389 - val_accuracy: 0.7892\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.3927 - accuracy: 0.7997 - val_loss: 0.4513 - val_accuracy: 0.7794\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.3953 - accuracy: 0.7974 - val_loss: 0.4337 - val_accuracy: 0.7790\n",
      "87/87 [==============================] - 0s 4ms/step\n",
      "accuracy 0.7790235081374322\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 132, 13)\n",
      "(9216,)\n",
      "(6451, 132, 13)\n",
      "(2765, 132, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_43 (Flatten)        (None, 1716)              0         \n",
      "                                                                 \n",
      " batch_normalization_43 (Ba  (None, 1716)              6864      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 256)               439552    \n",
      "                                                                 \n",
      " dropout_129 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_130 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_131 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_175 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 496082 (1.89 MB)\n",
      "Trainable params: 492650 (1.88 MB)\n",
      "Non-trainable params: 3432 (13.41 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 17ms/step - loss: 0.7166 - accuracy: 0.5644 - val_loss: 0.6195 - val_accuracy: 0.6492\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.6101 - accuracy: 0.6641 - val_loss: 0.5748 - val_accuracy: 0.6676\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.5420 - accuracy: 0.7179 - val_loss: 0.5250 - val_accuracy: 0.7306\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5138 - accuracy: 0.7335 - val_loss: 0.5245 - val_accuracy: 0.7338\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4990 - accuracy: 0.7478 - val_loss: 0.4884 - val_accuracy: 0.7385\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4871 - accuracy: 0.7546 - val_loss: 0.4928 - val_accuracy: 0.7382\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4793 - accuracy: 0.7611 - val_loss: 0.4859 - val_accuracy: 0.7559\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4665 - accuracy: 0.7625 - val_loss: 0.4619 - val_accuracy: 0.7703\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4619 - accuracy: 0.7745 - val_loss: 0.4702 - val_accuracy: 0.7631\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4527 - accuracy: 0.7735 - val_loss: 0.4622 - val_accuracy: 0.7606\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4427 - accuracy: 0.7799 - val_loss: 0.4599 - val_accuracy: 0.7707\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4501 - accuracy: 0.7752 - val_loss: 0.4533 - val_accuracy: 0.7779\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4460 - accuracy: 0.7813 - val_loss: 0.4431 - val_accuracy: 0.7816\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4333 - accuracy: 0.7831 - val_loss: 0.4388 - val_accuracy: 0.7844\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4282 - accuracy: 0.7872 - val_loss: 0.4313 - val_accuracy: 0.7805\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4366 - accuracy: 0.7839 - val_loss: 0.4574 - val_accuracy: 0.7671\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4272 - accuracy: 0.7966 - val_loss: 0.4342 - val_accuracy: 0.7808\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4249 - accuracy: 0.7912 - val_loss: 0.4335 - val_accuracy: 0.7812\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4226 - accuracy: 0.7884 - val_loss: 0.4325 - val_accuracy: 0.7971\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4150 - accuracy: 0.7949 - val_loss: 0.4301 - val_accuracy: 0.7812\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4132 - accuracy: 0.7935 - val_loss: 0.4449 - val_accuracy: 0.7859\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4135 - accuracy: 0.8010 - val_loss: 0.4298 - val_accuracy: 0.7837\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4071 - accuracy: 0.7979 - val_loss: 0.4255 - val_accuracy: 0.7913\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4064 - accuracy: 0.7979 - val_loss: 0.4464 - val_accuracy: 0.7859\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4024 - accuracy: 0.8056 - val_loss: 0.4278 - val_accuracy: 0.7913\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3966 - accuracy: 0.8038 - val_loss: 0.4538 - val_accuracy: 0.7779\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3959 - accuracy: 0.8019 - val_loss: 0.4271 - val_accuracy: 0.7902\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3899 - accuracy: 0.8058 - val_loss: 0.4555 - val_accuracy: 0.7844\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3916 - accuracy: 0.8070 - val_loss: 0.4404 - val_accuracy: 0.7823\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3997 - accuracy: 0.7951 - val_loss: 0.4306 - val_accuracy: 0.7931\n",
      "87/87 [==============================] - 1s 4ms/step\n",
      "accuracy 0.793128390596745\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 138, 13)\n",
      "(9216,)\n",
      "(6451, 138, 13)\n",
      "(2765, 138, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_44 (Flatten)        (None, 1794)              0         \n",
      "                                                                 \n",
      " batch_normalization_44 (Ba  (None, 1794)              7176      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_176 (Dense)           (None, 256)               459520    \n",
      "                                                                 \n",
      " dropout_132 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_133 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_134 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 516362 (1.97 MB)\n",
      "Trainable params: 512774 (1.96 MB)\n",
      "Non-trainable params: 3588 (14.02 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 16ms/step - loss: 0.7194 - accuracy: 0.5705 - val_loss: 0.6231 - val_accuracy: 0.6521\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.6160 - accuracy: 0.6515 - val_loss: 0.5695 - val_accuracy: 0.6807\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.5491 - accuracy: 0.7070 - val_loss: 0.5231 - val_accuracy: 0.7212\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.5093 - accuracy: 0.7357 - val_loss: 0.5502 - val_accuracy: 0.7215\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4967 - accuracy: 0.7400 - val_loss: 0.4946 - val_accuracy: 0.7338\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4890 - accuracy: 0.7557 - val_loss: 0.4877 - val_accuracy: 0.7468\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.4816 - accuracy: 0.7644 - val_loss: 0.4917 - val_accuracy: 0.7512\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4705 - accuracy: 0.7614 - val_loss: 0.4584 - val_accuracy: 0.7830\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.4621 - accuracy: 0.7701 - val_loss: 0.4737 - val_accuracy: 0.7559\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4496 - accuracy: 0.7788 - val_loss: 0.4631 - val_accuracy: 0.7620\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4475 - accuracy: 0.7751 - val_loss: 0.4599 - val_accuracy: 0.7725\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4490 - accuracy: 0.7813 - val_loss: 0.4559 - val_accuracy: 0.7671\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4493 - accuracy: 0.7749 - val_loss: 0.4481 - val_accuracy: 0.7794\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4384 - accuracy: 0.7810 - val_loss: 0.4400 - val_accuracy: 0.7808\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 4s 16ms/step - loss: 0.4288 - accuracy: 0.7856 - val_loss: 0.4369 - val_accuracy: 0.7819\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4420 - accuracy: 0.7813 - val_loss: 0.4454 - val_accuracy: 0.7801\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4258 - accuracy: 0.7890 - val_loss: 0.4522 - val_accuracy: 0.7754\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4288 - accuracy: 0.7906 - val_loss: 0.4505 - val_accuracy: 0.7830\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4256 - accuracy: 0.7873 - val_loss: 0.4453 - val_accuracy: 0.7964\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4220 - accuracy: 0.7876 - val_loss: 0.4286 - val_accuracy: 0.7863\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4120 - accuracy: 0.7943 - val_loss: 0.4385 - val_accuracy: 0.7928\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4187 - accuracy: 0.7972 - val_loss: 0.4311 - val_accuracy: 0.7859\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4104 - accuracy: 0.7955 - val_loss: 0.4396 - val_accuracy: 0.7826\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4105 - accuracy: 0.8002 - val_loss: 0.4432 - val_accuracy: 0.7826\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 15ms/step - loss: 0.4020 - accuracy: 0.8034 - val_loss: 0.4385 - val_accuracy: 0.7902\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4001 - accuracy: 0.8003 - val_loss: 0.4439 - val_accuracy: 0.7790\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.3949 - accuracy: 0.7994 - val_loss: 0.4323 - val_accuracy: 0.7884\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3924 - accuracy: 0.8065 - val_loss: 0.4401 - val_accuracy: 0.7808\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.3985 - accuracy: 0.7985 - val_loss: 0.4356 - val_accuracy: 0.7852\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4025 - accuracy: 0.7954 - val_loss: 0.4371 - val_accuracy: 0.7732\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.7732368896925859\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 144, 13)\n",
      "(9216,)\n",
      "(6451, 144, 13)\n",
      "(2765, 144, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_45 (Flatten)        (None, 1872)              0         \n",
      "                                                                 \n",
      " batch_normalization_45 (Ba  (None, 1872)              7488      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_180 (Dense)           (None, 256)               479488    \n",
      "                                                                 \n",
      " dropout_135 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_181 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_136 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_137 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 536642 (2.05 MB)\n",
      "Trainable params: 532898 (2.03 MB)\n",
      "Non-trainable params: 3744 (14.62 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 17ms/step - loss: 0.7126 - accuracy: 0.5805 - val_loss: 0.6171 - val_accuracy: 0.6510\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.6011 - accuracy: 0.6804 - val_loss: 0.5798 - val_accuracy: 0.6524\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.5380 - accuracy: 0.7159 - val_loss: 0.5286 - val_accuracy: 0.7125\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.5117 - accuracy: 0.7410 - val_loss: 0.5385 - val_accuracy: 0.7204\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4924 - accuracy: 0.7486 - val_loss: 0.5268 - val_accuracy: 0.7143\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4882 - accuracy: 0.7546 - val_loss: 0.4808 - val_accuracy: 0.7400\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4744 - accuracy: 0.7669 - val_loss: 0.5004 - val_accuracy: 0.7552\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4674 - accuracy: 0.7676 - val_loss: 0.4657 - val_accuracy: 0.7758\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4616 - accuracy: 0.7731 - val_loss: 0.4535 - val_accuracy: 0.7729\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4485 - accuracy: 0.7752 - val_loss: 0.4545 - val_accuracy: 0.7754\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4451 - accuracy: 0.7785 - val_loss: 0.4626 - val_accuracy: 0.7700\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4465 - accuracy: 0.7786 - val_loss: 0.4473 - val_accuracy: 0.7794\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4427 - accuracy: 0.7780 - val_loss: 0.4485 - val_accuracy: 0.7797\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4363 - accuracy: 0.7847 - val_loss: 0.4398 - val_accuracy: 0.7830\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4289 - accuracy: 0.7886 - val_loss: 0.4319 - val_accuracy: 0.7852\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4360 - accuracy: 0.7839 - val_loss: 0.4367 - val_accuracy: 0.7732\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4246 - accuracy: 0.7934 - val_loss: 0.4357 - val_accuracy: 0.7808\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4291 - accuracy: 0.7895 - val_loss: 0.4478 - val_accuracy: 0.7819\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4234 - accuracy: 0.7862 - val_loss: 0.4340 - val_accuracy: 0.7855\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4195 - accuracy: 0.7920 - val_loss: 0.4303 - val_accuracy: 0.7816\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4141 - accuracy: 0.7934 - val_loss: 0.4383 - val_accuracy: 0.7895\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4169 - accuracy: 0.7963 - val_loss: 0.4290 - val_accuracy: 0.7895\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4050 - accuracy: 0.7994 - val_loss: 0.4373 - val_accuracy: 0.7848\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3979 - accuracy: 0.8008 - val_loss: 0.4474 - val_accuracy: 0.7841\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4000 - accuracy: 0.8030 - val_loss: 0.4297 - val_accuracy: 0.7928\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3974 - accuracy: 0.7996 - val_loss: 0.4406 - val_accuracy: 0.7808\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3980 - accuracy: 0.7980 - val_loss: 0.4318 - val_accuracy: 0.7877\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3940 - accuracy: 0.8053 - val_loss: 0.4516 - val_accuracy: 0.7852\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3923 - accuracy: 0.7999 - val_loss: 0.4403 - val_accuracy: 0.7830\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3990 - accuracy: 0.8003 - val_loss: 0.4384 - val_accuracy: 0.7870\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.7869801084990958\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 150, 13)\n",
      "(9216,)\n",
      "(6451, 150, 13)\n",
      "(2765, 150, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_46 (Flatten)        (None, 1950)              0         \n",
      "                                                                 \n",
      " batch_normalization_46 (Ba  (None, 1950)              7800      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 256)               499456    \n",
      "                                                                 \n",
      " dropout_138 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_185 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_139 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_186 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_140 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 556922 (2.12 MB)\n",
      "Trainable params: 553022 (2.11 MB)\n",
      "Non-trainable params: 3900 (15.23 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 6s 22ms/step - loss: 0.7194 - accuracy: 0.5622 - val_loss: 0.6397 - val_accuracy: 0.6022\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.6071 - accuracy: 0.6627 - val_loss: 0.5593 - val_accuracy: 0.6948\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5450 - accuracy: 0.7140 - val_loss: 0.5350 - val_accuracy: 0.7052\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.5144 - accuracy: 0.7309 - val_loss: 0.4974 - val_accuracy: 0.7461\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4931 - accuracy: 0.7456 - val_loss: 0.5308 - val_accuracy: 0.7161\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4892 - accuracy: 0.7532 - val_loss: 0.4782 - val_accuracy: 0.7559\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4756 - accuracy: 0.7631 - val_loss: 0.4868 - val_accuracy: 0.7541\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4717 - accuracy: 0.7661 - val_loss: 0.4536 - val_accuracy: 0.7750\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4615 - accuracy: 0.7676 - val_loss: 0.4519 - val_accuracy: 0.7754\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4546 - accuracy: 0.7746 - val_loss: 0.4633 - val_accuracy: 0.7631\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4445 - accuracy: 0.7754 - val_loss: 0.4560 - val_accuracy: 0.7714\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4477 - accuracy: 0.7755 - val_loss: 0.4512 - val_accuracy: 0.7747\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4457 - accuracy: 0.7740 - val_loss: 0.4532 - val_accuracy: 0.7801\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4351 - accuracy: 0.7819 - val_loss: 0.4440 - val_accuracy: 0.7823\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4297 - accuracy: 0.7862 - val_loss: 0.4341 - val_accuracy: 0.7852\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4350 - accuracy: 0.7865 - val_loss: 0.4428 - val_accuracy: 0.7779\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4244 - accuracy: 0.7949 - val_loss: 0.4359 - val_accuracy: 0.7826\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4267 - accuracy: 0.7924 - val_loss: 0.4418 - val_accuracy: 0.7794\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4214 - accuracy: 0.7900 - val_loss: 0.4402 - val_accuracy: 0.7899\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4152 - accuracy: 0.7960 - val_loss: 0.4307 - val_accuracy: 0.7888\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4092 - accuracy: 0.7962 - val_loss: 0.4345 - val_accuracy: 0.7892\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4144 - accuracy: 0.7949 - val_loss: 0.4302 - val_accuracy: 0.7892\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4127 - accuracy: 0.7962 - val_loss: 0.4355 - val_accuracy: 0.7902\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.4041 - accuracy: 0.7991 - val_loss: 0.4392 - val_accuracy: 0.7910\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4019 - accuracy: 0.8003 - val_loss: 0.4401 - val_accuracy: 0.7888\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3955 - accuracy: 0.8019 - val_loss: 0.4462 - val_accuracy: 0.7761\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3975 - accuracy: 0.7972 - val_loss: 0.4297 - val_accuracy: 0.7841\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3969 - accuracy: 0.8034 - val_loss: 0.4537 - val_accuracy: 0.7899\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 17ms/step - loss: 0.3976 - accuracy: 0.7969 - val_loss: 0.4324 - val_accuracy: 0.7834\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3956 - accuracy: 0.7969 - val_loss: 0.4270 - val_accuracy: 0.7935\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "accuracy 0.7934900542495479\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 156, 13)\n",
      "(9216,)\n",
      "(6451, 156, 13)\n",
      "(2765, 156, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_47 (Flatten)        (None, 2028)              0         \n",
      "                                                                 \n",
      " batch_normalization_47 (Ba  (None, 2028)              8112      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 256)               519424    \n",
      "                                                                 \n",
      " dropout_141 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_142 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_143 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 577202 (2.20 MB)\n",
      "Trainable params: 573146 (2.19 MB)\n",
      "Non-trainable params: 4056 (15.84 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 6s 21ms/step - loss: 0.7134 - accuracy: 0.5691 - val_loss: 0.6130 - val_accuracy: 0.6611\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.6135 - accuracy: 0.6552 - val_loss: 0.5874 - val_accuracy: 0.6597\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5641 - accuracy: 0.6999 - val_loss: 0.5503 - val_accuracy: 0.6951\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.5291 - accuracy: 0.7219 - val_loss: 0.5224 - val_accuracy: 0.7280\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5015 - accuracy: 0.7414 - val_loss: 0.4900 - val_accuracy: 0.7363\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4943 - accuracy: 0.7467 - val_loss: 0.4845 - val_accuracy: 0.7476\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4767 - accuracy: 0.7613 - val_loss: 0.5051 - val_accuracy: 0.7421\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4682 - accuracy: 0.7652 - val_loss: 0.4685 - val_accuracy: 0.7743\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4641 - accuracy: 0.7690 - val_loss: 0.4852 - val_accuracy: 0.7638\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4583 - accuracy: 0.7727 - val_loss: 0.4627 - val_accuracy: 0.7602\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4452 - accuracy: 0.7789 - val_loss: 0.4598 - val_accuracy: 0.7743\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4519 - accuracy: 0.7752 - val_loss: 0.4544 - val_accuracy: 0.7732\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4433 - accuracy: 0.7800 - val_loss: 0.4444 - val_accuracy: 0.7826\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4312 - accuracy: 0.7825 - val_loss: 0.4514 - val_accuracy: 0.7667\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4262 - accuracy: 0.7859 - val_loss: 0.4356 - val_accuracy: 0.7870\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4369 - accuracy: 0.7853 - val_loss: 0.4420 - val_accuracy: 0.7841\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4266 - accuracy: 0.7881 - val_loss: 0.4528 - val_accuracy: 0.7613\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4309 - accuracy: 0.7870 - val_loss: 0.4423 - val_accuracy: 0.7855\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4229 - accuracy: 0.7853 - val_loss: 0.4355 - val_accuracy: 0.7902\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4191 - accuracy: 0.7910 - val_loss: 0.4514 - val_accuracy: 0.7667\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4144 - accuracy: 0.7957 - val_loss: 0.4332 - val_accuracy: 0.7942\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4155 - accuracy: 0.7985 - val_loss: 0.4314 - val_accuracy: 0.7816\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4088 - accuracy: 0.7991 - val_loss: 0.4374 - val_accuracy: 0.7910\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4057 - accuracy: 0.8022 - val_loss: 0.4416 - val_accuracy: 0.7881\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4023 - accuracy: 0.8010 - val_loss: 0.4383 - val_accuracy: 0.7855\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.3955 - accuracy: 0.7997 - val_loss: 0.4406 - val_accuracy: 0.7816\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3992 - accuracy: 0.8011 - val_loss: 0.4270 - val_accuracy: 0.7913\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.3872 - accuracy: 0.8039 - val_loss: 0.4487 - val_accuracy: 0.7884\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.3959 - accuracy: 0.7993 - val_loss: 0.4426 - val_accuracy: 0.7812\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.3942 - accuracy: 0.7965 - val_loss: 0.4353 - val_accuracy: 0.7859\n",
      "87/87 [==============================] - 1s 4ms/step\n",
      "accuracy 0.7858951175406872\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 162, 13)\n",
      "(9216,)\n",
      "(6451, 162, 13)\n",
      "(2765, 162, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_48 (Flatten)        (None, 2106)              0         \n",
      "                                                                 \n",
      " batch_normalization_48 (Ba  (None, 2106)              8424      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 256)               539392    \n",
      "                                                                 \n",
      " dropout_144 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_145 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_146 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 597482 (2.28 MB)\n",
      "Trainable params: 593270 (2.26 MB)\n",
      "Non-trainable params: 4212 (16.45 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 6s 20ms/step - loss: 0.7141 - accuracy: 0.5632 - val_loss: 0.6210 - val_accuracy: 0.6329\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.6107 - accuracy: 0.6614 - val_loss: 0.5742 - val_accuracy: 0.6821\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5613 - accuracy: 0.7010 - val_loss: 0.5383 - val_accuracy: 0.7067\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.5262 - accuracy: 0.7233 - val_loss: 0.5409 - val_accuracy: 0.7074\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.5117 - accuracy: 0.7374 - val_loss: 0.5024 - val_accuracy: 0.7371\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4957 - accuracy: 0.7411 - val_loss: 0.4766 - val_accuracy: 0.7526\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4805 - accuracy: 0.7610 - val_loss: 0.4844 - val_accuracy: 0.7490\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4764 - accuracy: 0.7665 - val_loss: 0.4837 - val_accuracy: 0.7548\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4647 - accuracy: 0.7686 - val_loss: 0.4579 - val_accuracy: 0.7743\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4547 - accuracy: 0.7766 - val_loss: 0.4598 - val_accuracy: 0.7649\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4472 - accuracy: 0.7752 - val_loss: 0.4699 - val_accuracy: 0.7664\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4465 - accuracy: 0.7822 - val_loss: 0.4508 - val_accuracy: 0.7754\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4508 - accuracy: 0.7769 - val_loss: 0.4527 - val_accuracy: 0.7790\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4357 - accuracy: 0.7845 - val_loss: 0.4458 - val_accuracy: 0.7769\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4360 - accuracy: 0.7838 - val_loss: 0.4333 - val_accuracy: 0.7841\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4346 - accuracy: 0.7865 - val_loss: 0.4443 - val_accuracy: 0.7754\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4216 - accuracy: 0.7969 - val_loss: 0.4381 - val_accuracy: 0.7740\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4302 - accuracy: 0.7886 - val_loss: 0.4442 - val_accuracy: 0.7765\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4227 - accuracy: 0.7903 - val_loss: 0.4318 - val_accuracy: 0.7917\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4154 - accuracy: 0.7938 - val_loss: 0.4356 - val_accuracy: 0.7779\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4146 - accuracy: 0.7909 - val_loss: 0.4486 - val_accuracy: 0.7772\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4198 - accuracy: 0.7951 - val_loss: 0.4284 - val_accuracy: 0.7823\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4065 - accuracy: 0.7952 - val_loss: 0.4434 - val_accuracy: 0.7870\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4077 - accuracy: 0.8007 - val_loss: 0.4416 - val_accuracy: 0.7892\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4047 - accuracy: 0.8016 - val_loss: 0.4333 - val_accuracy: 0.7895\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.3968 - accuracy: 0.7996 - val_loss: 0.4351 - val_accuracy: 0.7837\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3965 - accuracy: 0.8011 - val_loss: 0.4276 - val_accuracy: 0.7859\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3880 - accuracy: 0.8067 - val_loss: 0.4409 - val_accuracy: 0.7866\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3941 - accuracy: 0.8003 - val_loss: 0.4374 - val_accuracy: 0.7870\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3910 - accuracy: 0.7999 - val_loss: 0.4348 - val_accuracy: 0.7855\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "accuracy 0.7855334538878843\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 168, 13)\n",
      "(9216,)\n",
      "(6451, 168, 13)\n",
      "(2765, 168, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_49 (Flatten)        (None, 2184)              0         \n",
      "                                                                 \n",
      " batch_normalization_49 (Ba  (None, 2184)              8736      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_196 (Dense)           (None, 256)               559360    \n",
      "                                                                 \n",
      " dropout_147 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_148 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_149 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 617762 (2.36 MB)\n",
      "Trainable params: 613394 (2.34 MB)\n",
      "Non-trainable params: 4368 (17.06 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 5s 18ms/step - loss: 0.7118 - accuracy: 0.5762 - val_loss: 0.6123 - val_accuracy: 0.6535\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.6118 - accuracy: 0.6624 - val_loss: 0.5716 - val_accuracy: 0.6926\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.5572 - accuracy: 0.7033 - val_loss: 0.5375 - val_accuracy: 0.7139\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.5239 - accuracy: 0.7244 - val_loss: 0.5227 - val_accuracy: 0.7288\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.5014 - accuracy: 0.7404 - val_loss: 0.4809 - val_accuracy: 0.7472\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4899 - accuracy: 0.7600 - val_loss: 0.4788 - val_accuracy: 0.7508\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4808 - accuracy: 0.7552 - val_loss: 0.4850 - val_accuracy: 0.7443\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4677 - accuracy: 0.7656 - val_loss: 0.4617 - val_accuracy: 0.7620\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4663 - accuracy: 0.7717 - val_loss: 0.4625 - val_accuracy: 0.7689\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4519 - accuracy: 0.7771 - val_loss: 0.4674 - val_accuracy: 0.7649\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4502 - accuracy: 0.7779 - val_loss: 0.4563 - val_accuracy: 0.7823\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4480 - accuracy: 0.7766 - val_loss: 0.4498 - val_accuracy: 0.7830\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4491 - accuracy: 0.7723 - val_loss: 0.4427 - val_accuracy: 0.7823\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4350 - accuracy: 0.7771 - val_loss: 0.4417 - val_accuracy: 0.7772\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4345 - accuracy: 0.7881 - val_loss: 0.4380 - val_accuracy: 0.7859\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4364 - accuracy: 0.7813 - val_loss: 0.4536 - val_accuracy: 0.7743\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4268 - accuracy: 0.7906 - val_loss: 0.4350 - val_accuracy: 0.7801\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4304 - accuracy: 0.7892 - val_loss: 0.4379 - val_accuracy: 0.7805\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4250 - accuracy: 0.7909 - val_loss: 0.4360 - val_accuracy: 0.7892\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4085 - accuracy: 0.7937 - val_loss: 0.4326 - val_accuracy: 0.7805\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 18ms/step - loss: 0.4103 - accuracy: 0.7983 - val_loss: 0.4316 - val_accuracy: 0.7855\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4186 - accuracy: 0.7931 - val_loss: 0.4338 - val_accuracy: 0.7841\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4061 - accuracy: 0.8000 - val_loss: 0.4364 - val_accuracy: 0.7866\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4019 - accuracy: 0.7989 - val_loss: 0.4426 - val_accuracy: 0.7942\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4044 - accuracy: 0.7968 - val_loss: 0.4351 - val_accuracy: 0.7884\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3953 - accuracy: 0.8014 - val_loss: 0.4391 - val_accuracy: 0.7830\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.4007 - accuracy: 0.7974 - val_loss: 0.4277 - val_accuracy: 0.7816\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.3922 - accuracy: 0.8034 - val_loss: 0.4536 - val_accuracy: 0.7924\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.3891 - accuracy: 0.8045 - val_loss: 0.4330 - val_accuracy: 0.7866\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.3960 - accuracy: 0.7999 - val_loss: 0.4330 - val_accuracy: 0.7790\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "accuracy 0.7790235081374322\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 174, 13)\n",
      "(9216,)\n",
      "(6451, 174, 13)\n",
      "(2765, 174, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_50 (Flatten)        (None, 2262)              0         \n",
      "                                                                 \n",
      " batch_normalization_50 (Ba  (None, 2262)              9048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_200 (Dense)           (None, 256)               579328    \n",
      "                                                                 \n",
      " dropout_150 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_201 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_152 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_50 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 638042 (2.43 MB)\n",
      "Trainable params: 633518 (2.42 MB)\n",
      "Non-trainable params: 4524 (17.67 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 6s 20ms/step - loss: 0.7114 - accuracy: 0.5776 - val_loss: 0.6184 - val_accuracy: 0.6568\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.6107 - accuracy: 0.6601 - val_loss: 0.5761 - val_accuracy: 0.6875\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5617 - accuracy: 0.7104 - val_loss: 0.5311 - val_accuracy: 0.7298\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5273 - accuracy: 0.7235 - val_loss: 0.5227 - val_accuracy: 0.7345\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5042 - accuracy: 0.7408 - val_loss: 0.4844 - val_accuracy: 0.7414\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4954 - accuracy: 0.7523 - val_loss: 0.4822 - val_accuracy: 0.7588\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4777 - accuracy: 0.7616 - val_loss: 0.4933 - val_accuracy: 0.7537\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4675 - accuracy: 0.7613 - val_loss: 0.4572 - val_accuracy: 0.7682\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4636 - accuracy: 0.7703 - val_loss: 0.4502 - val_accuracy: 0.7801\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4509 - accuracy: 0.7752 - val_loss: 0.4546 - val_accuracy: 0.7747\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4471 - accuracy: 0.7755 - val_loss: 0.4548 - val_accuracy: 0.7754\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4438 - accuracy: 0.7793 - val_loss: 0.4443 - val_accuracy: 0.7750\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4449 - accuracy: 0.7734 - val_loss: 0.4385 - val_accuracy: 0.7852\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4409 - accuracy: 0.7819 - val_loss: 0.4407 - val_accuracy: 0.7812\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4319 - accuracy: 0.7893 - val_loss: 0.4361 - val_accuracy: 0.7844\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4363 - accuracy: 0.7867 - val_loss: 0.4352 - val_accuracy: 0.7910\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4194 - accuracy: 0.7948 - val_loss: 0.4373 - val_accuracy: 0.7783\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4249 - accuracy: 0.7853 - val_loss: 0.4355 - val_accuracy: 0.7848\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4276 - accuracy: 0.7861 - val_loss: 0.4432 - val_accuracy: 0.7816\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4140 - accuracy: 0.7926 - val_loss: 0.4323 - val_accuracy: 0.7855\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4142 - accuracy: 0.7937 - val_loss: 0.4293 - val_accuracy: 0.7939\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4143 - accuracy: 0.7915 - val_loss: 0.4354 - val_accuracy: 0.7783\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4063 - accuracy: 0.7952 - val_loss: 0.4348 - val_accuracy: 0.7844\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4024 - accuracy: 0.8016 - val_loss: 0.4405 - val_accuracy: 0.7848\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4034 - accuracy: 0.7976 - val_loss: 0.4328 - val_accuracy: 0.7873\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 4s 19ms/step - loss: 0.3986 - accuracy: 0.7977 - val_loss: 0.4560 - val_accuracy: 0.7769\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.3950 - accuracy: 0.7991 - val_loss: 0.4380 - val_accuracy: 0.7859\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.3888 - accuracy: 0.8031 - val_loss: 0.4381 - val_accuracy: 0.7866\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3849 - accuracy: 0.8031 - val_loss: 0.4339 - val_accuracy: 0.7830\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.3888 - accuracy: 0.8036 - val_loss: 0.4312 - val_accuracy: 0.7870\n",
      "87/87 [==============================] - 1s 4ms/step\n",
      "accuracy 0.7869801084990958\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 180, 13)\n",
      "(9216,)\n",
      "(6451, 180, 13)\n",
      "(2765, 180, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_51 (Flatten)        (None, 2340)              0         \n",
      "                                                                 \n",
      " batch_normalization_51 (Ba  (None, 2340)              9360      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_204 (Dense)           (None, 256)               599296    \n",
      "                                                                 \n",
      " dropout_153 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_205 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_154 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_206 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_155 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_51 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 658322 (2.51 MB)\n",
      "Trainable params: 653642 (2.49 MB)\n",
      "Non-trainable params: 4680 (18.28 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 6s 23ms/step - loss: 0.7035 - accuracy: 0.5731 - val_loss: 0.6071 - val_accuracy: 0.6568\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.6090 - accuracy: 0.6582 - val_loss: 0.5746 - val_accuracy: 0.6875\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5628 - accuracy: 0.6966 - val_loss: 0.5241 - val_accuracy: 0.7183\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.5285 - accuracy: 0.7236 - val_loss: 0.4963 - val_accuracy: 0.7436\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.5071 - accuracy: 0.7383 - val_loss: 0.5042 - val_accuracy: 0.7222\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4961 - accuracy: 0.7509 - val_loss: 0.4806 - val_accuracy: 0.7458\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4861 - accuracy: 0.7591 - val_loss: 0.4923 - val_accuracy: 0.7439\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4681 - accuracy: 0.7625 - val_loss: 0.4647 - val_accuracy: 0.7743\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4691 - accuracy: 0.7659 - val_loss: 0.4676 - val_accuracy: 0.7678\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4571 - accuracy: 0.7735 - val_loss: 0.4570 - val_accuracy: 0.7787\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4462 - accuracy: 0.7762 - val_loss: 0.4451 - val_accuracy: 0.7834\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4451 - accuracy: 0.7839 - val_loss: 0.4486 - val_accuracy: 0.7750\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4443 - accuracy: 0.7831 - val_loss: 0.4463 - val_accuracy: 0.7754\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4368 - accuracy: 0.7813 - val_loss: 0.4423 - val_accuracy: 0.7787\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4355 - accuracy: 0.7879 - val_loss: 0.4305 - val_accuracy: 0.7830\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4331 - accuracy: 0.7851 - val_loss: 0.4442 - val_accuracy: 0.7747\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4235 - accuracy: 0.7980 - val_loss: 0.4336 - val_accuracy: 0.7884\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4272 - accuracy: 0.7844 - val_loss: 0.4383 - val_accuracy: 0.7797\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.4219 - accuracy: 0.7903 - val_loss: 0.4358 - val_accuracy: 0.7881\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4137 - accuracy: 0.7968 - val_loss: 0.4330 - val_accuracy: 0.7830\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4121 - accuracy: 0.7946 - val_loss: 0.4557 - val_accuracy: 0.7826\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4165 - accuracy: 0.7948 - val_loss: 0.4296 - val_accuracy: 0.7884\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4092 - accuracy: 0.7952 - val_loss: 0.4357 - val_accuracy: 0.7888\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4055 - accuracy: 0.7979 - val_loss: 0.4448 - val_accuracy: 0.7884\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.3990 - accuracy: 0.7997 - val_loss: 0.4456 - val_accuracy: 0.7884\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.3974 - accuracy: 0.8042 - val_loss: 0.4439 - val_accuracy: 0.7787\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.3923 - accuracy: 0.8020 - val_loss: 0.4304 - val_accuracy: 0.7895\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.3887 - accuracy: 0.8058 - val_loss: 0.4535 - val_accuracy: 0.7819\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.3952 - accuracy: 0.8019 - val_loss: 0.4448 - val_accuracy: 0.7812\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.3916 - accuracy: 0.7999 - val_loss: 0.4301 - val_accuracy: 0.7855\n",
      "87/87 [==============================] - 1s 5ms/step\n",
      "accuracy 0.7858951175406872\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 186, 13)\n",
      "(9216,)\n",
      "(6451, 186, 13)\n",
      "(2765, 186, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_52 (Flatten)        (None, 2418)              0         \n",
      "                                                                 \n",
      " batch_normalization_52 (Ba  (None, 2418)              9672      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 256)               619264    \n",
      "                                                                 \n",
      " dropout_156 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_157 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_210 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_158 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_211 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_52 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 678602 (2.59 MB)\n",
      "Trainable params: 673766 (2.57 MB)\n",
      "Non-trainable params: 4836 (18.89 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 6s 21ms/step - loss: 0.7138 - accuracy: 0.5729 - val_loss: 0.6110 - val_accuracy: 0.6608\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.6133 - accuracy: 0.6604 - val_loss: 0.5779 - val_accuracy: 0.6893\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 4s 20ms/step - loss: 0.5621 - accuracy: 0.7076 - val_loss: 0.5530 - val_accuracy: 0.7009\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.5336 - accuracy: 0.7278 - val_loss: 0.5133 - val_accuracy: 0.7284\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5061 - accuracy: 0.7366 - val_loss: 0.5038 - val_accuracy: 0.7396\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4949 - accuracy: 0.7515 - val_loss: 0.4961 - val_accuracy: 0.7269\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.4879 - accuracy: 0.7555 - val_loss: 0.4904 - val_accuracy: 0.7461\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4742 - accuracy: 0.7590 - val_loss: 0.4712 - val_accuracy: 0.7631\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4661 - accuracy: 0.7686 - val_loss: 0.4664 - val_accuracy: 0.7732\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4600 - accuracy: 0.7749 - val_loss: 0.4604 - val_accuracy: 0.7703\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4530 - accuracy: 0.7729 - val_loss: 0.4554 - val_accuracy: 0.7747\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4463 - accuracy: 0.7771 - val_loss: 0.4508 - val_accuracy: 0.7776\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4466 - accuracy: 0.7715 - val_loss: 0.4578 - val_accuracy: 0.7656\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4407 - accuracy: 0.7816 - val_loss: 0.4461 - val_accuracy: 0.7772\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4373 - accuracy: 0.7872 - val_loss: 0.4417 - val_accuracy: 0.7819\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4364 - accuracy: 0.7887 - val_loss: 0.4455 - val_accuracy: 0.7808\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4284 - accuracy: 0.7976 - val_loss: 0.4333 - val_accuracy: 0.7881\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.4312 - accuracy: 0.7844 - val_loss: 0.4442 - val_accuracy: 0.7779\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4281 - accuracy: 0.7878 - val_loss: 0.4382 - val_accuracy: 0.7816\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4244 - accuracy: 0.7915 - val_loss: 0.4357 - val_accuracy: 0.7797\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4130 - accuracy: 0.7940 - val_loss: 0.4384 - val_accuracy: 0.7946\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4192 - accuracy: 0.7914 - val_loss: 0.4364 - val_accuracy: 0.7823\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4079 - accuracy: 0.7980 - val_loss: 0.4376 - val_accuracy: 0.7895\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4077 - accuracy: 0.7982 - val_loss: 0.4425 - val_accuracy: 0.7877\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.3997 - accuracy: 0.7997 - val_loss: 0.4352 - val_accuracy: 0.7834\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3959 - accuracy: 0.8038 - val_loss: 0.4483 - val_accuracy: 0.7718\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3954 - accuracy: 0.8024 - val_loss: 0.4365 - val_accuracy: 0.7877\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3889 - accuracy: 0.8042 - val_loss: 0.4587 - val_accuracy: 0.7884\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3922 - accuracy: 0.8013 - val_loss: 0.4525 - val_accuracy: 0.7866\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3890 - accuracy: 0.8034 - val_loss: 0.4380 - val_accuracy: 0.7881\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "accuracy 0.7880650994575045\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 192, 13)\n",
      "(9216,)\n",
      "(6451, 192, 13)\n",
      "(2765, 192, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_53 (Flatten)        (None, 2496)              0         \n",
      "                                                                 \n",
      " batch_normalization_53 (Ba  (None, 2496)              9984      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 256)               639232    \n",
      "                                                                 \n",
      " dropout_159 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_160 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_214 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_161 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_215 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_53 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 698882 (2.67 MB)\n",
      "Trainable params: 693890 (2.65 MB)\n",
      "Non-trainable params: 4992 (19.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 8s 27ms/step - loss: 0.7080 - accuracy: 0.5779 - val_loss: 0.6099 - val_accuracy: 0.6561\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.6163 - accuracy: 0.6565 - val_loss: 0.6116 - val_accuracy: 0.6553\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5698 - accuracy: 0.6911 - val_loss: 0.5398 - val_accuracy: 0.7197\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5353 - accuracy: 0.7188 - val_loss: 0.5737 - val_accuracy: 0.6958\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5114 - accuracy: 0.7421 - val_loss: 0.5153 - val_accuracy: 0.7367\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4991 - accuracy: 0.7433 - val_loss: 0.4859 - val_accuracy: 0.7418\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4897 - accuracy: 0.7616 - val_loss: 0.4918 - val_accuracy: 0.7443\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4731 - accuracy: 0.7600 - val_loss: 0.4621 - val_accuracy: 0.7620\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4671 - accuracy: 0.7703 - val_loss: 0.4596 - val_accuracy: 0.7664\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4518 - accuracy: 0.7810 - val_loss: 0.4457 - val_accuracy: 0.7765\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4485 - accuracy: 0.7782 - val_loss: 0.4720 - val_accuracy: 0.7664\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4517 - accuracy: 0.7760 - val_loss: 0.4543 - val_accuracy: 0.7736\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4445 - accuracy: 0.7760 - val_loss: 0.4471 - val_accuracy: 0.7805\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4322 - accuracy: 0.7851 - val_loss: 0.4393 - val_accuracy: 0.7844\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4349 - accuracy: 0.7825 - val_loss: 0.4417 - val_accuracy: 0.7805\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4389 - accuracy: 0.7844 - val_loss: 0.4434 - val_accuracy: 0.7776\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4220 - accuracy: 0.7929 - val_loss: 0.4343 - val_accuracy: 0.7917\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4293 - accuracy: 0.7851 - val_loss: 0.4469 - val_accuracy: 0.7769\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4199 - accuracy: 0.7869 - val_loss: 0.4340 - val_accuracy: 0.7942\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4158 - accuracy: 0.7948 - val_loss: 0.4331 - val_accuracy: 0.7783\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4136 - accuracy: 0.7917 - val_loss: 0.4516 - val_accuracy: 0.7830\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4189 - accuracy: 0.7966 - val_loss: 0.4346 - val_accuracy: 0.7873\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4072 - accuracy: 0.7960 - val_loss: 0.4424 - val_accuracy: 0.7877\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4019 - accuracy: 0.7943 - val_loss: 0.4593 - val_accuracy: 0.7805\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4018 - accuracy: 0.8005 - val_loss: 0.4387 - val_accuracy: 0.7852\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3958 - accuracy: 0.7966 - val_loss: 0.4391 - val_accuracy: 0.7714\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3941 - accuracy: 0.7972 - val_loss: 0.4316 - val_accuracy: 0.7881\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3929 - accuracy: 0.8103 - val_loss: 0.4424 - val_accuracy: 0.7855\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3907 - accuracy: 0.8007 - val_loss: 0.4267 - val_accuracy: 0.7848\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3870 - accuracy: 0.8019 - val_loss: 0.4540 - val_accuracy: 0.7805\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "accuracy 0.7804701627486438\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 198, 13)\n",
      "(9216,)\n",
      "(6451, 198, 13)\n",
      "(2765, 198, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_54 (Flatten)        (None, 2574)              0         \n",
      "                                                                 \n",
      " batch_normalization_54 (Ba  (None, 2574)              10296     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_216 (Dense)           (None, 256)               659200    \n",
      "                                                                 \n",
      " dropout_162 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_163 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_164 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_54 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 719162 (2.74 MB)\n",
      "Trainable params: 714014 (2.72 MB)\n",
      "Non-trainable params: 5148 (20.11 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 9s 27ms/step - loss: 0.7051 - accuracy: 0.5827 - val_loss: 0.6172 - val_accuracy: 0.6416\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.6099 - accuracy: 0.6658 - val_loss: 0.5617 - val_accuracy: 0.6962\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5640 - accuracy: 0.6993 - val_loss: 0.5458 - val_accuracy: 0.7085\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5291 - accuracy: 0.7217 - val_loss: 0.5839 - val_accuracy: 0.7045\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5102 - accuracy: 0.7343 - val_loss: 0.4951 - val_accuracy: 0.7320\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4930 - accuracy: 0.7507 - val_loss: 0.4785 - val_accuracy: 0.7479\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4847 - accuracy: 0.7596 - val_loss: 0.4882 - val_accuracy: 0.7483\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4697 - accuracy: 0.7619 - val_loss: 0.4588 - val_accuracy: 0.7671\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4644 - accuracy: 0.7696 - val_loss: 0.4704 - val_accuracy: 0.7595\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4524 - accuracy: 0.7794 - val_loss: 0.4516 - val_accuracy: 0.7769\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4502 - accuracy: 0.7794 - val_loss: 0.4494 - val_accuracy: 0.7758\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4490 - accuracy: 0.7765 - val_loss: 0.4600 - val_accuracy: 0.7682\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4490 - accuracy: 0.7748 - val_loss: 0.4462 - val_accuracy: 0.7808\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4389 - accuracy: 0.7769 - val_loss: 0.4417 - val_accuracy: 0.7761\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 6s 25ms/step - loss: 0.4283 - accuracy: 0.7881 - val_loss: 0.4295 - val_accuracy: 0.7899\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4360 - accuracy: 0.7836 - val_loss: 0.4505 - val_accuracy: 0.7732\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4239 - accuracy: 0.7938 - val_loss: 0.4393 - val_accuracy: 0.7729\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4260 - accuracy: 0.7890 - val_loss: 0.4349 - val_accuracy: 0.7805\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4194 - accuracy: 0.7886 - val_loss: 0.4507 - val_accuracy: 0.7895\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4159 - accuracy: 0.7952 - val_loss: 0.4332 - val_accuracy: 0.7873\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4167 - accuracy: 0.7896 - val_loss: 0.4435 - val_accuracy: 0.7826\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4080 - accuracy: 0.7976 - val_loss: 0.4340 - val_accuracy: 0.7855\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4090 - accuracy: 0.7929 - val_loss: 0.4400 - val_accuracy: 0.7848\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4002 - accuracy: 0.8002 - val_loss: 0.4570 - val_accuracy: 0.7772\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4036 - accuracy: 0.7955 - val_loss: 0.4289 - val_accuracy: 0.7859\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3956 - accuracy: 0.8003 - val_loss: 0.4492 - val_accuracy: 0.7783\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3896 - accuracy: 0.8064 - val_loss: 0.4409 - val_accuracy: 0.7844\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3919 - accuracy: 0.8020 - val_loss: 0.4544 - val_accuracy: 0.7794\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3892 - accuracy: 0.8007 - val_loss: 0.4439 - val_accuracy: 0.7801\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3918 - accuracy: 0.7994 - val_loss: 0.4457 - val_accuracy: 0.7870\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "accuracy 0.7869801084990958\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n",
      "53   -60s:138s        4608    False  0.786980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 204, 13)\n",
      "(9216,)\n",
      "(6451, 204, 13)\n",
      "(2765, 204, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_55 (Flatten)        (None, 2652)              0         \n",
      "                                                                 \n",
      " batch_normalization_55 (Ba  (None, 2652)              10608     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_220 (Dense)           (None, 256)               679168    \n",
      "                                                                 \n",
      " dropout_165 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_221 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_166 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_167 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_55 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 739442 (2.82 MB)\n",
      "Trainable params: 734138 (2.80 MB)\n",
      "Non-trainable params: 5304 (20.72 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 8s 26ms/step - loss: 0.7141 - accuracy: 0.5852 - val_loss: 0.6192 - val_accuracy: 0.6586\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.6172 - accuracy: 0.6628 - val_loss: 0.5711 - val_accuracy: 0.6933\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5662 - accuracy: 0.6987 - val_loss: 0.5432 - val_accuracy: 0.7154\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5365 - accuracy: 0.7194 - val_loss: 0.5369 - val_accuracy: 0.7157\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5100 - accuracy: 0.7386 - val_loss: 0.5041 - val_accuracy: 0.7259\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5074 - accuracy: 0.7400 - val_loss: 0.4901 - val_accuracy: 0.7414\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4877 - accuracy: 0.7617 - val_loss: 0.4802 - val_accuracy: 0.7486\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4755 - accuracy: 0.7588 - val_loss: 0.4693 - val_accuracy: 0.7718\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4693 - accuracy: 0.7678 - val_loss: 0.4658 - val_accuracy: 0.7606\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4606 - accuracy: 0.7731 - val_loss: 0.4602 - val_accuracy: 0.7689\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4498 - accuracy: 0.7704 - val_loss: 0.4575 - val_accuracy: 0.7703\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4504 - accuracy: 0.7718 - val_loss: 0.4463 - val_accuracy: 0.7801\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4456 - accuracy: 0.7748 - val_loss: 0.4463 - val_accuracy: 0.7801\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4384 - accuracy: 0.7824 - val_loss: 0.4506 - val_accuracy: 0.7653\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4343 - accuracy: 0.7842 - val_loss: 0.4384 - val_accuracy: 0.7881\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 4s 21ms/step - loss: 0.4426 - accuracy: 0.7819 - val_loss: 0.4399 - val_accuracy: 0.7881\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4250 - accuracy: 0.7917 - val_loss: 0.4341 - val_accuracy: 0.7863\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4204 - accuracy: 0.7895 - val_loss: 0.4437 - val_accuracy: 0.7776\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4304 - accuracy: 0.7886 - val_loss: 0.4324 - val_accuracy: 0.7920\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4120 - accuracy: 0.7958 - val_loss: 0.4339 - val_accuracy: 0.7797\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4129 - accuracy: 0.7927 - val_loss: 0.4375 - val_accuracy: 0.7848\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4161 - accuracy: 0.7958 - val_loss: 0.4321 - val_accuracy: 0.7877\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4078 - accuracy: 0.7948 - val_loss: 0.4410 - val_accuracy: 0.7902\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4019 - accuracy: 0.7983 - val_loss: 0.4551 - val_accuracy: 0.7808\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4030 - accuracy: 0.7976 - val_loss: 0.4324 - val_accuracy: 0.7855\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3919 - accuracy: 0.8003 - val_loss: 0.4440 - val_accuracy: 0.7779\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3956 - accuracy: 0.8030 - val_loss: 0.4256 - val_accuracy: 0.7895\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3923 - accuracy: 0.8025 - val_loss: 0.4438 - val_accuracy: 0.7866\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3933 - accuracy: 0.8008 - val_loss: 0.4530 - val_accuracy: 0.7776\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3899 - accuracy: 0.8025 - val_loss: 0.4408 - val_accuracy: 0.7873\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "accuracy 0.7873417721518987\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n",
      "53   -60s:138s        4608    False  0.786980\n",
      "54   -60s:144s        4608    False  0.787342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 210, 13)\n",
      "(9216,)\n",
      "(6451, 210, 13)\n",
      "(2765, 210, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_56 (Flatten)        (None, 2730)              0         \n",
      "                                                                 \n",
      " batch_normalization_56 (Ba  (None, 2730)              10920     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_224 (Dense)           (None, 256)               699136    \n",
      "                                                                 \n",
      " dropout_168 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_225 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_169 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_226 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_170 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_227 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_56 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 759722 (2.90 MB)\n",
      "Trainable params: 754262 (2.88 MB)\n",
      "Non-trainable params: 5460 (21.33 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 8s 24ms/step - loss: 0.7111 - accuracy: 0.5788 - val_loss: 0.6170 - val_accuracy: 0.6553\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.6124 - accuracy: 0.6611 - val_loss: 0.5804 - val_accuracy: 0.6759\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5678 - accuracy: 0.6979 - val_loss: 0.5506 - val_accuracy: 0.7049\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.5405 - accuracy: 0.7166 - val_loss: 0.5866 - val_accuracy: 0.6882\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.5164 - accuracy: 0.7346 - val_loss: 0.5093 - val_accuracy: 0.7302\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5026 - accuracy: 0.7452 - val_loss: 0.5043 - val_accuracy: 0.7291\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4845 - accuracy: 0.7551 - val_loss: 0.4944 - val_accuracy: 0.7400\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4791 - accuracy: 0.7597 - val_loss: 0.4651 - val_accuracy: 0.7631\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4716 - accuracy: 0.7681 - val_loss: 0.4748 - val_accuracy: 0.7617\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4598 - accuracy: 0.7755 - val_loss: 0.4509 - val_accuracy: 0.7722\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4532 - accuracy: 0.7793 - val_loss: 0.4625 - val_accuracy: 0.7584\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4491 - accuracy: 0.7777 - val_loss: 0.4520 - val_accuracy: 0.7732\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4475 - accuracy: 0.7758 - val_loss: 0.4412 - val_accuracy: 0.7848\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4341 - accuracy: 0.7786 - val_loss: 0.4535 - val_accuracy: 0.7736\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4357 - accuracy: 0.7816 - val_loss: 0.4390 - val_accuracy: 0.7772\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4372 - accuracy: 0.7833 - val_loss: 0.4489 - val_accuracy: 0.7700\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4218 - accuracy: 0.7935 - val_loss: 0.4424 - val_accuracy: 0.7848\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4296 - accuracy: 0.7869 - val_loss: 0.4449 - val_accuracy: 0.7758\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4240 - accuracy: 0.7862 - val_loss: 0.4317 - val_accuracy: 0.7910\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4188 - accuracy: 0.7917 - val_loss: 0.4269 - val_accuracy: 0.7877\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4084 - accuracy: 0.7935 - val_loss: 0.4378 - val_accuracy: 0.7920\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4145 - accuracy: 0.7910 - val_loss: 0.4434 - val_accuracy: 0.7805\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4086 - accuracy: 0.7920 - val_loss: 0.4327 - val_accuracy: 0.7899\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4058 - accuracy: 0.7983 - val_loss: 0.4469 - val_accuracy: 0.7848\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4005 - accuracy: 0.7979 - val_loss: 0.4409 - val_accuracy: 0.7837\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3971 - accuracy: 0.7971 - val_loss: 0.4393 - val_accuracy: 0.7801\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3931 - accuracy: 0.8038 - val_loss: 0.4266 - val_accuracy: 0.7884\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3880 - accuracy: 0.8107 - val_loss: 0.4530 - val_accuracy: 0.7906\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3941 - accuracy: 0.8038 - val_loss: 0.4368 - val_accuracy: 0.7844\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3888 - accuracy: 0.8014 - val_loss: 0.4376 - val_accuracy: 0.7917\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "accuracy 0.7916817359855335\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n",
      "53   -60s:138s        4608    False  0.786980\n",
      "54   -60s:144s        4608    False  0.787342\n",
      "55   -60s:150s        4608    False  0.791682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 216, 13)\n",
      "(9216,)\n",
      "(6451, 216, 13)\n",
      "(2765, 216, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_57 (Flatten)        (None, 2808)              0         \n",
      "                                                                 \n",
      " batch_normalization_57 (Ba  (None, 2808)              11232     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_228 (Dense)           (None, 256)               719104    \n",
      "                                                                 \n",
      " dropout_171 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_229 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_172 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_230 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_173 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_231 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_57 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 780002 (2.98 MB)\n",
      "Trainable params: 774386 (2.95 MB)\n",
      "Non-trainable params: 5616 (21.94 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 8s 26ms/step - loss: 0.7148 - accuracy: 0.5731 - val_loss: 0.6438 - val_accuracy: 0.6427\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.6175 - accuracy: 0.6591 - val_loss: 0.5746 - val_accuracy: 0.6788\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5771 - accuracy: 0.6901 - val_loss: 0.5409 - val_accuracy: 0.7183\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5439 - accuracy: 0.7151 - val_loss: 0.5189 - val_accuracy: 0.7335\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5180 - accuracy: 0.7279 - val_loss: 0.5066 - val_accuracy: 0.7306\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5030 - accuracy: 0.7459 - val_loss: 0.4912 - val_accuracy: 0.7385\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4907 - accuracy: 0.7579 - val_loss: 0.5045 - val_accuracy: 0.7342\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4781 - accuracy: 0.7560 - val_loss: 0.4760 - val_accuracy: 0.7526\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.4717 - accuracy: 0.7634 - val_loss: 0.4685 - val_accuracy: 0.7653\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4587 - accuracy: 0.7743 - val_loss: 0.4505 - val_accuracy: 0.7758\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4556 - accuracy: 0.7698 - val_loss: 0.4551 - val_accuracy: 0.7725\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4532 - accuracy: 0.7715 - val_loss: 0.4491 - val_accuracy: 0.7772\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4499 - accuracy: 0.7734 - val_loss: 0.4464 - val_accuracy: 0.7761\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4370 - accuracy: 0.7793 - val_loss: 0.4495 - val_accuracy: 0.7823\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4349 - accuracy: 0.7869 - val_loss: 0.4426 - val_accuracy: 0.7852\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4381 - accuracy: 0.7892 - val_loss: 0.4473 - val_accuracy: 0.7769\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4274 - accuracy: 0.7875 - val_loss: 0.4401 - val_accuracy: 0.7812\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4270 - accuracy: 0.7834 - val_loss: 0.4425 - val_accuracy: 0.7834\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4244 - accuracy: 0.7872 - val_loss: 0.4421 - val_accuracy: 0.7881\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4165 - accuracy: 0.7878 - val_loss: 0.4362 - val_accuracy: 0.7758\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4132 - accuracy: 0.7932 - val_loss: 0.4309 - val_accuracy: 0.7870\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4154 - accuracy: 0.7901 - val_loss: 0.4354 - val_accuracy: 0.7794\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4139 - accuracy: 0.7952 - val_loss: 0.4364 - val_accuracy: 0.7881\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4024 - accuracy: 0.7999 - val_loss: 0.4495 - val_accuracy: 0.7837\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3985 - accuracy: 0.8031 - val_loss: 0.4303 - val_accuracy: 0.7866\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3966 - accuracy: 0.8000 - val_loss: 0.4473 - val_accuracy: 0.7758\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3909 - accuracy: 0.7997 - val_loss: 0.4336 - val_accuracy: 0.7863\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3890 - accuracy: 0.8047 - val_loss: 0.4555 - val_accuracy: 0.7866\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3958 - accuracy: 0.8051 - val_loss: 0.4422 - val_accuracy: 0.7812\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3910 - accuracy: 0.8028 - val_loss: 0.4469 - val_accuracy: 0.7816\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "accuracy 0.7815551537070524\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n",
      "53   -60s:138s        4608    False  0.786980\n",
      "54   -60s:144s        4608    False  0.787342\n",
      "55   -60s:150s        4608    False  0.791682\n",
      "56   -60s:156s        4608    False  0.781555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 222, 13)\n",
      "(9216,)\n",
      "(6451, 222, 13)\n",
      "(2765, 222, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_58 (Flatten)        (None, 2886)              0         \n",
      "                                                                 \n",
      " batch_normalization_58 (Ba  (None, 2886)              11544     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_232 (Dense)           (None, 256)               739072    \n",
      "                                                                 \n",
      " dropout_174 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_233 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_175 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_176 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_235 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_58 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 800282 (3.05 MB)\n",
      "Trainable params: 794510 (3.03 MB)\n",
      "Non-trainable params: 5772 (22.55 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 9s 29ms/step - loss: 0.7052 - accuracy: 0.5717 - val_loss: 0.6232 - val_accuracy: 0.6571\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.6152 - accuracy: 0.6594 - val_loss: 0.5868 - val_accuracy: 0.6590\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5742 - accuracy: 0.6948 - val_loss: 0.5688 - val_accuracy: 0.6933\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5473 - accuracy: 0.7128 - val_loss: 0.5468 - val_accuracy: 0.6929\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5224 - accuracy: 0.7289 - val_loss: 0.5078 - val_accuracy: 0.7172\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.5118 - accuracy: 0.7438 - val_loss: 0.5090 - val_accuracy: 0.7197\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4968 - accuracy: 0.7523 - val_loss: 0.4941 - val_accuracy: 0.7418\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4823 - accuracy: 0.7557 - val_loss: 0.4666 - val_accuracy: 0.7660\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4770 - accuracy: 0.7625 - val_loss: 0.4678 - val_accuracy: 0.7718\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4622 - accuracy: 0.7737 - val_loss: 0.4637 - val_accuracy: 0.7653\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4585 - accuracy: 0.7712 - val_loss: 0.4534 - val_accuracy: 0.7758\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4520 - accuracy: 0.7741 - val_loss: 0.4580 - val_accuracy: 0.7714\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4529 - accuracy: 0.7732 - val_loss: 0.4612 - val_accuracy: 0.7718\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4381 - accuracy: 0.7820 - val_loss: 0.4427 - val_accuracy: 0.7823\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4354 - accuracy: 0.7834 - val_loss: 0.4469 - val_accuracy: 0.7816\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4365 - accuracy: 0.7850 - val_loss: 0.4522 - val_accuracy: 0.7790\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4281 - accuracy: 0.7921 - val_loss: 0.4543 - val_accuracy: 0.7769\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4313 - accuracy: 0.7851 - val_loss: 0.4375 - val_accuracy: 0.7841\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4250 - accuracy: 0.7842 - val_loss: 0.4474 - val_accuracy: 0.7819\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4184 - accuracy: 0.7940 - val_loss: 0.4348 - val_accuracy: 0.7787\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4121 - accuracy: 0.7985 - val_loss: 0.4429 - val_accuracy: 0.7834\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4182 - accuracy: 0.7921 - val_loss: 0.4396 - val_accuracy: 0.7837\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4136 - accuracy: 0.7912 - val_loss: 0.4409 - val_accuracy: 0.7910\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4054 - accuracy: 0.7972 - val_loss: 0.4515 - val_accuracy: 0.7837\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.3991 - accuracy: 0.8038 - val_loss: 0.4379 - val_accuracy: 0.7841\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3964 - accuracy: 0.7991 - val_loss: 0.4382 - val_accuracy: 0.7754\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4003 - accuracy: 0.7994 - val_loss: 0.4448 - val_accuracy: 0.7754\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 6s 27ms/step - loss: 0.3882 - accuracy: 0.8093 - val_loss: 0.4571 - val_accuracy: 0.7873\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.3951 - accuracy: 0.8041 - val_loss: 0.4447 - val_accuracy: 0.7837\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.3869 - accuracy: 0.8041 - val_loss: 0.4384 - val_accuracy: 0.7852\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "accuracy 0.7851717902350813\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n",
      "53   -60s:138s        4608    False  0.786980\n",
      "54   -60s:144s        4608    False  0.787342\n",
      "55   -60s:150s        4608    False  0.791682\n",
      "56   -60s:156s        4608    False  0.781555\n",
      "57   -60s:162s        4608    False  0.785172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 228, 13)\n",
      "(9216,)\n",
      "(6451, 228, 13)\n",
      "(2765, 228, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_59 (Flatten)        (None, 2964)              0         \n",
      "                                                                 \n",
      " batch_normalization_59 (Ba  (None, 2964)              11856     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_236 (Dense)           (None, 256)               759040    \n",
      "                                                                 \n",
      " dropout_177 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_178 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_179 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_59 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 820562 (3.13 MB)\n",
      "Trainable params: 814634 (3.11 MB)\n",
      "Non-trainable params: 5928 (23.16 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 9s 28ms/step - loss: 0.7146 - accuracy: 0.5822 - val_loss: 0.6248 - val_accuracy: 0.6561\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.6185 - accuracy: 0.6577 - val_loss: 0.5799 - val_accuracy: 0.6803\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5744 - accuracy: 0.6974 - val_loss: 0.5488 - val_accuracy: 0.7150\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.5485 - accuracy: 0.7131 - val_loss: 0.5312 - val_accuracy: 0.7132\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5218 - accuracy: 0.7264 - val_loss: 0.5153 - val_accuracy: 0.7049\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.5040 - accuracy: 0.7394 - val_loss: 0.5006 - val_accuracy: 0.7396\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4965 - accuracy: 0.7532 - val_loss: 0.5005 - val_accuracy: 0.7378\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4807 - accuracy: 0.7579 - val_loss: 0.4887 - val_accuracy: 0.7519\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4734 - accuracy: 0.7633 - val_loss: 0.4878 - val_accuracy: 0.7627\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4606 - accuracy: 0.7695 - val_loss: 0.4660 - val_accuracy: 0.7584\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4532 - accuracy: 0.7768 - val_loss: 0.4617 - val_accuracy: 0.7696\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4519 - accuracy: 0.7749 - val_loss: 0.4573 - val_accuracy: 0.7696\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4514 - accuracy: 0.7751 - val_loss: 0.4436 - val_accuracy: 0.7812\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4420 - accuracy: 0.7796 - val_loss: 0.4460 - val_accuracy: 0.7779\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4312 - accuracy: 0.7848 - val_loss: 0.4449 - val_accuracy: 0.7834\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4393 - accuracy: 0.7859 - val_loss: 0.4391 - val_accuracy: 0.7805\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4292 - accuracy: 0.7893 - val_loss: 0.4331 - val_accuracy: 0.7859\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4309 - accuracy: 0.7870 - val_loss: 0.4394 - val_accuracy: 0.7808\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4283 - accuracy: 0.7858 - val_loss: 0.4382 - val_accuracy: 0.7863\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4176 - accuracy: 0.7948 - val_loss: 0.4378 - val_accuracy: 0.7844\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4157 - accuracy: 0.7958 - val_loss: 0.4555 - val_accuracy: 0.7830\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4169 - accuracy: 0.7943 - val_loss: 0.4294 - val_accuracy: 0.7816\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4106 - accuracy: 0.7983 - val_loss: 0.4363 - val_accuracy: 0.7870\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4060 - accuracy: 0.7982 - val_loss: 0.4441 - val_accuracy: 0.7855\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4035 - accuracy: 0.8010 - val_loss: 0.4323 - val_accuracy: 0.7884\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.3931 - accuracy: 0.7986 - val_loss: 0.4456 - val_accuracy: 0.7732\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3921 - accuracy: 0.8013 - val_loss: 0.4437 - val_accuracy: 0.7837\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3941 - accuracy: 0.8041 - val_loss: 0.4490 - val_accuracy: 0.7866\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.3964 - accuracy: 0.8025 - val_loss: 0.4442 - val_accuracy: 0.7816\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 6s 27ms/step - loss: 0.3881 - accuracy: 0.8024 - val_loss: 0.4455 - val_accuracy: 0.7808\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "accuracy 0.7808318264014467\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n",
      "53   -60s:138s        4608    False  0.786980\n",
      "54   -60s:144s        4608    False  0.787342\n",
      "55   -60s:150s        4608    False  0.791682\n",
      "56   -60s:156s        4608    False  0.781555\n",
      "57   -60s:162s        4608    False  0.785172\n",
      "58   -60s:168s        4608    False  0.780832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 234, 13)\n",
      "(9216,)\n",
      "(6451, 234, 13)\n",
      "(2765, 234, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_60 (Flatten)        (None, 3042)              0         \n",
      "                                                                 \n",
      " batch_normalization_60 (Ba  (None, 3042)              12168     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_240 (Dense)           (None, 256)               779008    \n",
      "                                                                 \n",
      " dropout_180 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_181 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_182 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_60 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 840842 (3.21 MB)\n",
      "Trainable params: 834758 (3.18 MB)\n",
      "Non-trainable params: 6084 (23.77 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 8s 27ms/step - loss: 0.7028 - accuracy: 0.5818 - val_loss: 0.6199 - val_accuracy: 0.6495\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.6136 - accuracy: 0.6633 - val_loss: 0.5712 - val_accuracy: 0.6886\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5714 - accuracy: 0.6938 - val_loss: 0.5451 - val_accuracy: 0.7161\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.5449 - accuracy: 0.7126 - val_loss: 0.5362 - val_accuracy: 0.7186\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5298 - accuracy: 0.7244 - val_loss: 0.5168 - val_accuracy: 0.7128\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.5066 - accuracy: 0.7422 - val_loss: 0.4925 - val_accuracy: 0.7371\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4966 - accuracy: 0.7473 - val_loss: 0.4998 - val_accuracy: 0.7313\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4845 - accuracy: 0.7520 - val_loss: 0.4648 - val_accuracy: 0.7700\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4779 - accuracy: 0.7650 - val_loss: 0.4640 - val_accuracy: 0.7646\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4610 - accuracy: 0.7783 - val_loss: 0.4657 - val_accuracy: 0.7584\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4527 - accuracy: 0.7743 - val_loss: 0.4678 - val_accuracy: 0.7729\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4541 - accuracy: 0.7738 - val_loss: 0.4587 - val_accuracy: 0.7675\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4456 - accuracy: 0.7786 - val_loss: 0.4434 - val_accuracy: 0.7830\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4427 - accuracy: 0.7802 - val_loss: 0.4514 - val_accuracy: 0.7765\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4371 - accuracy: 0.7810 - val_loss: 0.4412 - val_accuracy: 0.7787\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4412 - accuracy: 0.7824 - val_loss: 0.4457 - val_accuracy: 0.7797\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4241 - accuracy: 0.7962 - val_loss: 0.4407 - val_accuracy: 0.7808\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4321 - accuracy: 0.7855 - val_loss: 0.4383 - val_accuracy: 0.7769\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4270 - accuracy: 0.7844 - val_loss: 0.4377 - val_accuracy: 0.7863\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4189 - accuracy: 0.7993 - val_loss: 0.4324 - val_accuracy: 0.7888\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4140 - accuracy: 0.7896 - val_loss: 0.4736 - val_accuracy: 0.7627\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4147 - accuracy: 0.7996 - val_loss: 0.4387 - val_accuracy: 0.7812\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4149 - accuracy: 0.7917 - val_loss: 0.4493 - val_accuracy: 0.7805\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4029 - accuracy: 0.7985 - val_loss: 0.4514 - val_accuracy: 0.7884\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4059 - accuracy: 0.7985 - val_loss: 0.4356 - val_accuracy: 0.7895\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3913 - accuracy: 0.7999 - val_loss: 0.4420 - val_accuracy: 0.7801\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 22ms/step - loss: 0.3926 - accuracy: 0.8022 - val_loss: 0.4451 - val_accuracy: 0.7863\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.3878 - accuracy: 0.8041 - val_loss: 0.4536 - val_accuracy: 0.7866\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 23ms/step - loss: 0.3966 - accuracy: 0.8020 - val_loss: 0.4376 - val_accuracy: 0.7783\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 21ms/step - loss: 0.3877 - accuracy: 0.8050 - val_loss: 0.4404 - val_accuracy: 0.7895\n",
      "87/87 [==============================] - 1s 4ms/step\n",
      "accuracy 0.7895117540687161\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "5      -0s:36s        4608    False  0.793490\n",
      "6      -0s:42s        4608    False  0.794937\n",
      "7      -0s:48s        4608    False  0.784448\n",
      "8      -0s:54s        4608    False  0.794213\n",
      "9      -0s:60s        4608    False  0.790235\n",
      "10     -0s:66s        4608    False  0.777577\n",
      "11     -0s:72s        4608    False  0.799277\n",
      "12     -0s:78s        4608    False  0.796745\n",
      "13     -0s:84s        4608    False  0.783725\n",
      "14     -0s:90s        4608    False  0.784810\n",
      "15     -0s:96s        4608    False  0.787703\n",
      "16    -0s:102s        4608    False  0.786980\n",
      "17    -0s:108s        4608    False  0.786980\n",
      "18    -0s:114s        4608    False  0.788427\n",
      "19    -0s:120s        4608    False  0.792767\n",
      "20    -0s:126s        4608    False  0.781555\n",
      "21    -0s:132s        4608    False  0.789150\n",
      "22    -0s:138s        4608    False  0.781555\n",
      "23    -0s:144s        4608    False  0.789873\n",
      "24    -0s:150s        4608    False  0.771067\n",
      "25    -0s:156s        4608    False  0.780108\n",
      "26    -0s:162s        4608    False  0.779747\n",
      "27    -0s:168s        4608    False  0.781555\n",
      "28    -0s:174s        4608    False  0.784087\n",
      "29    -0s:180s        4608    False  0.773960\n",
      "30    -0s:186s        4608    False  0.781193\n",
      "31     -60s:6s        4608    False  0.574322\n",
      "32    -60s:12s        4608    False  0.577215\n",
      "33    -60s:18s        4608    False  0.580832\n",
      "34    -60s:24s        4608    False  0.620253\n",
      "35    -60s:30s        4608    False  0.748282\n",
      "36    -60s:36s        4608    False  0.769982\n",
      "37    -60s:42s        4608    False  0.793852\n",
      "38    -60s:48s        4608    False  0.791320\n",
      "39    -60s:54s        4608    False  0.779747\n",
      "40    -60s:60s        4608    False  0.785895\n",
      "41    -60s:66s        4608    False  0.779024\n",
      "42    -60s:72s        4608    False  0.793128\n",
      "43    -60s:78s        4608    False  0.773237\n",
      "44    -60s:84s        4608    False  0.786980\n",
      "45    -60s:90s        4608    False  0.793490\n",
      "46    -60s:96s        4608    False  0.785895\n",
      "47   -60s:102s        4608    False  0.785533\n",
      "48   -60s:108s        4608    False  0.779024\n",
      "49   -60s:114s        4608    False  0.786980\n",
      "50   -60s:120s        4608    False  0.785895\n",
      "51   -60s:126s        4608    False  0.788065\n",
      "52   -60s:132s        4608    False  0.780470\n",
      "53   -60s:138s        4608    False  0.786980\n",
      "54   -60s:144s        4608    False  0.787342\n",
      "55   -60s:150s        4608    False  0.791682\n",
      "56   -60s:156s        4608    False  0.781555\n",
      "57   -60s:162s        4608    False  0.785172\n",
      "58   -60s:168s        4608    False  0.780832\n",
      "59   -60s:174s        4608    False  0.789512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 240, 13)\n",
      "(9216,)\n",
      "(6451, 240, 13)\n",
      "(2765, 240, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_61 (Flatten)        (None, 3120)              0         \n",
      "                                                                 \n",
      " batch_normalization_61 (Ba  (None, 3120)              12480     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 256)               798976    \n",
      "                                                                 \n",
      " dropout_183 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_245 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_184 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_185 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_61 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 861122 (3.28 MB)\n",
      "Trainable params: 854882 (3.26 MB)\n",
      "Non-trainable params: 6240 (24.38 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 9s 27ms/step - loss: 0.7131 - accuracy: 0.5734 - val_loss: 0.6297 - val_accuracy: 0.6495\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.6192 - accuracy: 0.6495 - val_loss: 0.6065 - val_accuracy: 0.6662\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5719 - accuracy: 0.6969 - val_loss: 0.5395 - val_accuracy: 0.7114\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5378 - accuracy: 0.7154 - val_loss: 0.5301 - val_accuracy: 0.7143\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5177 - accuracy: 0.7334 - val_loss: 0.4985 - val_accuracy: 0.7327\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5006 - accuracy: 0.7452 - val_loss: 0.4866 - val_accuracy: 0.7436\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4919 - accuracy: 0.7532 - val_loss: 0.4996 - val_accuracy: 0.7374\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4794 - accuracy: 0.7607 - val_loss: 0.4626 - val_accuracy: 0.7606\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4744 - accuracy: 0.7625 - val_loss: 0.4643 - val_accuracy: 0.7675\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4619 - accuracy: 0.7758 - val_loss: 0.4632 - val_accuracy: 0.7685\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4504 - accuracy: 0.7799 - val_loss: 0.4556 - val_accuracy: 0.7729\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4545 - accuracy: 0.7696 - val_loss: 0.4603 - val_accuracy: 0.7671\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4443 - accuracy: 0.7754 - val_loss: 0.4390 - val_accuracy: 0.7826\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4371 - accuracy: 0.7844 - val_loss: 0.4620 - val_accuracy: 0.7711\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4352 - accuracy: 0.7845 - val_loss: 0.4429 - val_accuracy: 0.7783\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4365 - accuracy: 0.7855 - val_loss: 0.4342 - val_accuracy: 0.7837\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4206 - accuracy: 0.7927 - val_loss: 0.4478 - val_accuracy: 0.7754\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4334 - accuracy: 0.7873 - val_loss: 0.4416 - val_accuracy: 0.7769\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4298 - accuracy: 0.7845 - val_loss: 0.4351 - val_accuracy: 0.7863\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4159 - accuracy: 0.7994 - val_loss: 0.4350 - val_accuracy: 0.7837\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4123 - accuracy: 0.7921 - val_loss: 0.4473 - val_accuracy: 0.7866\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4119 - accuracy: 0.7988 - val_loss: 0.4355 - val_accuracy: 0.7797\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4041 - accuracy: 0.7951 - val_loss: 0.4494 - val_accuracy: 0.7834\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4023 - accuracy: 0.7991 - val_loss: 0.4596 - val_accuracy: 0.7834\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.4004 - accuracy: 0.7993 - val_loss: 0.4345 - val_accuracy: 0.7863\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.3957 - accuracy: 0.8008 - val_loss: 0.4559 - val_accuracy: 0.7790\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 5s 24ms/step - loss: 0.3978 - accuracy: 0.7972 - val_loss: 0.4364 - val_accuracy: 0.7812\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3907 - accuracy: 0.7988 - val_loss: 0.4440 - val_accuracy: 0.7892\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3924 - accuracy: 0.7963 - val_loss: 0.4477 - val_accuracy: 0.7848\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3910 - accuracy: 0.8039 - val_loss: 0.4394 - val_accuracy: 0.7881\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "accuracy 0.7880650994575045\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "..         ...         ...      ...       ...\n",
      "56   -60s:156s        4608    False  0.781555\n",
      "57   -60s:162s        4608    False  0.785172\n",
      "58   -60s:168s        4608    False  0.780832\n",
      "59   -60s:174s        4608    False  0.789512\n",
      "60   -60s:180s        4608    False  0.788065\n",
      "\n",
      "[61 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nº looms: 4608\n",
      "nº no looms: 4608\n",
      "(9216, 246, 13)\n",
      "(9216,)\n",
      "(6451, 246, 13)\n",
      "(2765, 246, 13)\n",
      "(6451,)\n",
      "(2765,)\n",
      "Train set:\n",
      "0.0 occurs 3225 times\n",
      "1.0 occurs 3226 times\n",
      "Test set:\n",
      "0.0 occurs 1383 times\n",
      "1.0 occurs 1382 times\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_62 (Flatten)        (None, 3198)              0         \n",
      "                                                                 \n",
      " batch_normalization_62 (Ba  (None, 3198)              12792     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_248 (Dense)           (None, 256)               818944    \n",
      "                                                                 \n",
      " dropout_186 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_187 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_250 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_188 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_251 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_62 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 881402 (3.36 MB)\n",
      "Trainable params: 875006 (3.34 MB)\n",
      "Non-trainable params: 6396 (24.98 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 9s 27ms/step - loss: 0.7095 - accuracy: 0.5849 - val_loss: 0.6175 - val_accuracy: 0.6430\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.6181 - accuracy: 0.6549 - val_loss: 0.5816 - val_accuracy: 0.6864\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.5787 - accuracy: 0.6932 - val_loss: 0.5544 - val_accuracy: 0.6951\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.5449 - accuracy: 0.7093 - val_loss: 0.5672 - val_accuracy: 0.7027\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 6s 27ms/step - loss: 0.5236 - accuracy: 0.7276 - val_loss: 0.4985 - val_accuracy: 0.7313\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.5078 - accuracy: 0.7388 - val_loss: 0.4878 - val_accuracy: 0.7465\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4950 - accuracy: 0.7467 - val_loss: 0.4988 - val_accuracy: 0.7363\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4852 - accuracy: 0.7509 - val_loss: 0.4809 - val_accuracy: 0.7570\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4760 - accuracy: 0.7645 - val_loss: 0.4665 - val_accuracy: 0.7664\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4600 - accuracy: 0.7729 - val_loss: 0.4683 - val_accuracy: 0.7609\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4565 - accuracy: 0.7720 - val_loss: 0.4609 - val_accuracy: 0.7638\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4554 - accuracy: 0.7735 - val_loss: 0.4632 - val_accuracy: 0.7682\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4479 - accuracy: 0.7762 - val_loss: 0.4478 - val_accuracy: 0.7754\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4422 - accuracy: 0.7780 - val_loss: 0.4576 - val_accuracy: 0.7693\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4382 - accuracy: 0.7853 - val_loss: 0.4467 - val_accuracy: 0.7754\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4432 - accuracy: 0.7783 - val_loss: 0.4428 - val_accuracy: 0.7844\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4249 - accuracy: 0.7893 - val_loss: 0.4401 - val_accuracy: 0.7787\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4286 - accuracy: 0.7873 - val_loss: 0.4433 - val_accuracy: 0.7754\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4234 - accuracy: 0.7903 - val_loss: 0.4481 - val_accuracy: 0.7873\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 6s 27ms/step - loss: 0.4197 - accuracy: 0.7898 - val_loss: 0.4332 - val_accuracy: 0.7758\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4129 - accuracy: 0.7921 - val_loss: 0.4443 - val_accuracy: 0.7881\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.4090 - accuracy: 0.7965 - val_loss: 0.4349 - val_accuracy: 0.7834\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4055 - accuracy: 0.7957 - val_loss: 0.4421 - val_accuracy: 0.7826\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 6s 26ms/step - loss: 0.4032 - accuracy: 0.7974 - val_loss: 0.4423 - val_accuracy: 0.7895\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3966 - accuracy: 0.8020 - val_loss: 0.4358 - val_accuracy: 0.7819\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3963 - accuracy: 0.7974 - val_loss: 0.4405 - val_accuracy: 0.7722\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 6s 27ms/step - loss: 0.3941 - accuracy: 0.8010 - val_loss: 0.4379 - val_accuracy: 0.7758\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3925 - accuracy: 0.8033 - val_loss: 0.4469 - val_accuracy: 0.7881\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3880 - accuracy: 0.8024 - val_loss: 0.4369 - val_accuracy: 0.7863\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 5s 25ms/step - loss: 0.3862 - accuracy: 0.8058 - val_loss: 0.4366 - val_accuracy: 0.7855\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "accuracy 0.7855334538878843\n",
      "   Time_Window Nº of Looms Freezing  Accuracy\n",
      "0       -0s:6s        4608    False  0.577577\n",
      "1      -0s:12s        4608    False  0.581917\n",
      "2      -0s:18s        4608    False  0.589512\n",
      "3      -0s:24s        4608    False  0.634358\n",
      "4      -0s:30s        4608    False  0.770344\n",
      "..         ...         ...      ...       ...\n",
      "57   -60s:162s        4608    False  0.785172\n",
      "58   -60s:168s        4608    False  0.780832\n",
      "59   -60s:174s        4608    False  0.789512\n",
      "60   -60s:180s        4608    False  0.788065\n",
      "61   -60s:186s        4608    False  0.785533\n",
      "\n",
      "[62 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n",
      "C:\\Users\\catar\\AppData\\Local\\Temp\\ipykernel_11224\\1324064191.py:213: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = results_df.append(new_line, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_change = 0\n",
    "while n_change < 181: \n",
    "\n",
    "\n",
    "    #CHANGE: \n",
    "    number_looms_per_video = 3\n",
    "    number_of_frames_per_loom = 6 + n_change #Frames after loom onset\n",
    "    number_of_frames_before =  60 #Frames before loom onset\n",
    "    incorrect_loom = 15\n",
    "    freezing_state = False  #True: exclude freezing; False: include freezing\n",
    "    same_number_of_freezing = False #True: put the same number of freezing for the loom and no loom; False: Keep everything normal\n",
    "    max_freeze = 1\n",
    "    ####\n",
    "\n",
    "\n",
    "    #GET THE LOOM PARTS\n",
    "    k = 0 \n",
    "    number_of_freezes_loom = 0 \n",
    "    loom_appenings = []\n",
    "    freezing_ones = []\n",
    "    new_loom_matrix = []\n",
    "\n",
    "    while k < len(coordinates) and len(loom_appenings) < 5000:\n",
    "        n = 0\n",
    "        n_looms = 0 \n",
    "        while n < len(coordinates[k][:,1]) and n_looms < number_looms_per_video and len(loom_appenings) < 5000  : \n",
    "            \n",
    "            if freezing_state == False:\n",
    "                \n",
    "                if coordinates[k][n,1] == 0 and coordinates[k][n + incorrect_loom ,1] == 0: #To make sure that random 0s (not a loom) don't interfere\n",
    "                    loom_appenings.append(list(range(n - number_of_frames_before , n + number_of_frames_per_loom ))) \n",
    "                    l = 0\n",
    "                    while l < (number_of_frames_before + number_of_frames_per_loom):\n",
    "                        new_loom_matrix.append(coordinates[k][n - number_of_frames_before + l ])\n",
    "                        l = l + 1                                                   \n",
    "                                                                                    \n",
    "                    if any(freezing_files[k][i,15] == 1 for i in range(n - number_of_frames_before, n + number_of_frames_per_loom)): \n",
    "                        freezing_ones.append(list(range(n - number_of_frames_before , n + number_of_frames_per_loom )))\n",
    "                        number_of_freezes_loom = number_of_freezes_loom + 1 \n",
    "                    \n",
    "                    n = n + 100\n",
    "                        \n",
    "                    n_looms = n_looms + 1\n",
    "                                            \n",
    "            else: \n",
    "                if coordinates[k][n,1] == 0 and coordinates[k][n + incorrect_loom ,1] == 0  :  #To make sure that random 0s (not a loom) don't interfere\n",
    "                    if sum(freezing_files[k][i, 15] for i in range(n - number_of_frames_before, n + number_of_frames_per_loom)) < max_freeze:\n",
    "                        loom_appenings.append(list(range(n - number_of_frames_before , n + number_of_frames_per_loom ))) \n",
    "                        l = 0\n",
    "                        while l < (number_of_frames_before + number_of_frames_per_loom):\n",
    "                            new_loom_matrix.append(coordinates[k][n - number_of_frames_before + l ])\n",
    "                            l = l + 1    \n",
    "                        n_looms = n_looms + 1\n",
    "                    n = n + 100\n",
    "                \n",
    "            n = n + 1\n",
    "        k = k + 1\n",
    "\n",
    "    loom_matrix = np.array(new_loom_matrix).reshape(len(loom_appenings), number_of_frames_per_loom + number_of_frames_before  , coordinates[0].shape[1]) # * Number of looms x Frames per loom x Columns \n",
    "    loom_matrix2 = loom_matrix[: ,:, 2:] #Take out 'FrameIndex','VisualStim' from the inputs \n",
    "\n",
    "\n",
    "    print(\"nº looms:\", len(loom_appenings))\n",
    "\n",
    "\n",
    "\n",
    "    #GET NO LOOM PARTS IN BETWEEN THE LOOMS\n",
    "\n",
    "\n",
    "    start_of_looms = []\n",
    "    k = 0 \n",
    "    no_loom_appenings = []\n",
    "    number_of_freezes_noloom = 0 \n",
    "    new_no_loom_matrix = []\n",
    "\n",
    "    while k  < len(coordinates) and len(new_no_loom_matrix) < len(new_loom_matrix): \n",
    "        n = 0\n",
    "        n_looms = 0 \n",
    "        while n < len(coordinates[k][:,1]) and n_looms < (number_looms_per_video ) and len(new_no_loom_matrix) < len(new_loom_matrix) : #sometimes it is needed to get more data per file, just sum (ex: number_looms_per_video + 2)\n",
    "            \n",
    "            if freezing_state == False:\n",
    "                if coordinates[k][n,1] == 0 and coordinates[k][n + 10,1] == 0:  #To make sure that random 0s (not a loom) don't interfere\n",
    "                    start_of_looms.append(n) \n",
    "                    if len(start_of_looms) > 1: #The first No Loom has to be between the first and second looms so we have to wait to have information about the second loom \n",
    "                        n_looms = n_looms + 1\n",
    "                        start_position = int((start_of_looms[n_looms] + start_of_looms[n_looms - 1] ) / 2) #We want the No Loom to start halfway between looms\n",
    "        \n",
    "                        if same_number_of_freezing == True: #Making sure we have the same number of freezing data as the Looms \n",
    "                            if number_of_freezes_loom <= number_of_freezes_noloom: \n",
    "                                if all(freezing_files[k][i,15] == 0 for i in range(start_position, start_position + number_of_frames_per_loom + number_of_frames_before)):\n",
    "                                    no_loom_appenings.append(list(range(  start_position  , start_position + number_of_frames_per_loom + number_of_frames_before  )))\n",
    "                                    l = 0\n",
    "                                    while l < (number_of_frames_before + number_of_frames_per_loom):\n",
    "                                        new_no_loom_matrix.append(coordinates[k][start_position + l ])\n",
    "                                        l = l + 1    \n",
    "\n",
    "                            if number_of_freezes_loom > number_of_freezes_noloom: \n",
    "                                no_loom_appenings.append(list(range(  start_position  , start_position + number_of_frames_per_loom + number_of_frames_before  )))\n",
    "                                l = 0\n",
    "                                while l < (number_of_frames_before + number_of_frames_per_loom):\n",
    "                                    new_no_loom_matrix.append(coordinates[k][start_position + l ])\n",
    "                                    l = l + 1  \n",
    "                        else:\n",
    "                            no_loom_appenings.append(list(range(  start_position  , start_position + number_of_frames_per_loom + number_of_frames_before  )))\n",
    "                            l = 0\n",
    "                            while l < (number_of_frames_before + number_of_frames_per_loom):\n",
    "                                new_no_loom_matrix.append(coordinates[k][start_position + l ])\n",
    "                                l = l + 1  \n",
    "                            \n",
    "                    n = n + 100\n",
    "            else: \n",
    "                if coordinates[k][n,1] == 0 and coordinates[k][n + 10,1] == 0 :  #To make sure that random 0s (not a loom) don't interfere\n",
    "                    start_of_looms.append(n) \n",
    "                    if len(start_of_looms) > 1: #The first No Loom has to be between the first and second looms so we have to wait to have information about the second loom \n",
    "                        n_looms = n_looms + 1\n",
    "                        start_position = int((start_of_looms[n_looms] + start_of_looms[n_looms - 1] ) / 2) #We want the No Loom to start halfway between looms\n",
    "                        if sum(freezing_files[k][i, 15] for i in range(n - number_of_frames_before, n + number_of_frames_per_loom)) < max_freeze:\n",
    "                            no_loom_appenings.append(list(range(  start_position  , start_position + number_of_frames_per_loom + number_of_frames_before  )))\n",
    "                            l = 0\n",
    "                            while l < (number_of_frames_before + number_of_frames_per_loom):\n",
    "                                new_no_loom_matrix.append(coordinates[k][start_position + l ])\n",
    "                                l = l + 1   \n",
    "                        \n",
    "                    n = n + 100\n",
    "                    \n",
    "            n = n + 1\n",
    "                \n",
    "        k = k + 1\n",
    "\n",
    "    no_loom_matrix = np.array(new_no_loom_matrix).reshape(len(no_loom_appenings), number_of_frames_per_loom  + number_of_frames_before , coordinates[0].shape[1]) # * Number of looms x Frames per loom x Columns \n",
    "    no_loom_matrix2 = no_loom_matrix[: ,:, 2:]\n",
    "\n",
    "\n",
    "    print(\"nº no looms:\", len(no_loom_appenings))\n",
    "\n",
    "    if same_number_of_freezing == True: \n",
    "        print(\"no freezing looms\" , number_of_freezes_loom)\n",
    "        print(\"no freezing no looms\" , number_of_freezes_noloom)\n",
    "\n",
    "\n",
    "    #CLASSIFY LOOMS AND NO LOOMS AND PUT IT ALL TOGETHER IN A MATRIX \n",
    "    vector_of_1s = np.ones(len(loom_appenings)) #Classify looms as 1 \n",
    "    vector_of_0s = np.zeros(len(no_loom_appenings))  #Classify no looms as 0\n",
    "\n",
    "    #Put together \n",
    "    together_x = np.concatenate((loom_matrix2, no_loom_matrix2), axis = 0 )\n",
    "    together_y = np.concatenate((vector_of_1s,vector_of_0s), axis = 0 )\n",
    "\n",
    "    #Shuffle\n",
    "    np.random.seed(42)  \n",
    "    shuffled_indices = np.random.permutation(len(together_y))\n",
    "\n",
    "    x=together_x[shuffled_indices]\n",
    "    y=together_y[shuffled_indices]\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "\n",
    "\n",
    "    #TRAIN AND TEST SPLIT\n",
    "    x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.30, random_state=4, stratify = y ) #Stratify = balance classes\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "    # See how unbaleced the classes are - for train \n",
    "    print(\"Train set:\")\n",
    "    unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "    for value, count in zip(unique_values, counts):\n",
    "        print(f\"{value} occurs {count} times\")\n",
    "\n",
    "    # See how unbaleced the classes are - for test \n",
    "    print(\"Test set:\")\n",
    "    unique_values1, counts1 = np.unique(y_test, return_counts=True)\n",
    "    for value, count in zip(unique_values1, counts1):\n",
    "        print(f\"{value} occurs {count} times\")\n",
    "\n",
    "\n",
    "    #BUILD AND RUN THE MODEL - GETTING THE ACCURACY VALUES WITH DEFINED EPOCH NUMBER AND BATCH SIZE\n",
    "    input_size = number_of_frames_per_loom + number_of_frames_before\n",
    "    train_Y_one_hot = to_categorical(y_train)\n",
    "    test_Y_one_hot = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(input_size,13)))\n",
    "    model.add(keras.layers.BatchNormalization())  \n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dropout(0.2)) #vary dropout see if gets better or worse\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, kernel_regularizer=keras.regularizers.l2(0.02))) #ridge - loss = l2 * reduce_sum(square(x))  \n",
    "\n",
    "    model.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy']) \n",
    "    model.build()\n",
    "    model.summary()\n",
    "\n",
    "    model_train = model.fit(x_train, train_Y_one_hot, batch_size = 30, epochs = 30 , verbose = 1, validation_data=(x_test, test_Y_one_hot))\n",
    "    predicted_classes = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(np.round(predicted_classes), axis=1)\n",
    "    acc = accuracy_score(y_test, predicted_classes)\n",
    "    print(\"accuracy\", acc) \n",
    "\n",
    "\n",
    "    #Adding the results to the table: \n",
    "    new_line = {'Time_Window': '-' + str(number_of_frames_before) + 's:' + str(number_of_frames_per_loom) + 's', 'Nº of Looms' : len(loom_appenings),  'Freezing': freezing_state, 'Accuracy': acc}\n",
    "    if not any((results_df['Time_Window'] == new_line['Time_Window']) & (results_df['Freezing'] == new_line['Freezing']) & (results_df['Nº of Looms'] == new_line['Nº of Looms']) & (results_df['Accuracy'] == new_line['Accuracy'])):\n",
    "        results_df = results_df.append(new_line, ignore_index=True)\n",
    "    print(results_df)\n",
    "    \n",
    "    n_change = n_change + 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING AND TESTING - FINDING THE NUMBER OF EPOCHS \n",
    "acc = 0\n",
    "i=0\n",
    "test_accuracy_per_epoch = []\n",
    "train_loss_per_epoch = []\n",
    "while i < 100:\n",
    "    model_train1 = model.fit(x_train, train_Y_one_hot, batch_size = 10, epochs = 1 ,verbose=1, validation_data=(x_test, test_Y_one_hot))\n",
    "    predicted_classes = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(np.round(predicted_classes), axis=1)\n",
    "    acc = accuracy_score(y_test, predicted_classes)\n",
    "    test_accuracy_per_epoch.append(acc)\n",
    "    train_loss_per_epoch.append(model_train1.history['loss'][0])\n",
    "    print(acc)\n",
    "    print('epoch no', i)\n",
    "    i+=1\n",
    "    \n",
    "#PLOT FOR FINDING PERFECT NUMBER OF EPOCHS \n",
    "plt.plot(range(1, 101), test_accuracy_per_epoch)\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy Plot for [0.5s : 2s] Time Window')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Batch size of the highest accuracy:\", test_accuracy_per_epoch.index(max(test_accuracy_per_epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "6451/6451 [==============================] - 58s 9ms/step - loss: 0.6941 - accuracy: 0.5021 - val_loss: 0.6934 - val_accuracy: 0.4998\n",
      "Epoch 2/30\n",
      "6451/6451 [==============================] - 58s 9ms/step - loss: 0.6934 - accuracy: 0.4866 - val_loss: 0.6931 - val_accuracy: 0.4998\n",
      "Epoch 3/30\n",
      "6451/6451 [==============================] - 56s 9ms/step - loss: 0.6932 - accuracy: 0.5088 - val_loss: 0.6934 - val_accuracy: 0.5002\n",
      "Epoch 4/30\n",
      "6451/6451 [==============================] - 56s 9ms/step - loss: 0.6932 - accuracy: 0.5015 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 5/30\n",
      "6451/6451 [==============================] - 52s 8ms/step - loss: 0.6932 - accuracy: 0.5026 - val_loss: 0.6932 - val_accuracy: 0.5013\n",
      "Epoch 6/30\n",
      "6451/6451 [==============================] - 49s 8ms/step - loss: 0.6934 - accuracy: 0.4905 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 7/30\n",
      "6451/6451 [==============================] - 51s 8ms/step - loss: 0.6933 - accuracy: 0.5024 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 8/30\n",
      "6451/6451 [==============================] - 49s 8ms/step - loss: 0.6933 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 9/30\n",
      "6451/6451 [==============================] - 49s 8ms/step - loss: 0.6933 - accuracy: 0.4939 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 10/30\n",
      "6451/6451 [==============================] - 49s 8ms/step - loss: 0.6933 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 11/30\n",
      "6451/6451 [==============================] - 51s 8ms/step - loss: 0.6933 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 12/30\n",
      "6451/6451 [==============================] - 50s 8ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 13/30\n",
      "6451/6451 [==============================] - 49s 8ms/step - loss: 0.6933 - accuracy: 0.4957 - val_loss: 0.6931 - val_accuracy: 0.5002\n",
      "Epoch 14/30\n",
      "6451/6451 [==============================] - 50s 8ms/step - loss: 0.6934 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 15/30\n",
      "6451/6451 [==============================] - 51s 8ms/step - loss: 0.6934 - accuracy: 0.4931 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 16/30\n",
      "6451/6451 [==============================] - 49s 8ms/step - loss: 0.6933 - accuracy: 0.5035 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 17/30\n",
      "6451/6451 [==============================] - 50s 8ms/step - loss: 0.6933 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 18/30\n",
      "6451/6451 [==============================] - 49s 8ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 19/30\n",
      "6451/6451 [==============================] - 50s 8ms/step - loss: 0.6933 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 20/30\n",
      "6451/6451 [==============================] - 53s 8ms/step - loss: 0.6933 - accuracy: 0.4999 - val_loss: 0.6933 - val_accuracy: 0.5002\n",
      "Epoch 21/30\n",
      "6451/6451 [==============================] - 54s 8ms/step - loss: 0.6933 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.4998\n",
      "Epoch 22/30\n",
      "6451/6451 [==============================] - 55s 9ms/step - loss: 0.6933 - accuracy: 0.4979 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 23/30\n",
      "6451/6451 [==============================] - 56s 9ms/step - loss: 0.6933 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 24/30\n",
      "6451/6451 [==============================] - 51s 8ms/step - loss: 0.6934 - accuracy: 0.4947 - val_loss: 0.6932 - val_accuracy: 0.4976\n",
      "Epoch 25/30\n",
      "6451/6451 [==============================] - 51s 8ms/step - loss: 0.6934 - accuracy: 0.4888 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 26/30\n",
      "6451/6451 [==============================] - 50s 8ms/step - loss: 0.6933 - accuracy: 0.4940 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 27/30\n",
      "6451/6451 [==============================] - 48s 7ms/step - loss: 0.6934 - accuracy: 0.4954 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 28/30\n",
      "6451/6451 [==============================] - 48s 7ms/step - loss: 0.6934 - accuracy: 0.4937 - val_loss: 0.6931 - val_accuracy: 0.5002\n",
      "Epoch 29/30\n",
      "6451/6451 [==============================] - 50s 8ms/step - loss: 0.6933 - accuracy: 0.4981 - val_loss: 0.6932 - val_accuracy: 0.5002\n",
      "Epoch 30/30\n",
      "6451/6451 [==============================] - 51s 8ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.49981916817359856\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "3226/3226 [==============================] - 29s 9ms/step - loss: 0.7093 - accuracy: 0.4995 - val_loss: 0.6933 - val_accuracy: 0.5013\n",
      "Epoch 2/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6989 - accuracy: 0.5055 - val_loss: 0.6957 - val_accuracy: 0.5042\n",
      "Epoch 3/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6984 - accuracy: 0.5055 - val_loss: 0.7093 - val_accuracy: 0.5367\n",
      "Epoch 4/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6958 - accuracy: 0.5156 - val_loss: 0.6884 - val_accuracy: 0.5620\n",
      "Epoch 5/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6941 - accuracy: 0.5289 - val_loss: 0.6837 - val_accuracy: 0.5363\n",
      "Epoch 6/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6911 - accuracy: 0.5227 - val_loss: 0.6687 - val_accuracy: 0.5761\n",
      "Epoch 7/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6809 - accuracy: 0.5433 - val_loss: 0.6807 - val_accuracy: 0.5917\n",
      "Epoch 8/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6766 - accuracy: 0.5718 - val_loss: 0.6492 - val_accuracy: 0.6347\n",
      "Epoch 9/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6735 - accuracy: 0.5767 - val_loss: 0.6755 - val_accuracy: 0.6228\n",
      "Epoch 10/30\n",
      "3226/3226 [==============================] - 27s 9ms/step - loss: 0.6747 - accuracy: 0.5742 - val_loss: 0.6890 - val_accuracy: 0.5906\n",
      "Epoch 11/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6723 - accuracy: 0.5780 - val_loss: 0.7018 - val_accuracy: 0.6362\n",
      "Epoch 12/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6749 - accuracy: 0.5779 - val_loss: 0.6662 - val_accuracy: 0.6293\n",
      "Epoch 13/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6730 - accuracy: 0.5771 - val_loss: 0.6809 - val_accuracy: 0.6329\n",
      "Epoch 14/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6725 - accuracy: 0.5749 - val_loss: 0.6863 - val_accuracy: 0.6141\n",
      "Epoch 15/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6711 - accuracy: 0.5762 - val_loss: 0.6875 - val_accuracy: 0.6166\n",
      "Epoch 16/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6665 - accuracy: 0.5866 - val_loss: 0.7488 - val_accuracy: 0.6503\n",
      "Epoch 17/30\n",
      "3226/3226 [==============================] - 30s 9ms/step - loss: 0.6633 - accuracy: 0.5864 - val_loss: 0.7948 - val_accuracy: 0.6087\n",
      "Epoch 18/30\n",
      "3226/3226 [==============================] - 30s 9ms/step - loss: 0.6671 - accuracy: 0.5867 - val_loss: 0.6747 - val_accuracy: 0.6051\n",
      "Epoch 19/30\n",
      "3226/3226 [==============================] - 29s 9ms/step - loss: 0.6640 - accuracy: 0.5729 - val_loss: 0.7571 - val_accuracy: 0.6127\n",
      "Epoch 20/30\n",
      "3226/3226 [==============================] - 29s 9ms/step - loss: 0.6706 - accuracy: 0.5765 - val_loss: 0.8125 - val_accuracy: 0.6148\n",
      "Epoch 21/30\n",
      "3226/3226 [==============================] - 29s 9ms/step - loss: 0.6691 - accuracy: 0.5705 - val_loss: 0.6828 - val_accuracy: 0.6246\n",
      "Epoch 22/30\n",
      "3226/3226 [==============================] - 29s 9ms/step - loss: 0.6674 - accuracy: 0.5864 - val_loss: 0.8517 - val_accuracy: 0.6069\n",
      "Epoch 23/30\n",
      "3226/3226 [==============================] - 30s 9ms/step - loss: 0.6649 - accuracy: 0.5839 - val_loss: 0.6663 - val_accuracy: 0.6383\n",
      "Epoch 24/30\n",
      "3226/3226 [==============================] - 28s 9ms/step - loss: 0.6612 - accuracy: 0.5956 - val_loss: 0.7437 - val_accuracy: 0.6148\n",
      "Epoch 25/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6676 - accuracy: 0.5805 - val_loss: 1.0029 - val_accuracy: 0.6289\n",
      "Epoch 26/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6665 - accuracy: 0.5776 - val_loss: 0.7135 - val_accuracy: 0.6177\n",
      "Epoch 27/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6658 - accuracy: 0.5847 - val_loss: 0.7602 - val_accuracy: 0.6289\n",
      "Epoch 28/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6641 - accuracy: 0.5892 - val_loss: 0.6639 - val_accuracy: 0.6040\n",
      "Epoch 29/30\n",
      "3226/3226 [==============================] - 27s 8ms/step - loss: 0.6660 - accuracy: 0.5847 - val_loss: 0.8304 - val_accuracy: 0.6025\n",
      "Epoch 30/30\n",
      "3226/3226 [==============================] - 26s 8ms/step - loss: 0.6665 - accuracy: 0.5832 - val_loss: 0.7517 - val_accuracy: 0.6083\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.6083182640144665\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "2151/2151 [==============================] - 19s 8ms/step - loss: 0.7095 - accuracy: 0.4981 - val_loss: 0.6932 - val_accuracy: 0.5060\n",
      "Epoch 2/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6958 - accuracy: 0.5354 - val_loss: 0.6736 - val_accuracy: 0.6029\n",
      "Epoch 3/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6830 - accuracy: 0.5453 - val_loss: 0.6414 - val_accuracy: 0.6040\n",
      "Epoch 4/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6718 - accuracy: 0.5773 - val_loss: 0.6539 - val_accuracy: 0.5931\n",
      "Epoch 5/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6710 - accuracy: 0.5661 - val_loss: 0.6593 - val_accuracy: 0.6065\n",
      "Epoch 6/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6673 - accuracy: 0.5807 - val_loss: 0.6242 - val_accuracy: 0.6405\n",
      "Epoch 7/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6607 - accuracy: 0.5905 - val_loss: 0.6252 - val_accuracy: 0.6289\n",
      "Epoch 8/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6545 - accuracy: 0.6033 - val_loss: 0.6245 - val_accuracy: 0.6438\n",
      "Epoch 9/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6538 - accuracy: 0.6024 - val_loss: 0.6145 - val_accuracy: 0.6456\n",
      "Epoch 10/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6473 - accuracy: 0.6117 - val_loss: 0.6142 - val_accuracy: 0.6582\n",
      "Epoch 11/30\n",
      "2151/2151 [==============================] - 18s 9ms/step - loss: 0.6465 - accuracy: 0.6165 - val_loss: 0.6270 - val_accuracy: 0.6495\n",
      "Epoch 12/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6450 - accuracy: 0.6177 - val_loss: 0.6110 - val_accuracy: 0.6749\n",
      "Epoch 13/30\n",
      "2151/2151 [==============================] - 17s 8ms/step - loss: 0.6527 - accuracy: 0.6134 - val_loss: 0.6042 - val_accuracy: 0.6756\n",
      "Epoch 14/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6487 - accuracy: 0.6111 - val_loss: 0.6002 - val_accuracy: 0.6835\n",
      "Epoch 15/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6435 - accuracy: 0.6151 - val_loss: 0.6031 - val_accuracy: 0.6778\n",
      "Epoch 16/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6469 - accuracy: 0.6201 - val_loss: 0.6008 - val_accuracy: 0.6785\n",
      "Epoch 17/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6372 - accuracy: 0.6238 - val_loss: 0.5909 - val_accuracy: 0.6712\n",
      "Epoch 18/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6453 - accuracy: 0.6227 - val_loss: 0.5921 - val_accuracy: 0.6752\n",
      "Epoch 19/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6407 - accuracy: 0.6213 - val_loss: 0.5904 - val_accuracy: 0.6832\n",
      "Epoch 20/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6343 - accuracy: 0.6230 - val_loss: 0.5987 - val_accuracy: 0.6854\n",
      "Epoch 21/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6342 - accuracy: 0.6261 - val_loss: 0.6037 - val_accuracy: 0.6846\n",
      "Epoch 22/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6388 - accuracy: 0.6275 - val_loss: 0.5937 - val_accuracy: 0.6803\n",
      "Epoch 23/30\n",
      "2151/2151 [==============================] - 18s 9ms/step - loss: 0.6379 - accuracy: 0.6272 - val_loss: 0.5919 - val_accuracy: 0.7074\n",
      "Epoch 24/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6271 - accuracy: 0.6356 - val_loss: 0.5786 - val_accuracy: 0.6955\n",
      "Epoch 25/30\n",
      "2151/2151 [==============================] - 19s 9ms/step - loss: 0.6344 - accuracy: 0.6278 - val_loss: 0.5826 - val_accuracy: 0.6901\n",
      "Epoch 26/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6380 - accuracy: 0.6228 - val_loss: 0.5821 - val_accuracy: 0.6890\n",
      "Epoch 27/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6321 - accuracy: 0.6322 - val_loss: 0.5912 - val_accuracy: 0.7034\n",
      "Epoch 28/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6346 - accuracy: 0.6286 - val_loss: 0.6019 - val_accuracy: 0.6835\n",
      "Epoch 29/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6367 - accuracy: 0.6249 - val_loss: 0.5858 - val_accuracy: 0.6995\n",
      "Epoch 30/30\n",
      "2151/2151 [==============================] - 18s 8ms/step - loss: 0.6368 - accuracy: 0.6287 - val_loss: 0.5855 - val_accuracy: 0.7139\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7139240506329114\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1613/1613 [==============================] - 15s 9ms/step - loss: 0.7089 - accuracy: 0.5184 - val_loss: 0.6914 - val_accuracy: 0.5165\n",
      "Epoch 2/30\n",
      "1613/1613 [==============================] - 14s 8ms/step - loss: 0.6884 - accuracy: 0.5435 - val_loss: 0.6498 - val_accuracy: 0.6141\n",
      "Epoch 3/30\n",
      "1613/1613 [==============================] - 14s 8ms/step - loss: 0.6644 - accuracy: 0.5736 - val_loss: 0.6317 - val_accuracy: 0.6018\n",
      "Epoch 4/30\n",
      "1613/1613 [==============================] - 14s 8ms/step - loss: 0.6578 - accuracy: 0.5959 - val_loss: 0.6253 - val_accuracy: 0.6474\n",
      "Epoch 5/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6504 - accuracy: 0.6022 - val_loss: 0.6291 - val_accuracy: 0.6452\n",
      "Epoch 6/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6461 - accuracy: 0.6117 - val_loss: 0.5978 - val_accuracy: 0.6575\n",
      "Epoch 7/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6371 - accuracy: 0.6213 - val_loss: 0.5853 - val_accuracy: 0.6716\n",
      "Epoch 8/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6250 - accuracy: 0.6332 - val_loss: 0.5936 - val_accuracy: 0.6716\n",
      "Epoch 9/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6289 - accuracy: 0.6428 - val_loss: 0.5841 - val_accuracy: 0.6702\n",
      "Epoch 10/30\n",
      "1613/1613 [==============================] - 14s 8ms/step - loss: 0.6269 - accuracy: 0.6402 - val_loss: 0.6043 - val_accuracy: 0.6521\n",
      "Epoch 11/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6165 - accuracy: 0.6433 - val_loss: 0.5794 - val_accuracy: 0.7096\n",
      "Epoch 12/30\n",
      "1613/1613 [==============================] - 13s 8ms/step - loss: 0.6249 - accuracy: 0.6382 - val_loss: 0.5711 - val_accuracy: 0.7067\n",
      "Epoch 13/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6269 - accuracy: 0.6346 - val_loss: 0.5698 - val_accuracy: 0.7002\n",
      "Epoch 14/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6274 - accuracy: 0.6317 - val_loss: 0.5639 - val_accuracy: 0.7067\n",
      "Epoch 15/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6179 - accuracy: 0.6576 - val_loss: 0.5632 - val_accuracy: 0.6872\n",
      "Epoch 16/30\n",
      "1613/1613 [==============================] - 15s 9ms/step - loss: 0.6161 - accuracy: 0.6453 - val_loss: 0.5651 - val_accuracy: 0.6875\n",
      "Epoch 17/30\n",
      "1613/1613 [==============================] - 15s 9ms/step - loss: 0.6183 - accuracy: 0.6481 - val_loss: 0.5666 - val_accuracy: 0.6980\n",
      "Epoch 18/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6164 - accuracy: 0.6501 - val_loss: 0.5876 - val_accuracy: 0.6702\n",
      "Epoch 19/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6140 - accuracy: 0.6481 - val_loss: 0.5564 - val_accuracy: 0.7024\n",
      "Epoch 20/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6142 - accuracy: 0.6497 - val_loss: 0.5778 - val_accuracy: 0.6817\n",
      "Epoch 21/30\n",
      "1613/1613 [==============================] - 16s 10ms/step - loss: 0.6123 - accuracy: 0.6473 - val_loss: 0.5701 - val_accuracy: 0.7063\n",
      "Epoch 22/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6050 - accuracy: 0.6594 - val_loss: 0.5459 - val_accuracy: 0.7251\n",
      "Epoch 23/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6060 - accuracy: 0.6557 - val_loss: 0.5624 - val_accuracy: 0.7027\n",
      "Epoch 24/30\n",
      "1613/1613 [==============================] - 14s 8ms/step - loss: 0.6083 - accuracy: 0.6577 - val_loss: 0.5598 - val_accuracy: 0.7005\n",
      "Epoch 25/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6105 - accuracy: 0.6546 - val_loss: 0.5535 - val_accuracy: 0.7096\n",
      "Epoch 26/30\n",
      "1613/1613 [==============================] - 14s 8ms/step - loss: 0.6054 - accuracy: 0.6579 - val_loss: 0.5608 - val_accuracy: 0.6984\n",
      "Epoch 27/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6080 - accuracy: 0.6616 - val_loss: 0.5551 - val_accuracy: 0.7038\n",
      "Epoch 28/30\n",
      "1613/1613 [==============================] - 14s 9ms/step - loss: 0.6068 - accuracy: 0.6616 - val_loss: 0.5726 - val_accuracy: 0.7031\n",
      "Epoch 29/30\n",
      "1613/1613 [==============================] - 13s 8ms/step - loss: 0.6041 - accuracy: 0.6573 - val_loss: 0.5570 - val_accuracy: 0.7201\n",
      "Epoch 30/30\n",
      "1613/1613 [==============================] - 13s 8ms/step - loss: 0.6068 - accuracy: 0.6579 - val_loss: 0.5506 - val_accuracy: 0.7016\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.701627486437613\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1291/1291 [==============================] - 12s 8ms/step - loss: 0.7125 - accuracy: 0.5095 - val_loss: 0.6993 - val_accuracy: 0.5143\n",
      "Epoch 2/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6876 - accuracy: 0.5506 - val_loss: 0.6546 - val_accuracy: 0.6004\n",
      "Epoch 3/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6588 - accuracy: 0.5760 - val_loss: 0.6299 - val_accuracy: 0.6203\n",
      "Epoch 4/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6493 - accuracy: 0.6084 - val_loss: 0.6121 - val_accuracy: 0.6647\n",
      "Epoch 5/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6421 - accuracy: 0.6118 - val_loss: 0.6315 - val_accuracy: 0.6477\n",
      "Epoch 6/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6423 - accuracy: 0.6191 - val_loss: 0.5869 - val_accuracy: 0.6796\n",
      "Epoch 7/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.6276 - accuracy: 0.6380 - val_loss: 0.6114 - val_accuracy: 0.6593\n",
      "Epoch 8/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.6218 - accuracy: 0.6391 - val_loss: 0.5893 - val_accuracy: 0.6763\n",
      "Epoch 9/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6242 - accuracy: 0.6379 - val_loss: 0.5798 - val_accuracy: 0.6759\n",
      "Epoch 10/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.6206 - accuracy: 0.6450 - val_loss: 0.5774 - val_accuracy: 0.6763\n",
      "Epoch 11/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6026 - accuracy: 0.6650 - val_loss: 0.5840 - val_accuracy: 0.6868\n",
      "Epoch 12/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6133 - accuracy: 0.6484 - val_loss: 0.5600 - val_accuracy: 0.7089\n",
      "Epoch 13/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6102 - accuracy: 0.6466 - val_loss: 0.5713 - val_accuracy: 0.7042\n",
      "Epoch 14/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.6087 - accuracy: 0.6590 - val_loss: 0.5712 - val_accuracy: 0.6998\n",
      "Epoch 15/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5989 - accuracy: 0.6732 - val_loss: 0.5586 - val_accuracy: 0.6882\n",
      "Epoch 16/30\n",
      "1291/1291 [==============================] - 12s 9ms/step - loss: 0.5996 - accuracy: 0.6628 - val_loss: 0.5658 - val_accuracy: 0.6944\n",
      "Epoch 17/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.6012 - accuracy: 0.6659 - val_loss: 0.5492 - val_accuracy: 0.7125\n",
      "Epoch 18/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5984 - accuracy: 0.6645 - val_loss: 0.5752 - val_accuracy: 0.6828\n",
      "Epoch 19/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5942 - accuracy: 0.6704 - val_loss: 0.5550 - val_accuracy: 0.7034\n",
      "Epoch 20/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5908 - accuracy: 0.6697 - val_loss: 0.5538 - val_accuracy: 0.6940\n",
      "Epoch 21/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5839 - accuracy: 0.6779 - val_loss: 0.5615 - val_accuracy: 0.7052\n",
      "Epoch 22/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5841 - accuracy: 0.6717 - val_loss: 0.5381 - val_accuracy: 0.7132\n",
      "Epoch 23/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5855 - accuracy: 0.6752 - val_loss: 0.5504 - val_accuracy: 0.7060\n",
      "Epoch 24/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5869 - accuracy: 0.6756 - val_loss: 0.5464 - val_accuracy: 0.7034\n",
      "Epoch 25/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.5864 - accuracy: 0.6721 - val_loss: 0.5463 - val_accuracy: 0.7150\n",
      "Epoch 26/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.5920 - accuracy: 0.6732 - val_loss: 0.5460 - val_accuracy: 0.6984\n",
      "Epoch 27/30\n",
      "1291/1291 [==============================] - 11s 9ms/step - loss: 0.5841 - accuracy: 0.6732 - val_loss: 0.5505 - val_accuracy: 0.7212\n",
      "Epoch 28/30\n",
      "1291/1291 [==============================] - 11s 8ms/step - loss: 0.5774 - accuracy: 0.6765 - val_loss: 0.5653 - val_accuracy: 0.7107\n",
      "Epoch 29/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.5813 - accuracy: 0.6751 - val_loss: 0.5429 - val_accuracy: 0.7143\n",
      "Epoch 30/30\n",
      "1291/1291 [==============================] - 10s 8ms/step - loss: 0.5808 - accuracy: 0.6810 - val_loss: 0.5491 - val_accuracy: 0.7074\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7074141048824593\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1076/1076 [==============================] - 10s 8ms/step - loss: 0.7121 - accuracy: 0.5168 - val_loss: 0.6858 - val_accuracy: 0.5284\n",
      "Epoch 2/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6855 - accuracy: 0.5470 - val_loss: 0.6390 - val_accuracy: 0.6192\n",
      "Epoch 3/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6581 - accuracy: 0.5911 - val_loss: 0.6169 - val_accuracy: 0.6336\n",
      "Epoch 4/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6461 - accuracy: 0.6100 - val_loss: 0.6217 - val_accuracy: 0.6336\n",
      "Epoch 5/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6378 - accuracy: 0.6292 - val_loss: 0.6233 - val_accuracy: 0.6401\n",
      "Epoch 6/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6268 - accuracy: 0.6298 - val_loss: 0.5922 - val_accuracy: 0.6608\n",
      "Epoch 7/30\n",
      "1076/1076 [==============================] - 9s 9ms/step - loss: 0.6133 - accuracy: 0.6487 - val_loss: 0.5912 - val_accuracy: 0.6535\n",
      "Epoch 8/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6071 - accuracy: 0.6503 - val_loss: 0.5897 - val_accuracy: 0.6756\n",
      "Epoch 9/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6041 - accuracy: 0.6571 - val_loss: 0.5699 - val_accuracy: 0.6774\n",
      "Epoch 10/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.6011 - accuracy: 0.6638 - val_loss: 0.5712 - val_accuracy: 0.6817\n",
      "Epoch 11/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5972 - accuracy: 0.6562 - val_loss: 0.5746 - val_accuracy: 0.6915\n",
      "Epoch 12/30\n",
      "1076/1076 [==============================] - 8s 8ms/step - loss: 0.5960 - accuracy: 0.6644 - val_loss: 0.5658 - val_accuracy: 0.7027\n",
      "Epoch 13/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5947 - accuracy: 0.6707 - val_loss: 0.5644 - val_accuracy: 0.7118\n",
      "Epoch 14/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5860 - accuracy: 0.6709 - val_loss: 0.5736 - val_accuracy: 0.7024\n",
      "Epoch 15/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5878 - accuracy: 0.6738 - val_loss: 0.5629 - val_accuracy: 0.6810\n",
      "Epoch 16/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5855 - accuracy: 0.6749 - val_loss: 0.5405 - val_accuracy: 0.7291\n",
      "Epoch 17/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5856 - accuracy: 0.6762 - val_loss: 0.5480 - val_accuracy: 0.7013\n",
      "Epoch 18/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5841 - accuracy: 0.6779 - val_loss: 0.5507 - val_accuracy: 0.6966\n",
      "Epoch 19/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5767 - accuracy: 0.6762 - val_loss: 0.5488 - val_accuracy: 0.7099\n",
      "Epoch 20/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5834 - accuracy: 0.6703 - val_loss: 0.5391 - val_accuracy: 0.7107\n",
      "Epoch 21/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5756 - accuracy: 0.6873 - val_loss: 0.5436 - val_accuracy: 0.7136\n",
      "Epoch 22/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5684 - accuracy: 0.6873 - val_loss: 0.5319 - val_accuracy: 0.7089\n",
      "Epoch 23/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5735 - accuracy: 0.6828 - val_loss: 0.5391 - val_accuracy: 0.7118\n",
      "Epoch 24/30\n",
      "1076/1076 [==============================] - 9s 9ms/step - loss: 0.5688 - accuracy: 0.6895 - val_loss: 0.5356 - val_accuracy: 0.7161\n",
      "Epoch 25/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5696 - accuracy: 0.6890 - val_loss: 0.5385 - val_accuracy: 0.7060\n",
      "Epoch 26/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5683 - accuracy: 0.6938 - val_loss: 0.5464 - val_accuracy: 0.7042\n",
      "Epoch 27/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5710 - accuracy: 0.6883 - val_loss: 0.5377 - val_accuracy: 0.7230\n",
      "Epoch 28/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5661 - accuracy: 0.6948 - val_loss: 0.5484 - val_accuracy: 0.7183\n",
      "Epoch 29/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5629 - accuracy: 0.6925 - val_loss: 0.5356 - val_accuracy: 0.7165\n",
      "Epoch 30/30\n",
      "1076/1076 [==============================] - 9s 8ms/step - loss: 0.5716 - accuracy: 0.6807 - val_loss: 0.5423 - val_accuracy: 0.7081\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7081374321880651\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_7 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "922/922 [==============================] - 10s 9ms/step - loss: 0.7101 - accuracy: 0.5261 - val_loss: 0.6824 - val_accuracy: 0.5573\n",
      "Epoch 2/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6738 - accuracy: 0.5624 - val_loss: 0.6405 - val_accuracy: 0.5931\n",
      "Epoch 3/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6469 - accuracy: 0.5917 - val_loss: 0.6338 - val_accuracy: 0.6260\n",
      "Epoch 4/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6381 - accuracy: 0.6211 - val_loss: 0.6053 - val_accuracy: 0.6514\n",
      "Epoch 5/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6311 - accuracy: 0.6340 - val_loss: 0.6059 - val_accuracy: 0.6738\n",
      "Epoch 6/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6175 - accuracy: 0.6531 - val_loss: 0.5888 - val_accuracy: 0.6600\n",
      "Epoch 7/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6107 - accuracy: 0.6506 - val_loss: 0.5806 - val_accuracy: 0.6698\n",
      "Epoch 8/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6066 - accuracy: 0.6500 - val_loss: 0.5710 - val_accuracy: 0.6933\n",
      "Epoch 9/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.6042 - accuracy: 0.6562 - val_loss: 0.5638 - val_accuracy: 0.6922\n",
      "Epoch 10/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5958 - accuracy: 0.6638 - val_loss: 0.5618 - val_accuracy: 0.6879\n",
      "Epoch 11/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5971 - accuracy: 0.6621 - val_loss: 0.5625 - val_accuracy: 0.6926\n",
      "Epoch 12/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5898 - accuracy: 0.6670 - val_loss: 0.5740 - val_accuracy: 0.6919\n",
      "Epoch 13/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5883 - accuracy: 0.6717 - val_loss: 0.5594 - val_accuracy: 0.6908\n",
      "Epoch 14/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5897 - accuracy: 0.6689 - val_loss: 0.5584 - val_accuracy: 0.6875\n",
      "Epoch 15/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5742 - accuracy: 0.6805 - val_loss: 0.5621 - val_accuracy: 0.6828\n",
      "Epoch 16/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5780 - accuracy: 0.6825 - val_loss: 0.5405 - val_accuracy: 0.7092\n",
      "Epoch 17/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5764 - accuracy: 0.6768 - val_loss: 0.5469 - val_accuracy: 0.7063\n",
      "Epoch 18/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5772 - accuracy: 0.6852 - val_loss: 0.5605 - val_accuracy: 0.6854\n",
      "Epoch 19/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5728 - accuracy: 0.6771 - val_loss: 0.5448 - val_accuracy: 0.6973\n",
      "Epoch 20/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5712 - accuracy: 0.6773 - val_loss: 0.5442 - val_accuracy: 0.7002\n",
      "Epoch 21/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5648 - accuracy: 0.6875 - val_loss: 0.5478 - val_accuracy: 0.7161\n",
      "Epoch 22/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5666 - accuracy: 0.6822 - val_loss: 0.5270 - val_accuracy: 0.7212\n",
      "Epoch 23/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5639 - accuracy: 0.6895 - val_loss: 0.5344 - val_accuracy: 0.7121\n",
      "Epoch 24/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5643 - accuracy: 0.6886 - val_loss: 0.5263 - val_accuracy: 0.7226\n",
      "Epoch 25/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5587 - accuracy: 0.6887 - val_loss: 0.5390 - val_accuracy: 0.7179\n",
      "Epoch 26/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5597 - accuracy: 0.6931 - val_loss: 0.5333 - val_accuracy: 0.7139\n",
      "Epoch 27/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5652 - accuracy: 0.6928 - val_loss: 0.5335 - val_accuracy: 0.7382\n",
      "Epoch 28/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5544 - accuracy: 0.6963 - val_loss: 0.5616 - val_accuracy: 0.7110\n",
      "Epoch 29/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5620 - accuracy: 0.6890 - val_loss: 0.5486 - val_accuracy: 0.7085\n",
      "Epoch 30/30\n",
      "922/922 [==============================] - 8s 9ms/step - loss: 0.5520 - accuracy: 0.6974 - val_loss: 0.5380 - val_accuracy: 0.7049\n",
      "87/87 [==============================] - 0s 2ms/step\n",
      "0.7048824593128391\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "807/807 [==============================] - 8s 9ms/step - loss: 0.7129 - accuracy: 0.5205 - val_loss: 0.6872 - val_accuracy: 0.5497\n",
      "Epoch 2/30\n",
      "807/807 [==============================] - 7s 8ms/step - loss: 0.6754 - accuracy: 0.5687 - val_loss: 0.6505 - val_accuracy: 0.6029\n",
      "Epoch 3/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.6486 - accuracy: 0.5932 - val_loss: 0.6188 - val_accuracy: 0.6246\n",
      "Epoch 4/30\n",
      "807/807 [==============================] - 7s 8ms/step - loss: 0.6349 - accuracy: 0.6188 - val_loss: 0.5997 - val_accuracy: 0.6622\n",
      "Epoch 5/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.6261 - accuracy: 0.6322 - val_loss: 0.5932 - val_accuracy: 0.6687\n",
      "Epoch 6/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.6107 - accuracy: 0.6514 - val_loss: 0.5732 - val_accuracy: 0.6694\n",
      "Epoch 7/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.6015 - accuracy: 0.6531 - val_loss: 0.5814 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5926 - accuracy: 0.6608 - val_loss: 0.5608 - val_accuracy: 0.7024\n",
      "Epoch 9/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5939 - accuracy: 0.6633 - val_loss: 0.5682 - val_accuracy: 0.6901\n",
      "Epoch 10/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5890 - accuracy: 0.6749 - val_loss: 0.5642 - val_accuracy: 0.6763\n",
      "Epoch 11/30\n",
      "807/807 [==============================] - 7s 8ms/step - loss: 0.5825 - accuracy: 0.6768 - val_loss: 0.5598 - val_accuracy: 0.6991\n",
      "Epoch 12/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5758 - accuracy: 0.6804 - val_loss: 0.5527 - val_accuracy: 0.6951\n",
      "Epoch 13/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5792 - accuracy: 0.6752 - val_loss: 0.5508 - val_accuracy: 0.7197\n",
      "Epoch 14/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5820 - accuracy: 0.6725 - val_loss: 0.5584 - val_accuracy: 0.6926\n",
      "Epoch 15/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5676 - accuracy: 0.6929 - val_loss: 0.5530 - val_accuracy: 0.6958\n",
      "Epoch 16/30\n",
      "807/807 [==============================] - 7s 8ms/step - loss: 0.5629 - accuracy: 0.6954 - val_loss: 0.5645 - val_accuracy: 0.6922\n",
      "Epoch 17/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5650 - accuracy: 0.6901 - val_loss: 0.5384 - val_accuracy: 0.7049\n",
      "Epoch 18/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5644 - accuracy: 0.6861 - val_loss: 0.5632 - val_accuracy: 0.6857\n",
      "Epoch 19/30\n",
      "807/807 [==============================] - 7s 8ms/step - loss: 0.5576 - accuracy: 0.6990 - val_loss: 0.5338 - val_accuracy: 0.7078\n",
      "Epoch 20/30\n",
      "807/807 [==============================] - 7s 8ms/step - loss: 0.5604 - accuracy: 0.6940 - val_loss: 0.5416 - val_accuracy: 0.7150\n",
      "Epoch 21/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5585 - accuracy: 0.6932 - val_loss: 0.5450 - val_accuracy: 0.7049\n",
      "Epoch 22/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5460 - accuracy: 0.7031 - val_loss: 0.5310 - val_accuracy: 0.7161\n",
      "Epoch 23/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5519 - accuracy: 0.6990 - val_loss: 0.5325 - val_accuracy: 0.7244\n",
      "Epoch 24/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5566 - accuracy: 0.7004 - val_loss: 0.5407 - val_accuracy: 0.7038\n",
      "Epoch 25/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5517 - accuracy: 0.7019 - val_loss: 0.5325 - val_accuracy: 0.7190\n",
      "Epoch 26/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5467 - accuracy: 0.7016 - val_loss: 0.5307 - val_accuracy: 0.7107\n",
      "Epoch 27/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5400 - accuracy: 0.6976 - val_loss: 0.5297 - val_accuracy: 0.7172\n",
      "Epoch 28/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5437 - accuracy: 0.7087 - val_loss: 0.5306 - val_accuracy: 0.7233\n",
      "Epoch 29/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5479 - accuracy: 0.7033 - val_loss: 0.5278 - val_accuracy: 0.7204\n",
      "Epoch 30/30\n",
      "807/807 [==============================] - 7s 9ms/step - loss: 0.5491 - accuracy: 0.6987 - val_loss: 0.5357 - val_accuracy: 0.7114\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7113924050632912\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_9 (Flatten)         (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 1170)              4680      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "717/717 [==============================] - 8s 9ms/step - loss: 0.7154 - accuracy: 0.5191 - val_loss: 0.6856 - val_accuracy: 0.5602\n",
      "Epoch 2/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.6702 - accuracy: 0.5784 - val_loss: 0.6244 - val_accuracy: 0.6398\n",
      "Epoch 3/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.6445 - accuracy: 0.6049 - val_loss: 0.6135 - val_accuracy: 0.6452\n",
      "Epoch 4/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.6281 - accuracy: 0.6340 - val_loss: 0.5867 - val_accuracy: 0.6749\n",
      "Epoch 5/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.6158 - accuracy: 0.6459 - val_loss: 0.5750 - val_accuracy: 0.6998\n",
      "Epoch 6/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.6080 - accuracy: 0.6604 - val_loss: 0.5815 - val_accuracy: 0.6803\n",
      "Epoch 7/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5971 - accuracy: 0.6622 - val_loss: 0.5610 - val_accuracy: 0.6882\n",
      "Epoch 8/30\n",
      "717/717 [==============================] - 7s 9ms/step - loss: 0.5887 - accuracy: 0.6655 - val_loss: 0.5496 - val_accuracy: 0.7103\n",
      "Epoch 9/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5895 - accuracy: 0.6706 - val_loss: 0.5645 - val_accuracy: 0.6911\n",
      "Epoch 10/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5785 - accuracy: 0.6836 - val_loss: 0.5446 - val_accuracy: 0.7081\n",
      "Epoch 11/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5772 - accuracy: 0.6745 - val_loss: 0.5609 - val_accuracy: 0.6955\n",
      "Epoch 12/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5750 - accuracy: 0.6692 - val_loss: 0.5497 - val_accuracy: 0.7081\n",
      "Epoch 13/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5751 - accuracy: 0.6779 - val_loss: 0.5467 - val_accuracy: 0.7183\n",
      "Epoch 14/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5692 - accuracy: 0.6863 - val_loss: 0.5555 - val_accuracy: 0.6861\n",
      "Epoch 15/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5607 - accuracy: 0.6907 - val_loss: 0.5513 - val_accuracy: 0.6951\n",
      "Epoch 16/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5584 - accuracy: 0.6937 - val_loss: 0.5336 - val_accuracy: 0.7248\n",
      "Epoch 17/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5643 - accuracy: 0.6881 - val_loss: 0.5400 - val_accuracy: 0.7034\n",
      "Epoch 18/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5578 - accuracy: 0.6966 - val_loss: 0.5496 - val_accuracy: 0.6846\n",
      "Epoch 19/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5533 - accuracy: 0.6966 - val_loss: 0.5441 - val_accuracy: 0.6991\n",
      "Epoch 20/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5547 - accuracy: 0.6948 - val_loss: 0.5467 - val_accuracy: 0.7049\n",
      "Epoch 21/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5481 - accuracy: 0.6997 - val_loss: 0.5403 - val_accuracy: 0.7118\n",
      "Epoch 22/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5427 - accuracy: 0.7084 - val_loss: 0.5350 - val_accuracy: 0.7186\n",
      "Epoch 23/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5408 - accuracy: 0.7103 - val_loss: 0.5335 - val_accuracy: 0.7121\n",
      "Epoch 24/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5468 - accuracy: 0.6983 - val_loss: 0.5251 - val_accuracy: 0.7172\n",
      "Epoch 25/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5424 - accuracy: 0.7021 - val_loss: 0.5394 - val_accuracy: 0.7193\n",
      "Epoch 26/30\n",
      "717/717 [==============================] - 7s 9ms/step - loss: 0.5411 - accuracy: 0.6997 - val_loss: 0.5294 - val_accuracy: 0.7071\n",
      "Epoch 27/30\n",
      "717/717 [==============================] - 6s 9ms/step - loss: 0.5356 - accuracy: 0.7095 - val_loss: 0.5388 - val_accuracy: 0.7241\n",
      "Epoch 28/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5421 - accuracy: 0.7036 - val_loss: 0.5481 - val_accuracy: 0.7193\n",
      "Epoch 29/30\n",
      "717/717 [==============================] - 6s 8ms/step - loss: 0.5417 - accuracy: 0.7039 - val_loss: 0.5324 - val_accuracy: 0.7222\n",
      "Epoch 30/30\n",
      "717/717 [==============================] - 7s 9ms/step - loss: 0.5324 - accuracy: 0.7038 - val_loss: 0.5278 - val_accuracy: 0.7197\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7197106690777577\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_10 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "646/646 [==============================] - 7s 9ms/step - loss: 0.7134 - accuracy: 0.5376 - val_loss: 0.6887 - val_accuracy: 0.5533\n",
      "Epoch 2/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.6693 - accuracy: 0.5737 - val_loss: 0.6190 - val_accuracy: 0.6423\n",
      "Epoch 3/30\n",
      "646/646 [==============================] - 5s 8ms/step - loss: 0.6405 - accuracy: 0.6056 - val_loss: 0.6088 - val_accuracy: 0.6481\n",
      "Epoch 4/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.6306 - accuracy: 0.6213 - val_loss: 0.5858 - val_accuracy: 0.6828\n",
      "Epoch 5/30\n",
      "646/646 [==============================] - 6s 10ms/step - loss: 0.6163 - accuracy: 0.6433 - val_loss: 0.5827 - val_accuracy: 0.6835\n",
      "Epoch 6/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.6097 - accuracy: 0.6523 - val_loss: 0.5870 - val_accuracy: 0.6749\n",
      "Epoch 7/30\n",
      "646/646 [==============================] - 5s 8ms/step - loss: 0.5947 - accuracy: 0.6618 - val_loss: 0.5739 - val_accuracy: 0.6774\n",
      "Epoch 8/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5857 - accuracy: 0.6686 - val_loss: 0.5604 - val_accuracy: 0.6915\n",
      "Epoch 9/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5871 - accuracy: 0.6703 - val_loss: 0.5793 - val_accuracy: 0.6922\n",
      "Epoch 10/30\n",
      "646/646 [==============================] - 5s 8ms/step - loss: 0.5781 - accuracy: 0.6780 - val_loss: 0.5492 - val_accuracy: 0.6980\n",
      "Epoch 11/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5756 - accuracy: 0.6819 - val_loss: 0.5582 - val_accuracy: 0.6955\n",
      "Epoch 12/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5737 - accuracy: 0.6746 - val_loss: 0.5446 - val_accuracy: 0.7038\n",
      "Epoch 13/30\n",
      "646/646 [==============================] - 5s 8ms/step - loss: 0.5697 - accuracy: 0.6833 - val_loss: 0.5461 - val_accuracy: 0.7045\n",
      "Epoch 14/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5586 - accuracy: 0.6954 - val_loss: 0.5480 - val_accuracy: 0.6995\n",
      "Epoch 15/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5541 - accuracy: 0.7005 - val_loss: 0.5721 - val_accuracy: 0.6756\n",
      "Epoch 16/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5582 - accuracy: 0.6894 - val_loss: 0.5415 - val_accuracy: 0.7027\n",
      "Epoch 17/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5563 - accuracy: 0.6928 - val_loss: 0.5295 - val_accuracy: 0.7157\n",
      "Epoch 18/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5513 - accuracy: 0.6979 - val_loss: 0.5423 - val_accuracy: 0.6882\n",
      "Epoch 19/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5523 - accuracy: 0.6954 - val_loss: 0.5441 - val_accuracy: 0.7071\n",
      "Epoch 20/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5525 - accuracy: 0.6903 - val_loss: 0.5362 - val_accuracy: 0.6966\n",
      "Epoch 21/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5490 - accuracy: 0.6977 - val_loss: 0.5428 - val_accuracy: 0.7143\n",
      "Epoch 22/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5425 - accuracy: 0.7011 - val_loss: 0.5259 - val_accuracy: 0.7248\n",
      "Epoch 23/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5390 - accuracy: 0.7103 - val_loss: 0.5292 - val_accuracy: 0.7175\n",
      "Epoch 24/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5330 - accuracy: 0.7109 - val_loss: 0.5365 - val_accuracy: 0.7179\n",
      "Epoch 25/30\n",
      "646/646 [==============================] - 5s 8ms/step - loss: 0.5401 - accuracy: 0.7058 - val_loss: 0.5455 - val_accuracy: 0.7125\n",
      "Epoch 26/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5373 - accuracy: 0.7073 - val_loss: 0.5251 - val_accuracy: 0.7107\n",
      "Epoch 27/30\n",
      "646/646 [==============================] - 5s 8ms/step - loss: 0.5261 - accuracy: 0.7180 - val_loss: 0.5406 - val_accuracy: 0.7161\n",
      "Epoch 28/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5318 - accuracy: 0.7069 - val_loss: 0.5303 - val_accuracy: 0.7338\n",
      "Epoch 29/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5353 - accuracy: 0.7194 - val_loss: 0.5262 - val_accuracy: 0.7150\n",
      "Epoch 30/30\n",
      "646/646 [==============================] - 6s 9ms/step - loss: 0.5262 - accuracy: 0.7075 - val_loss: 0.5340 - val_accuracy: 0.7081\n",
      "87/87 [==============================] - 0s 2ms/step\n",
      "0.7081374321880651\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_11 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "587/587 [==============================] - 6s 9ms/step - loss: 0.7145 - accuracy: 0.5359 - val_loss: 0.6870 - val_accuracy: 0.5685\n",
      "Epoch 2/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.6636 - accuracy: 0.5852 - val_loss: 0.6179 - val_accuracy: 0.6409\n",
      "Epoch 3/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.6361 - accuracy: 0.6159 - val_loss: 0.6099 - val_accuracy: 0.6488\n",
      "Epoch 4/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.6273 - accuracy: 0.6304 - val_loss: 0.5929 - val_accuracy: 0.6767\n",
      "Epoch 5/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.6111 - accuracy: 0.6497 - val_loss: 0.5807 - val_accuracy: 0.6814\n",
      "Epoch 6/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.6006 - accuracy: 0.6658 - val_loss: 0.5677 - val_accuracy: 0.6832\n",
      "Epoch 7/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5948 - accuracy: 0.6591 - val_loss: 0.5627 - val_accuracy: 0.6799\n",
      "Epoch 8/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5813 - accuracy: 0.6777 - val_loss: 0.5499 - val_accuracy: 0.7005\n",
      "Epoch 9/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5768 - accuracy: 0.6743 - val_loss: 0.5751 - val_accuracy: 0.6872\n",
      "Epoch 10/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5763 - accuracy: 0.6787 - val_loss: 0.5502 - val_accuracy: 0.6901\n",
      "Epoch 11/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5705 - accuracy: 0.6847 - val_loss: 0.5446 - val_accuracy: 0.7034\n",
      "Epoch 12/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5605 - accuracy: 0.6876 - val_loss: 0.5416 - val_accuracy: 0.7045\n",
      "Epoch 13/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5653 - accuracy: 0.6873 - val_loss: 0.5452 - val_accuracy: 0.6984\n",
      "Epoch 14/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5602 - accuracy: 0.6937 - val_loss: 0.5462 - val_accuracy: 0.6854\n",
      "Epoch 15/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5526 - accuracy: 0.6909 - val_loss: 0.5424 - val_accuracy: 0.6955\n",
      "Epoch 16/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5479 - accuracy: 0.7007 - val_loss: 0.5415 - val_accuracy: 0.7114\n",
      "Epoch 17/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5499 - accuracy: 0.6931 - val_loss: 0.5389 - val_accuracy: 0.7067\n",
      "Epoch 18/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5497 - accuracy: 0.6991 - val_loss: 0.5413 - val_accuracy: 0.6893\n",
      "Epoch 19/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5452 - accuracy: 0.6987 - val_loss: 0.5344 - val_accuracy: 0.7114\n",
      "Epoch 20/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5434 - accuracy: 0.6977 - val_loss: 0.5365 - val_accuracy: 0.7031\n",
      "Epoch 21/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5412 - accuracy: 0.7137 - val_loss: 0.5415 - val_accuracy: 0.7175\n",
      "Epoch 22/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5347 - accuracy: 0.7067 - val_loss: 0.5273 - val_accuracy: 0.7208\n",
      "Epoch 23/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5390 - accuracy: 0.7042 - val_loss: 0.5346 - val_accuracy: 0.7139\n",
      "Epoch 24/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5328 - accuracy: 0.7033 - val_loss: 0.5270 - val_accuracy: 0.7313\n",
      "Epoch 25/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5380 - accuracy: 0.6990 - val_loss: 0.5332 - val_accuracy: 0.7168\n",
      "Epoch 26/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5313 - accuracy: 0.7142 - val_loss: 0.5245 - val_accuracy: 0.7154\n",
      "Epoch 27/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5270 - accuracy: 0.7174 - val_loss: 0.5260 - val_accuracy: 0.7219\n",
      "Epoch 28/30\n",
      "587/587 [==============================] - 5s 8ms/step - loss: 0.5327 - accuracy: 0.7070 - val_loss: 0.5350 - val_accuracy: 0.7280\n",
      "Epoch 29/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5329 - accuracy: 0.7101 - val_loss: 0.5271 - val_accuracy: 0.7226\n",
      "Epoch 30/30\n",
      "587/587 [==============================] - 5s 9ms/step - loss: 0.5230 - accuracy: 0.7154 - val_loss: 0.5312 - val_accuracy: 0.7132\n",
      "87/87 [==============================] - 0s 2ms/step\n",
      "0.7132007233273056\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_12 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "538/538 [==============================] - 6s 9ms/step - loss: 0.7170 - accuracy: 0.5319 - val_loss: 0.6871 - val_accuracy: 0.5508\n",
      "Epoch 2/30\n",
      "538/538 [==============================] - 5s 10ms/step - loss: 0.6668 - accuracy: 0.5790 - val_loss: 0.6444 - val_accuracy: 0.6141\n",
      "Epoch 3/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.6381 - accuracy: 0.6109 - val_loss: 0.6068 - val_accuracy: 0.6640\n",
      "Epoch 4/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.6238 - accuracy: 0.6348 - val_loss: 0.5872 - val_accuracy: 0.6778\n",
      "Epoch 5/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.6151 - accuracy: 0.6373 - val_loss: 0.5857 - val_accuracy: 0.6669\n",
      "Epoch 6/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5999 - accuracy: 0.6678 - val_loss: 0.5785 - val_accuracy: 0.6788\n",
      "Epoch 7/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5887 - accuracy: 0.6666 - val_loss: 0.5585 - val_accuracy: 0.6886\n",
      "Epoch 8/30\n",
      "538/538 [==============================] - 5s 8ms/step - loss: 0.5776 - accuracy: 0.6740 - val_loss: 0.5624 - val_accuracy: 0.6929\n",
      "Epoch 9/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5787 - accuracy: 0.6754 - val_loss: 0.5633 - val_accuracy: 0.6962\n",
      "Epoch 10/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5632 - accuracy: 0.6876 - val_loss: 0.5443 - val_accuracy: 0.6864\n",
      "Epoch 11/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5595 - accuracy: 0.6900 - val_loss: 0.5486 - val_accuracy: 0.7013\n",
      "Epoch 12/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5636 - accuracy: 0.6836 - val_loss: 0.5438 - val_accuracy: 0.7081\n",
      "Epoch 13/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5632 - accuracy: 0.6917 - val_loss: 0.5478 - val_accuracy: 0.7016\n",
      "Epoch 14/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5574 - accuracy: 0.6935 - val_loss: 0.5475 - val_accuracy: 0.6893\n",
      "Epoch 15/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5561 - accuracy: 0.6917 - val_loss: 0.5408 - val_accuracy: 0.6915\n",
      "Epoch 16/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5527 - accuracy: 0.6982 - val_loss: 0.5364 - val_accuracy: 0.7175\n",
      "Epoch 17/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5510 - accuracy: 0.6987 - val_loss: 0.5370 - val_accuracy: 0.7154\n",
      "Epoch 18/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5500 - accuracy: 0.7008 - val_loss: 0.5424 - val_accuracy: 0.6868\n",
      "Epoch 19/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5482 - accuracy: 0.7027 - val_loss: 0.5318 - val_accuracy: 0.7139\n",
      "Epoch 20/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5442 - accuracy: 0.7007 - val_loss: 0.5437 - val_accuracy: 0.6998\n",
      "Epoch 21/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5354 - accuracy: 0.7011 - val_loss: 0.5287 - val_accuracy: 0.7128\n",
      "Epoch 22/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5316 - accuracy: 0.7053 - val_loss: 0.5287 - val_accuracy: 0.7186\n",
      "Epoch 23/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5306 - accuracy: 0.7160 - val_loss: 0.5297 - val_accuracy: 0.7139\n",
      "Epoch 24/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5353 - accuracy: 0.7093 - val_loss: 0.5241 - val_accuracy: 0.7208\n",
      "Epoch 25/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5334 - accuracy: 0.7093 - val_loss: 0.5286 - val_accuracy: 0.7230\n",
      "Epoch 26/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5274 - accuracy: 0.7146 - val_loss: 0.5290 - val_accuracy: 0.7074\n",
      "Epoch 27/30\n",
      "538/538 [==============================] - 4s 8ms/step - loss: 0.5277 - accuracy: 0.7148 - val_loss: 0.5318 - val_accuracy: 0.7244\n",
      "Epoch 28/30\n",
      "538/538 [==============================] - 5s 8ms/step - loss: 0.5212 - accuracy: 0.7123 - val_loss: 0.5305 - val_accuracy: 0.7150\n",
      "Epoch 29/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5225 - accuracy: 0.7224 - val_loss: 0.5310 - val_accuracy: 0.7201\n",
      "Epoch 30/30\n",
      "538/538 [==============================] - 5s 9ms/step - loss: 0.5307 - accuracy: 0.7118 - val_loss: 0.5393 - val_accuracy: 0.7074\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7074141048824593\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_13 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "497/497 [==============================] - 6s 10ms/step - loss: 0.7167 - accuracy: 0.5291 - val_loss: 0.6893 - val_accuracy: 0.5533\n",
      "Epoch 2/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.6688 - accuracy: 0.5801 - val_loss: 0.6314 - val_accuracy: 0.6362\n",
      "Epoch 3/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.6404 - accuracy: 0.6070 - val_loss: 0.6192 - val_accuracy: 0.6474\n",
      "Epoch 4/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.6324 - accuracy: 0.6267 - val_loss: 0.5943 - val_accuracy: 0.6698\n",
      "Epoch 5/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.6157 - accuracy: 0.6348 - val_loss: 0.5784 - val_accuracy: 0.6832\n",
      "Epoch 6/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.6024 - accuracy: 0.6649 - val_loss: 0.5773 - val_accuracy: 0.6835\n",
      "Epoch 7/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5857 - accuracy: 0.6706 - val_loss: 0.5637 - val_accuracy: 0.6868\n",
      "Epoch 8/30\n",
      "497/497 [==============================] - 4s 8ms/step - loss: 0.5813 - accuracy: 0.6740 - val_loss: 0.5466 - val_accuracy: 0.7031\n",
      "Epoch 9/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5779 - accuracy: 0.6780 - val_loss: 0.5606 - val_accuracy: 0.6886\n",
      "Epoch 10/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5723 - accuracy: 0.6833 - val_loss: 0.5437 - val_accuracy: 0.6922\n",
      "Epoch 11/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5623 - accuracy: 0.6884 - val_loss: 0.5423 - val_accuracy: 0.6955\n",
      "Epoch 12/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5627 - accuracy: 0.6821 - val_loss: 0.5526 - val_accuracy: 0.7060\n",
      "Epoch 13/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5581 - accuracy: 0.6901 - val_loss: 0.5429 - val_accuracy: 0.7139\n",
      "Epoch 14/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5544 - accuracy: 0.6943 - val_loss: 0.5623 - val_accuracy: 0.6864\n",
      "Epoch 15/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5507 - accuracy: 0.7028 - val_loss: 0.5407 - val_accuracy: 0.7071\n",
      "Epoch 16/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5493 - accuracy: 0.6997 - val_loss: 0.5406 - val_accuracy: 0.7031\n",
      "Epoch 17/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5419 - accuracy: 0.7002 - val_loss: 0.5299 - val_accuracy: 0.7110\n",
      "Epoch 18/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5407 - accuracy: 0.7016 - val_loss: 0.5431 - val_accuracy: 0.6879\n",
      "Epoch 19/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5422 - accuracy: 0.7018 - val_loss: 0.5315 - val_accuracy: 0.7230\n",
      "Epoch 20/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5410 - accuracy: 0.7039 - val_loss: 0.5461 - val_accuracy: 0.6922\n",
      "Epoch 21/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5391 - accuracy: 0.7041 - val_loss: 0.5281 - val_accuracy: 0.7345\n",
      "Epoch 22/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5266 - accuracy: 0.7056 - val_loss: 0.5260 - val_accuracy: 0.7262\n",
      "Epoch 23/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5338 - accuracy: 0.7104 - val_loss: 0.5371 - val_accuracy: 0.7042\n",
      "Epoch 24/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5270 - accuracy: 0.7061 - val_loss: 0.5270 - val_accuracy: 0.7118\n",
      "Epoch 25/30\n",
      "497/497 [==============================] - 4s 8ms/step - loss: 0.5287 - accuracy: 0.7101 - val_loss: 0.5278 - val_accuracy: 0.7288\n",
      "Epoch 26/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5197 - accuracy: 0.7168 - val_loss: 0.5335 - val_accuracy: 0.7024\n",
      "Epoch 27/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5246 - accuracy: 0.7128 - val_loss: 0.5355 - val_accuracy: 0.7005\n",
      "Epoch 28/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5193 - accuracy: 0.7173 - val_loss: 0.5213 - val_accuracy: 0.7262\n",
      "Epoch 29/30\n",
      "497/497 [==============================] - 5s 9ms/step - loss: 0.5271 - accuracy: 0.7111 - val_loss: 0.5310 - val_accuracy: 0.7237\n",
      "Epoch 30/30\n",
      "497/497 [==============================] - 4s 9ms/step - loss: 0.5143 - accuracy: 0.7247 - val_loss: 0.5252 - val_accuracy: 0.7190\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7189873417721518\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_14 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "461/461 [==============================] - 6s 10ms/step - loss: 0.7121 - accuracy: 0.5308 - val_loss: 0.6784 - val_accuracy: 0.5664\n",
      "Epoch 2/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.6643 - accuracy: 0.5900 - val_loss: 0.6242 - val_accuracy: 0.6474\n",
      "Epoch 3/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.6326 - accuracy: 0.6230 - val_loss: 0.6014 - val_accuracy: 0.6694\n",
      "Epoch 4/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.6194 - accuracy: 0.6365 - val_loss: 0.5815 - val_accuracy: 0.6821\n",
      "Epoch 5/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.6024 - accuracy: 0.6633 - val_loss: 0.5765 - val_accuracy: 0.6774\n",
      "Epoch 6/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5885 - accuracy: 0.6683 - val_loss: 0.5665 - val_accuracy: 0.6781\n",
      "Epoch 7/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5834 - accuracy: 0.6757 - val_loss: 0.5572 - val_accuracy: 0.6792\n",
      "Epoch 8/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5741 - accuracy: 0.6835 - val_loss: 0.5600 - val_accuracy: 0.6846\n",
      "Epoch 9/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5718 - accuracy: 0.6828 - val_loss: 0.5643 - val_accuracy: 0.6948\n",
      "Epoch 10/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5644 - accuracy: 0.6909 - val_loss: 0.5428 - val_accuracy: 0.7024\n",
      "Epoch 11/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5635 - accuracy: 0.6900 - val_loss: 0.5492 - val_accuracy: 0.7013\n",
      "Epoch 12/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5544 - accuracy: 0.6886 - val_loss: 0.5462 - val_accuracy: 0.7099\n",
      "Epoch 13/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5573 - accuracy: 0.6945 - val_loss: 0.5495 - val_accuracy: 0.7013\n",
      "Epoch 14/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5470 - accuracy: 0.7019 - val_loss: 0.5390 - val_accuracy: 0.6998\n",
      "Epoch 15/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5480 - accuracy: 0.7036 - val_loss: 0.5356 - val_accuracy: 0.7118\n",
      "Epoch 16/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5491 - accuracy: 0.7025 - val_loss: 0.5340 - val_accuracy: 0.7150\n",
      "Epoch 17/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5398 - accuracy: 0.7033 - val_loss: 0.5303 - val_accuracy: 0.7168\n",
      "Epoch 18/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5428 - accuracy: 0.7041 - val_loss: 0.5331 - val_accuracy: 0.6976\n",
      "Epoch 19/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5396 - accuracy: 0.7021 - val_loss: 0.5255 - val_accuracy: 0.7143\n",
      "Epoch 20/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5427 - accuracy: 0.7052 - val_loss: 0.5353 - val_accuracy: 0.7081\n",
      "Epoch 21/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5314 - accuracy: 0.7106 - val_loss: 0.5188 - val_accuracy: 0.7201\n",
      "Epoch 22/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5234 - accuracy: 0.7121 - val_loss: 0.5165 - val_accuracy: 0.7320\n",
      "Epoch 23/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5307 - accuracy: 0.7101 - val_loss: 0.5255 - val_accuracy: 0.7096\n",
      "Epoch 24/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5345 - accuracy: 0.7123 - val_loss: 0.5191 - val_accuracy: 0.7215\n",
      "Epoch 25/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5287 - accuracy: 0.7123 - val_loss: 0.5237 - val_accuracy: 0.7230\n",
      "Epoch 26/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5220 - accuracy: 0.7173 - val_loss: 0.5189 - val_accuracy: 0.7172\n",
      "Epoch 27/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5216 - accuracy: 0.7176 - val_loss: 0.5271 - val_accuracy: 0.7230\n",
      "Epoch 28/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5228 - accuracy: 0.7176 - val_loss: 0.5195 - val_accuracy: 0.7161\n",
      "Epoch 29/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5243 - accuracy: 0.7137 - val_loss: 0.5218 - val_accuracy: 0.7241\n",
      "Epoch 30/30\n",
      "461/461 [==============================] - 4s 9ms/step - loss: 0.5150 - accuracy: 0.7250 - val_loss: 0.5228 - val_accuracy: 0.7146\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7146473779385172\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_15 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "431/431 [==============================] - 6s 10ms/step - loss: 0.7150 - accuracy: 0.5424 - val_loss: 0.6787 - val_accuracy: 0.5910\n",
      "Epoch 2/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.6587 - accuracy: 0.5912 - val_loss: 0.6182 - val_accuracy: 0.6369\n",
      "Epoch 3/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.6325 - accuracy: 0.6280 - val_loss: 0.5963 - val_accuracy: 0.6720\n",
      "Epoch 4/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.6186 - accuracy: 0.6357 - val_loss: 0.5747 - val_accuracy: 0.6821\n",
      "Epoch 5/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.6070 - accuracy: 0.6523 - val_loss: 0.5763 - val_accuracy: 0.6969\n",
      "Epoch 6/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5884 - accuracy: 0.6714 - val_loss: 0.5739 - val_accuracy: 0.6749\n",
      "Epoch 7/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5795 - accuracy: 0.6757 - val_loss: 0.5509 - val_accuracy: 0.6926\n",
      "Epoch 8/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5746 - accuracy: 0.6779 - val_loss: 0.5533 - val_accuracy: 0.7096\n",
      "Epoch 9/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5689 - accuracy: 0.6814 - val_loss: 0.5497 - val_accuracy: 0.6966\n",
      "Epoch 10/30\n",
      "431/431 [==============================] - 5s 11ms/step - loss: 0.5631 - accuracy: 0.6898 - val_loss: 0.5296 - val_accuracy: 0.7139\n",
      "Epoch 11/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5628 - accuracy: 0.6912 - val_loss: 0.5453 - val_accuracy: 0.6926\n",
      "Epoch 12/30\n",
      "431/431 [==============================] - 5s 10ms/step - loss: 0.5589 - accuracy: 0.6945 - val_loss: 0.5391 - val_accuracy: 0.7212\n",
      "Epoch 13/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5583 - accuracy: 0.6870 - val_loss: 0.5406 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5501 - accuracy: 0.6929 - val_loss: 0.5331 - val_accuracy: 0.6904\n",
      "Epoch 15/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5447 - accuracy: 0.7059 - val_loss: 0.5361 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5438 - accuracy: 0.7045 - val_loss: 0.5666 - val_accuracy: 0.6948\n",
      "Epoch 17/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5426 - accuracy: 0.6968 - val_loss: 0.5247 - val_accuracy: 0.7172\n",
      "Epoch 18/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5376 - accuracy: 0.7049 - val_loss: 0.5435 - val_accuracy: 0.6937\n",
      "Epoch 19/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5334 - accuracy: 0.7083 - val_loss: 0.5261 - val_accuracy: 0.7157\n",
      "Epoch 20/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5424 - accuracy: 0.7014 - val_loss: 0.5318 - val_accuracy: 0.6987\n",
      "Epoch 21/30\n",
      "431/431 [==============================] - 4s 9ms/step - loss: 0.5339 - accuracy: 0.7076 - val_loss: 0.5214 - val_accuracy: 0.7248\n",
      "Epoch 22/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5241 - accuracy: 0.7148 - val_loss: 0.5249 - val_accuracy: 0.7233\n",
      "Epoch 23/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5288 - accuracy: 0.7109 - val_loss: 0.5235 - val_accuracy: 0.7248\n",
      "Epoch 24/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5262 - accuracy: 0.7143 - val_loss: 0.5234 - val_accuracy: 0.7212\n",
      "Epoch 25/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5273 - accuracy: 0.7112 - val_loss: 0.5190 - val_accuracy: 0.7193\n",
      "Epoch 26/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5183 - accuracy: 0.7152 - val_loss: 0.5240 - val_accuracy: 0.7110\n",
      "Epoch 27/30\n",
      "431/431 [==============================] - 5s 11ms/step - loss: 0.5170 - accuracy: 0.7210 - val_loss: 0.5323 - val_accuracy: 0.7132\n",
      "Epoch 28/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5189 - accuracy: 0.7242 - val_loss: 0.5141 - val_accuracy: 0.7237\n",
      "Epoch 29/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5176 - accuracy: 0.7208 - val_loss: 0.5273 - val_accuracy: 0.7255\n",
      "Epoch 30/30\n",
      "431/431 [==============================] - 4s 10ms/step - loss: 0.5143 - accuracy: 0.7222 - val_loss: 0.5235 - val_accuracy: 0.7222\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.722242314647378\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_16 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "404/404 [==============================] - 6s 12ms/step - loss: 0.7133 - accuracy: 0.5421 - val_loss: 0.6804 - val_accuracy: 0.5986\n",
      "Epoch 2/30\n",
      "404/404 [==============================] - 4s 11ms/step - loss: 0.6600 - accuracy: 0.5951 - val_loss: 0.6195 - val_accuracy: 0.6383\n",
      "Epoch 3/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.6318 - accuracy: 0.6247 - val_loss: 0.6011 - val_accuracy: 0.6561\n",
      "Epoch 4/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.6237 - accuracy: 0.6325 - val_loss: 0.5807 - val_accuracy: 0.6814\n",
      "Epoch 5/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.6094 - accuracy: 0.6500 - val_loss: 0.5746 - val_accuracy: 0.6875\n",
      "Epoch 6/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5934 - accuracy: 0.6652 - val_loss: 0.5693 - val_accuracy: 0.6886\n",
      "Epoch 7/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5785 - accuracy: 0.6833 - val_loss: 0.5563 - val_accuracy: 0.6951\n",
      "Epoch 8/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5761 - accuracy: 0.6814 - val_loss: 0.5479 - val_accuracy: 0.7027\n",
      "Epoch 9/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5688 - accuracy: 0.6890 - val_loss: 0.5499 - val_accuracy: 0.6948\n",
      "Epoch 10/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5615 - accuracy: 0.6925 - val_loss: 0.5404 - val_accuracy: 0.6908\n",
      "Epoch 11/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5557 - accuracy: 0.6951 - val_loss: 0.5440 - val_accuracy: 0.6969\n",
      "Epoch 12/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5533 - accuracy: 0.6943 - val_loss: 0.5294 - val_accuracy: 0.7172\n",
      "Epoch 13/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5486 - accuracy: 0.7013 - val_loss: 0.5474 - val_accuracy: 0.7045\n",
      "Epoch 14/30\n",
      "404/404 [==============================] - 4s 9ms/step - loss: 0.5481 - accuracy: 0.6942 - val_loss: 0.5701 - val_accuracy: 0.6850\n",
      "Epoch 15/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5442 - accuracy: 0.7042 - val_loss: 0.5371 - val_accuracy: 0.7038\n",
      "Epoch 16/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5368 - accuracy: 0.7117 - val_loss: 0.5492 - val_accuracy: 0.7154\n",
      "Epoch 17/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5453 - accuracy: 0.6993 - val_loss: 0.5299 - val_accuracy: 0.7114\n",
      "Epoch 18/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5370 - accuracy: 0.7036 - val_loss: 0.5351 - val_accuracy: 0.6944\n",
      "Epoch 19/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5305 - accuracy: 0.7058 - val_loss: 0.5317 - val_accuracy: 0.7118\n",
      "Epoch 20/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5372 - accuracy: 0.7075 - val_loss: 0.5378 - val_accuracy: 0.7034\n",
      "Epoch 21/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5328 - accuracy: 0.7093 - val_loss: 0.5267 - val_accuracy: 0.7161\n",
      "Epoch 22/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5186 - accuracy: 0.7148 - val_loss: 0.5358 - val_accuracy: 0.7237\n",
      "Epoch 23/30\n",
      "404/404 [==============================] - 4s 9ms/step - loss: 0.5276 - accuracy: 0.7114 - val_loss: 0.5258 - val_accuracy: 0.7237\n",
      "Epoch 24/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5272 - accuracy: 0.7210 - val_loss: 0.5277 - val_accuracy: 0.7143\n",
      "Epoch 25/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5235 - accuracy: 0.7121 - val_loss: 0.5245 - val_accuracy: 0.7204\n",
      "Epoch 26/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5170 - accuracy: 0.7242 - val_loss: 0.5277 - val_accuracy: 0.7074\n",
      "Epoch 27/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5158 - accuracy: 0.7216 - val_loss: 0.5297 - val_accuracy: 0.7226\n",
      "Epoch 28/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5148 - accuracy: 0.7261 - val_loss: 0.5468 - val_accuracy: 0.7118\n",
      "Epoch 29/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5193 - accuracy: 0.7245 - val_loss: 0.5261 - val_accuracy: 0.7208\n",
      "Epoch 30/30\n",
      "404/404 [==============================] - 4s 10ms/step - loss: 0.5137 - accuracy: 0.7225 - val_loss: 0.5349 - val_accuracy: 0.7067\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7066907775768535\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_17 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "380/380 [==============================] - 5s 10ms/step - loss: 0.7156 - accuracy: 0.5331 - val_loss: 0.6898 - val_accuracy: 0.5526\n",
      "Epoch 2/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.6581 - accuracy: 0.5980 - val_loss: 0.6188 - val_accuracy: 0.6467\n",
      "Epoch 3/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.6342 - accuracy: 0.6122 - val_loss: 0.5945 - val_accuracy: 0.6745\n",
      "Epoch 4/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.6211 - accuracy: 0.6365 - val_loss: 0.5884 - val_accuracy: 0.6637\n",
      "Epoch 5/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.6051 - accuracy: 0.6551 - val_loss: 0.5700 - val_accuracy: 0.6810\n",
      "Epoch 6/30\n",
      "380/380 [==============================] - 4s 9ms/step - loss: 0.5883 - accuracy: 0.6746 - val_loss: 0.5678 - val_accuracy: 0.6872\n",
      "Epoch 7/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5788 - accuracy: 0.6835 - val_loss: 0.5466 - val_accuracy: 0.7038\n",
      "Epoch 8/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5660 - accuracy: 0.6855 - val_loss: 0.5502 - val_accuracy: 0.6922\n",
      "Epoch 9/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5651 - accuracy: 0.6878 - val_loss: 0.5537 - val_accuracy: 0.6955\n",
      "Epoch 10/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5609 - accuracy: 0.6925 - val_loss: 0.5402 - val_accuracy: 0.6973\n",
      "Epoch 11/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5531 - accuracy: 0.6971 - val_loss: 0.5431 - val_accuracy: 0.7085\n",
      "Epoch 12/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5562 - accuracy: 0.6928 - val_loss: 0.5273 - val_accuracy: 0.7078\n",
      "Epoch 13/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5491 - accuracy: 0.6920 - val_loss: 0.5440 - val_accuracy: 0.7016\n",
      "Epoch 14/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5438 - accuracy: 0.6991 - val_loss: 0.5408 - val_accuracy: 0.6958\n",
      "Epoch 15/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5411 - accuracy: 0.7027 - val_loss: 0.5373 - val_accuracy: 0.7074\n",
      "Epoch 16/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5409 - accuracy: 0.7059 - val_loss: 0.5426 - val_accuracy: 0.7052\n",
      "Epoch 17/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5408 - accuracy: 0.7007 - val_loss: 0.5344 - val_accuracy: 0.7161\n",
      "Epoch 18/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5316 - accuracy: 0.7042 - val_loss: 0.5332 - val_accuracy: 0.6976\n",
      "Epoch 19/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5319 - accuracy: 0.7062 - val_loss: 0.5243 - val_accuracy: 0.7154\n",
      "Epoch 20/30\n",
      "380/380 [==============================] - 3s 9ms/step - loss: 0.5320 - accuracy: 0.7073 - val_loss: 0.5311 - val_accuracy: 0.7078\n",
      "Epoch 21/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5252 - accuracy: 0.7157 - val_loss: 0.5327 - val_accuracy: 0.7280\n",
      "Epoch 22/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5227 - accuracy: 0.7093 - val_loss: 0.5211 - val_accuracy: 0.7327\n",
      "Epoch 23/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5213 - accuracy: 0.7163 - val_loss: 0.5247 - val_accuracy: 0.7251\n",
      "Epoch 24/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5224 - accuracy: 0.7129 - val_loss: 0.5356 - val_accuracy: 0.7103\n",
      "Epoch 25/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5247 - accuracy: 0.7183 - val_loss: 0.5226 - val_accuracy: 0.7244\n",
      "Epoch 26/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5161 - accuracy: 0.7278 - val_loss: 0.5315 - val_accuracy: 0.7114\n",
      "Epoch 27/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5170 - accuracy: 0.7233 - val_loss: 0.5291 - val_accuracy: 0.7204\n",
      "Epoch 28/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5118 - accuracy: 0.7179 - val_loss: 0.5277 - val_accuracy: 0.7306\n",
      "Epoch 29/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5141 - accuracy: 0.7196 - val_loss: 0.5446 - val_accuracy: 0.7118\n",
      "Epoch 30/30\n",
      "380/380 [==============================] - 4s 10ms/step - loss: 0.5106 - accuracy: 0.7279 - val_loss: 0.5418 - val_accuracy: 0.7118\n",
      "87/87 [==============================] - 0s 4ms/step\n",
      "0.711754068716094\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_18 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "359/359 [==============================] - 5s 11ms/step - loss: 0.7173 - accuracy: 0.5419 - val_loss: 0.6804 - val_accuracy: 0.5685\n",
      "Epoch 2/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.6580 - accuracy: 0.6013 - val_loss: 0.6143 - val_accuracy: 0.6481\n",
      "Epoch 3/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.6261 - accuracy: 0.6284 - val_loss: 0.5941 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.6198 - accuracy: 0.6433 - val_loss: 0.5854 - val_accuracy: 0.6825\n",
      "Epoch 5/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.6056 - accuracy: 0.6526 - val_loss: 0.5689 - val_accuracy: 0.6915\n",
      "Epoch 6/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5883 - accuracy: 0.6738 - val_loss: 0.5637 - val_accuracy: 0.6886\n",
      "Epoch 7/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5772 - accuracy: 0.6839 - val_loss: 0.5536 - val_accuracy: 0.6868\n",
      "Epoch 8/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5683 - accuracy: 0.6863 - val_loss: 0.5423 - val_accuracy: 0.7118\n",
      "Epoch 9/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5636 - accuracy: 0.6912 - val_loss: 0.5669 - val_accuracy: 0.6948\n",
      "Epoch 10/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5590 - accuracy: 0.6894 - val_loss: 0.5303 - val_accuracy: 0.7092\n",
      "Epoch 11/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5505 - accuracy: 0.6983 - val_loss: 0.5353 - val_accuracy: 0.7107\n",
      "Epoch 12/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5510 - accuracy: 0.6917 - val_loss: 0.5370 - val_accuracy: 0.7034\n",
      "Epoch 13/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.5507 - accuracy: 0.7011 - val_loss: 0.5399 - val_accuracy: 0.7128\n",
      "Epoch 14/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5430 - accuracy: 0.7007 - val_loss: 0.5618 - val_accuracy: 0.6807\n",
      "Epoch 15/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.5432 - accuracy: 0.6977 - val_loss: 0.5393 - val_accuracy: 0.6966\n",
      "Epoch 16/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5325 - accuracy: 0.7128 - val_loss: 0.5366 - val_accuracy: 0.7099\n",
      "Epoch 17/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5352 - accuracy: 0.7018 - val_loss: 0.5332 - val_accuracy: 0.7042\n",
      "Epoch 18/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5323 - accuracy: 0.7109 - val_loss: 0.5381 - val_accuracy: 0.6948\n",
      "Epoch 19/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.5306 - accuracy: 0.7039 - val_loss: 0.5239 - val_accuracy: 0.7150\n",
      "Epoch 20/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5359 - accuracy: 0.7075 - val_loss: 0.5395 - val_accuracy: 0.6948\n",
      "Epoch 21/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.5202 - accuracy: 0.7162 - val_loss: 0.5201 - val_accuracy: 0.7342\n",
      "Epoch 22/30\n",
      "359/359 [==============================] - 4s 10ms/step - loss: 0.5220 - accuracy: 0.7174 - val_loss: 0.5217 - val_accuracy: 0.7154\n",
      "Epoch 23/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.5161 - accuracy: 0.7222 - val_loss: 0.5181 - val_accuracy: 0.7248\n",
      "Epoch 24/30\n",
      "359/359 [==============================] - 3s 9ms/step - loss: 0.5171 - accuracy: 0.7235 - val_loss: 0.5198 - val_accuracy: 0.7280\n",
      "Epoch 25/30\n",
      "359/359 [==============================] - 3s 10ms/step - loss: 0.5124 - accuracy: 0.7221 - val_loss: 0.5259 - val_accuracy: 0.7313\n",
      "Epoch 26/30\n",
      "359/359 [==============================] - 3s 9ms/step - loss: 0.5170 - accuracy: 0.7173 - val_loss: 0.5259 - val_accuracy: 0.7013\n",
      "Epoch 27/30\n",
      "359/359 [==============================] - 3s 9ms/step - loss: 0.5056 - accuracy: 0.7191 - val_loss: 0.5312 - val_accuracy: 0.7128\n",
      "Epoch 28/30\n",
      "359/359 [==============================] - 3s 9ms/step - loss: 0.5141 - accuracy: 0.7146 - val_loss: 0.5193 - val_accuracy: 0.7190\n",
      "Epoch 29/30\n",
      "359/359 [==============================] - 3s 9ms/step - loss: 0.5097 - accuracy: 0.7199 - val_loss: 0.5318 - val_accuracy: 0.7277\n",
      "Epoch 30/30\n",
      "359/359 [==============================] - 3s 9ms/step - loss: 0.5045 - accuracy: 0.7264 - val_loss: 0.5275 - val_accuracy: 0.7136\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7135623869801085\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_19 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "340/340 [==============================] - 5s 11ms/step - loss: 0.7169 - accuracy: 0.5413 - val_loss: 0.6764 - val_accuracy: 0.5920\n",
      "Epoch 2/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6588 - accuracy: 0.5973 - val_loss: 0.6243 - val_accuracy: 0.6492\n",
      "Epoch 3/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.6284 - accuracy: 0.6357 - val_loss: 0.6051 - val_accuracy: 0.6615\n",
      "Epoch 4/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6192 - accuracy: 0.6446 - val_loss: 0.5999 - val_accuracy: 0.6633\n",
      "Epoch 5/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.6111 - accuracy: 0.6504 - val_loss: 0.5816 - val_accuracy: 0.6951\n",
      "Epoch 6/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5918 - accuracy: 0.6684 - val_loss: 0.5679 - val_accuracy: 0.6966\n",
      "Epoch 7/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5785 - accuracy: 0.6852 - val_loss: 0.5660 - val_accuracy: 0.6901\n",
      "Epoch 8/30\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 0.5749 - accuracy: 0.6847 - val_loss: 0.5431 - val_accuracy: 0.7103\n",
      "Epoch 9/30\n",
      "340/340 [==============================] - 4s 10ms/step - loss: 0.5626 - accuracy: 0.6965 - val_loss: 0.5581 - val_accuracy: 0.6995\n",
      "Epoch 10/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5644 - accuracy: 0.6864 - val_loss: 0.5401 - val_accuracy: 0.7089\n",
      "Epoch 11/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5525 - accuracy: 0.6974 - val_loss: 0.5347 - val_accuracy: 0.7096\n",
      "Epoch 12/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5541 - accuracy: 0.6974 - val_loss: 0.5357 - val_accuracy: 0.7190\n",
      "Epoch 13/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.5492 - accuracy: 0.6999 - val_loss: 0.5437 - val_accuracy: 0.6995\n",
      "Epoch 14/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.5468 - accuracy: 0.6990 - val_loss: 0.5443 - val_accuracy: 0.6937\n",
      "Epoch 15/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5429 - accuracy: 0.7039 - val_loss: 0.5363 - val_accuracy: 0.7027\n",
      "Epoch 16/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5422 - accuracy: 0.7086 - val_loss: 0.5428 - val_accuracy: 0.7002\n",
      "Epoch 17/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5366 - accuracy: 0.7062 - val_loss: 0.5371 - val_accuracy: 0.7154\n",
      "Epoch 18/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5358 - accuracy: 0.7081 - val_loss: 0.5291 - val_accuracy: 0.6962\n",
      "Epoch 19/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.5315 - accuracy: 0.7093 - val_loss: 0.5282 - val_accuracy: 0.7280\n",
      "Epoch 20/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5355 - accuracy: 0.7013 - val_loss: 0.5402 - val_accuracy: 0.7002\n",
      "Epoch 21/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5274 - accuracy: 0.7126 - val_loss: 0.5319 - val_accuracy: 0.7215\n",
      "Epoch 22/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.5215 - accuracy: 0.7140 - val_loss: 0.5239 - val_accuracy: 0.7208\n",
      "Epoch 23/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.5272 - accuracy: 0.7135 - val_loss: 0.5220 - val_accuracy: 0.7226\n",
      "Epoch 24/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5211 - accuracy: 0.7182 - val_loss: 0.5231 - val_accuracy: 0.7320\n",
      "Epoch 25/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5221 - accuracy: 0.7171 - val_loss: 0.5257 - val_accuracy: 0.7215\n",
      "Epoch 26/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5183 - accuracy: 0.7160 - val_loss: 0.5174 - val_accuracy: 0.7212\n",
      "Epoch 27/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5101 - accuracy: 0.7247 - val_loss: 0.5376 - val_accuracy: 0.7089\n",
      "Epoch 28/30\n",
      "340/340 [==============================] - 3s 10ms/step - loss: 0.5082 - accuracy: 0.7248 - val_loss: 0.5252 - val_accuracy: 0.7172\n",
      "Epoch 29/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.5163 - accuracy: 0.7230 - val_loss: 0.5283 - val_accuracy: 0.7335\n",
      "Epoch 30/30\n",
      "340/340 [==============================] - 3s 9ms/step - loss: 0.5038 - accuracy: 0.7252 - val_loss: 0.5402 - val_accuracy: 0.7092\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7092224231464738\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_20 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_61 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_62 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "323/323 [==============================] - 5s 11ms/step - loss: 0.7195 - accuracy: 0.5407 - val_loss: 0.6748 - val_accuracy: 0.5826\n",
      "Epoch 2/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.6593 - accuracy: 0.6016 - val_loss: 0.6346 - val_accuracy: 0.6358\n",
      "Epoch 3/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.6252 - accuracy: 0.6342 - val_loss: 0.5992 - val_accuracy: 0.6702\n",
      "Epoch 4/30\n",
      "323/323 [==============================] - 3s 9ms/step - loss: 0.6200 - accuracy: 0.6405 - val_loss: 0.5815 - val_accuracy: 0.6846\n",
      "Epoch 5/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.6058 - accuracy: 0.6498 - val_loss: 0.5730 - val_accuracy: 0.6958\n",
      "Epoch 6/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5887 - accuracy: 0.6664 - val_loss: 0.5647 - val_accuracy: 0.6933\n",
      "Epoch 7/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5740 - accuracy: 0.6861 - val_loss: 0.5609 - val_accuracy: 0.6948\n",
      "Epoch 8/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5712 - accuracy: 0.6808 - val_loss: 0.5392 - val_accuracy: 0.7107\n",
      "Epoch 9/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5697 - accuracy: 0.6912 - val_loss: 0.5769 - val_accuracy: 0.6825\n",
      "Epoch 10/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5608 - accuracy: 0.6898 - val_loss: 0.5428 - val_accuracy: 0.6973\n",
      "Epoch 11/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5515 - accuracy: 0.6960 - val_loss: 0.5468 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5494 - accuracy: 0.6968 - val_loss: 0.5357 - val_accuracy: 0.6998\n",
      "Epoch 13/30\n",
      "323/323 [==============================] - 3s 11ms/step - loss: 0.5494 - accuracy: 0.6997 - val_loss: 0.5462 - val_accuracy: 0.7045\n",
      "Epoch 14/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5388 - accuracy: 0.7036 - val_loss: 0.5491 - val_accuracy: 0.6872\n",
      "Epoch 15/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5398 - accuracy: 0.7075 - val_loss: 0.5414 - val_accuracy: 0.7013\n",
      "Epoch 16/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5335 - accuracy: 0.7061 - val_loss: 0.5816 - val_accuracy: 0.7049\n",
      "Epoch 17/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5346 - accuracy: 0.7072 - val_loss: 0.5401 - val_accuracy: 0.6951\n",
      "Epoch 18/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5329 - accuracy: 0.7073 - val_loss: 0.5424 - val_accuracy: 0.6803\n",
      "Epoch 19/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5276 - accuracy: 0.7092 - val_loss: 0.5251 - val_accuracy: 0.7107\n",
      "Epoch 20/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5319 - accuracy: 0.7056 - val_loss: 0.5285 - val_accuracy: 0.6984\n",
      "Epoch 21/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5238 - accuracy: 0.7106 - val_loss: 0.5263 - val_accuracy: 0.7241\n",
      "Epoch 22/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5168 - accuracy: 0.7191 - val_loss: 0.5216 - val_accuracy: 0.7118\n",
      "Epoch 23/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5191 - accuracy: 0.7210 - val_loss: 0.5229 - val_accuracy: 0.7222\n",
      "Epoch 24/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5230 - accuracy: 0.7169 - val_loss: 0.5233 - val_accuracy: 0.7128\n",
      "Epoch 25/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5153 - accuracy: 0.7262 - val_loss: 0.5317 - val_accuracy: 0.7121\n",
      "Epoch 26/30\n",
      "323/323 [==============================] - 3s 9ms/step - loss: 0.5091 - accuracy: 0.7255 - val_loss: 0.5353 - val_accuracy: 0.7103\n",
      "Epoch 27/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5107 - accuracy: 0.7255 - val_loss: 0.5290 - val_accuracy: 0.7125\n",
      "Epoch 28/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5071 - accuracy: 0.7278 - val_loss: 0.5260 - val_accuracy: 0.7089\n",
      "Epoch 29/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5108 - accuracy: 0.7219 - val_loss: 0.5489 - val_accuracy: 0.7114\n",
      "Epoch 30/30\n",
      "323/323 [==============================] - 3s 10ms/step - loss: 0.5077 - accuracy: 0.7297 - val_loss: 0.5334 - val_accuracy: 0.7190\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7189873417721518\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_21 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_63 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_64 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "308/308 [==============================] - 5s 10ms/step - loss: 0.7177 - accuracy: 0.5367 - val_loss: 0.6823 - val_accuracy: 0.5653\n",
      "Epoch 2/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.6621 - accuracy: 0.5931 - val_loss: 0.6321 - val_accuracy: 0.6264\n",
      "Epoch 3/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.6299 - accuracy: 0.6323 - val_loss: 0.6062 - val_accuracy: 0.6608\n",
      "Epoch 4/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.6204 - accuracy: 0.6382 - val_loss: 0.5947 - val_accuracy: 0.6553\n",
      "Epoch 5/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.6008 - accuracy: 0.6521 - val_loss: 0.5694 - val_accuracy: 0.6781\n",
      "Epoch 6/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5883 - accuracy: 0.6725 - val_loss: 0.5583 - val_accuracy: 0.6911\n",
      "Epoch 7/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5735 - accuracy: 0.6842 - val_loss: 0.5551 - val_accuracy: 0.6962\n",
      "Epoch 8/30\n",
      "308/308 [==============================] - 3s 9ms/step - loss: 0.5699 - accuracy: 0.6852 - val_loss: 0.5451 - val_accuracy: 0.7024\n",
      "Epoch 9/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5652 - accuracy: 0.6929 - val_loss: 0.5539 - val_accuracy: 0.7071\n",
      "Epoch 10/30\n",
      "308/308 [==============================] - 3s 9ms/step - loss: 0.5621 - accuracy: 0.6838 - val_loss: 0.5347 - val_accuracy: 0.6955\n",
      "Epoch 11/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5535 - accuracy: 0.6982 - val_loss: 0.5365 - val_accuracy: 0.7009\n",
      "Epoch 12/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5465 - accuracy: 0.7022 - val_loss: 0.5294 - val_accuracy: 0.7165\n",
      "Epoch 13/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5438 - accuracy: 0.7011 - val_loss: 0.5397 - val_accuracy: 0.7128\n",
      "Epoch 14/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5408 - accuracy: 0.7039 - val_loss: 0.5344 - val_accuracy: 0.6962\n",
      "Epoch 15/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5395 - accuracy: 0.7036 - val_loss: 0.5340 - val_accuracy: 0.7078\n",
      "Epoch 16/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5310 - accuracy: 0.7058 - val_loss: 0.5469 - val_accuracy: 0.7009\n",
      "Epoch 17/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5339 - accuracy: 0.7030 - val_loss: 0.5370 - val_accuracy: 0.6995\n",
      "Epoch 18/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5328 - accuracy: 0.7121 - val_loss: 0.5365 - val_accuracy: 0.6875\n",
      "Epoch 19/30\n",
      "308/308 [==============================] - 3s 9ms/step - loss: 0.5298 - accuracy: 0.7115 - val_loss: 0.5344 - val_accuracy: 0.7052\n",
      "Epoch 20/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5288 - accuracy: 0.7093 - val_loss: 0.5302 - val_accuracy: 0.6991\n",
      "Epoch 21/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5212 - accuracy: 0.7179 - val_loss: 0.5251 - val_accuracy: 0.7215\n",
      "Epoch 22/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5237 - accuracy: 0.7121 - val_loss: 0.5137 - val_accuracy: 0.7230\n",
      "Epoch 23/30\n",
      "308/308 [==============================] - 3s 11ms/step - loss: 0.5199 - accuracy: 0.7128 - val_loss: 0.5211 - val_accuracy: 0.7219\n",
      "Epoch 24/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5133 - accuracy: 0.7221 - val_loss: 0.5210 - val_accuracy: 0.7269\n",
      "Epoch 25/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5203 - accuracy: 0.7208 - val_loss: 0.5277 - val_accuracy: 0.7103\n",
      "Epoch 26/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5092 - accuracy: 0.7286 - val_loss: 0.5198 - val_accuracy: 0.7165\n",
      "Epoch 27/30\n",
      "308/308 [==============================] - 3s 9ms/step - loss: 0.5124 - accuracy: 0.7217 - val_loss: 0.5367 - val_accuracy: 0.6995\n",
      "Epoch 28/30\n",
      "308/308 [==============================] - 3s 9ms/step - loss: 0.5101 - accuracy: 0.7245 - val_loss: 0.5234 - val_accuracy: 0.7244\n",
      "Epoch 29/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5074 - accuracy: 0.7186 - val_loss: 0.5276 - val_accuracy: 0.7226\n",
      "Epoch 30/30\n",
      "308/308 [==============================] - 3s 10ms/step - loss: 0.5022 - accuracy: 0.7340 - val_loss: 0.5354 - val_accuracy: 0.7089\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7088607594936709\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_22 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_67 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "294/294 [==============================] - 4s 11ms/step - loss: 0.7201 - accuracy: 0.5319 - val_loss: 0.6904 - val_accuracy: 0.5327\n",
      "Epoch 2/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.6635 - accuracy: 0.5900 - val_loss: 0.6248 - val_accuracy: 0.6481\n",
      "Epoch 3/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.6275 - accuracy: 0.6247 - val_loss: 0.6036 - val_accuracy: 0.6608\n",
      "Epoch 4/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.6188 - accuracy: 0.6373 - val_loss: 0.5846 - val_accuracy: 0.6976\n",
      "Epoch 5/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.6045 - accuracy: 0.6580 - val_loss: 0.5923 - val_accuracy: 0.6687\n",
      "Epoch 6/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5878 - accuracy: 0.6763 - val_loss: 0.5612 - val_accuracy: 0.7038\n",
      "Epoch 7/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5727 - accuracy: 0.6890 - val_loss: 0.5470 - val_accuracy: 0.7005\n",
      "Epoch 8/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5653 - accuracy: 0.6887 - val_loss: 0.5374 - val_accuracy: 0.7092\n",
      "Epoch 9/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5630 - accuracy: 0.6887 - val_loss: 0.5484 - val_accuracy: 0.7049\n",
      "Epoch 10/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5555 - accuracy: 0.6983 - val_loss: 0.5308 - val_accuracy: 0.7042\n",
      "Epoch 11/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5513 - accuracy: 0.6988 - val_loss: 0.5439 - val_accuracy: 0.7063\n",
      "Epoch 12/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5463 - accuracy: 0.7022 - val_loss: 0.5327 - val_accuracy: 0.7161\n",
      "Epoch 13/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5411 - accuracy: 0.7106 - val_loss: 0.5346 - val_accuracy: 0.7157\n",
      "Epoch 14/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5374 - accuracy: 0.7030 - val_loss: 0.5469 - val_accuracy: 0.6976\n",
      "Epoch 15/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5368 - accuracy: 0.7062 - val_loss: 0.5317 - val_accuracy: 0.7143\n",
      "Epoch 16/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5265 - accuracy: 0.7173 - val_loss: 0.5353 - val_accuracy: 0.7067\n",
      "Epoch 17/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5312 - accuracy: 0.7028 - val_loss: 0.5354 - val_accuracy: 0.7038\n",
      "Epoch 18/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5265 - accuracy: 0.7185 - val_loss: 0.5238 - val_accuracy: 0.7136\n",
      "Epoch 19/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5189 - accuracy: 0.7146 - val_loss: 0.5276 - val_accuracy: 0.7074\n",
      "Epoch 20/30\n",
      "294/294 [==============================] - 3s 9ms/step - loss: 0.5191 - accuracy: 0.7132 - val_loss: 0.5343 - val_accuracy: 0.7128\n",
      "Epoch 21/30\n",
      "294/294 [==============================] - 3s 11ms/step - loss: 0.5133 - accuracy: 0.7193 - val_loss: 0.5336 - val_accuracy: 0.7165\n",
      "Epoch 22/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5158 - accuracy: 0.7188 - val_loss: 0.5229 - val_accuracy: 0.7190\n",
      "Epoch 23/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5132 - accuracy: 0.7235 - val_loss: 0.5244 - val_accuracy: 0.7269\n",
      "Epoch 24/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5109 - accuracy: 0.7248 - val_loss: 0.5285 - val_accuracy: 0.7168\n",
      "Epoch 25/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5182 - accuracy: 0.7179 - val_loss: 0.5329 - val_accuracy: 0.7215\n",
      "Epoch 26/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5118 - accuracy: 0.7253 - val_loss: 0.5346 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5002 - accuracy: 0.7281 - val_loss: 0.5375 - val_accuracy: 0.7193\n",
      "Epoch 28/30\n",
      "294/294 [==============================] - 3s 11ms/step - loss: 0.5021 - accuracy: 0.7252 - val_loss: 0.5227 - val_accuracy: 0.7255\n",
      "Epoch 29/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5099 - accuracy: 0.7238 - val_loss: 0.5304 - val_accuracy: 0.7183\n",
      "Epoch 30/30\n",
      "294/294 [==============================] - 3s 10ms/step - loss: 0.5022 - accuracy: 0.7248 - val_loss: 0.5400 - val_accuracy: 0.7045\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7045207956600361\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_23 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "281/281 [==============================] - 4s 10ms/step - loss: 0.7223 - accuracy: 0.5396 - val_loss: 0.6745 - val_accuracy: 0.5693\n",
      "Epoch 2/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.6585 - accuracy: 0.5914 - val_loss: 0.6127 - val_accuracy: 0.6553\n",
      "Epoch 3/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.6259 - accuracy: 0.6309 - val_loss: 0.5935 - val_accuracy: 0.6723\n",
      "Epoch 4/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.6156 - accuracy: 0.6402 - val_loss: 0.5841 - val_accuracy: 0.6702\n",
      "Epoch 5/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.6027 - accuracy: 0.6642 - val_loss: 0.5731 - val_accuracy: 0.6857\n",
      "Epoch 6/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5871 - accuracy: 0.6723 - val_loss: 0.5733 - val_accuracy: 0.6969\n",
      "Epoch 7/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5788 - accuracy: 0.6838 - val_loss: 0.5551 - val_accuracy: 0.6951\n",
      "Epoch 8/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5705 - accuracy: 0.6828 - val_loss: 0.5460 - val_accuracy: 0.7121\n",
      "Epoch 9/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5604 - accuracy: 0.6923 - val_loss: 0.5469 - val_accuracy: 0.6915\n",
      "Epoch 10/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5557 - accuracy: 0.6925 - val_loss: 0.5375 - val_accuracy: 0.7049\n",
      "Epoch 11/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5513 - accuracy: 0.6987 - val_loss: 0.5432 - val_accuracy: 0.7063\n",
      "Epoch 12/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5450 - accuracy: 0.7019 - val_loss: 0.5305 - val_accuracy: 0.7197\n",
      "Epoch 13/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5452 - accuracy: 0.6969 - val_loss: 0.5366 - val_accuracy: 0.7103\n",
      "Epoch 14/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5396 - accuracy: 0.6996 - val_loss: 0.5554 - val_accuracy: 0.6832\n",
      "Epoch 15/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5387 - accuracy: 0.7101 - val_loss: 0.5325 - val_accuracy: 0.7078\n",
      "Epoch 16/30\n",
      "281/281 [==============================] - 3s 11ms/step - loss: 0.5337 - accuracy: 0.7098 - val_loss: 0.5389 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5294 - accuracy: 0.7092 - val_loss: 0.5286 - val_accuracy: 0.7085\n",
      "Epoch 18/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5293 - accuracy: 0.7115 - val_loss: 0.5276 - val_accuracy: 0.7038\n",
      "Epoch 19/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5345 - accuracy: 0.7031 - val_loss: 0.5257 - val_accuracy: 0.7136\n",
      "Epoch 20/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5240 - accuracy: 0.7123 - val_loss: 0.5255 - val_accuracy: 0.7165\n",
      "Epoch 21/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5174 - accuracy: 0.7186 - val_loss: 0.5199 - val_accuracy: 0.7172\n",
      "Epoch 22/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5133 - accuracy: 0.7188 - val_loss: 0.5279 - val_accuracy: 0.7298\n",
      "Epoch 23/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5198 - accuracy: 0.7183 - val_loss: 0.5257 - val_accuracy: 0.7193\n",
      "Epoch 24/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5157 - accuracy: 0.7171 - val_loss: 0.5273 - val_accuracy: 0.7092\n",
      "Epoch 25/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5146 - accuracy: 0.7233 - val_loss: 0.5199 - val_accuracy: 0.7201\n",
      "Epoch 26/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5082 - accuracy: 0.7244 - val_loss: 0.5230 - val_accuracy: 0.7107\n",
      "Epoch 27/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5076 - accuracy: 0.7236 - val_loss: 0.5249 - val_accuracy: 0.7052\n",
      "Epoch 28/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5074 - accuracy: 0.7242 - val_loss: 0.5258 - val_accuracy: 0.7262\n",
      "Epoch 29/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.5073 - accuracy: 0.7283 - val_loss: 0.5318 - val_accuracy: 0.7186\n",
      "Epoch 30/30\n",
      "281/281 [==============================] - 3s 10ms/step - loss: 0.4983 - accuracy: 0.7266 - val_loss: 0.5270 - val_accuracy: 0.7168\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7168173598553346\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_24 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "269/269 [==============================] - 5s 12ms/step - loss: 0.7195 - accuracy: 0.5455 - val_loss: 0.6808 - val_accuracy: 0.5725\n",
      "Epoch 2/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.6551 - accuracy: 0.6021 - val_loss: 0.6288 - val_accuracy: 0.6340\n",
      "Epoch 3/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.6247 - accuracy: 0.6289 - val_loss: 0.5973 - val_accuracy: 0.6759\n",
      "Epoch 4/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.6121 - accuracy: 0.6467 - val_loss: 0.5819 - val_accuracy: 0.6640\n",
      "Epoch 5/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.6075 - accuracy: 0.6539 - val_loss: 0.5806 - val_accuracy: 0.6926\n",
      "Epoch 6/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5873 - accuracy: 0.6756 - val_loss: 0.5708 - val_accuracy: 0.6850\n",
      "Epoch 7/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5734 - accuracy: 0.6844 - val_loss: 0.5593 - val_accuracy: 0.6825\n",
      "Epoch 8/30\n",
      "269/269 [==============================] - 3s 12ms/step - loss: 0.5723 - accuracy: 0.6845 - val_loss: 0.5431 - val_accuracy: 0.7078\n",
      "Epoch 9/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5607 - accuracy: 0.6934 - val_loss: 0.5436 - val_accuracy: 0.7161\n",
      "Epoch 10/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5556 - accuracy: 0.6943 - val_loss: 0.5351 - val_accuracy: 0.6987\n",
      "Epoch 11/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5494 - accuracy: 0.7013 - val_loss: 0.5397 - val_accuracy: 0.7078\n",
      "Epoch 12/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5474 - accuracy: 0.7002 - val_loss: 0.5256 - val_accuracy: 0.7219\n",
      "Epoch 13/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5486 - accuracy: 0.6928 - val_loss: 0.5332 - val_accuracy: 0.7121\n",
      "Epoch 14/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5428 - accuracy: 0.7016 - val_loss: 0.5349 - val_accuracy: 0.6987\n",
      "Epoch 15/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5432 - accuracy: 0.7047 - val_loss: 0.5597 - val_accuracy: 0.6825\n",
      "Epoch 16/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5303 - accuracy: 0.7112 - val_loss: 0.5399 - val_accuracy: 0.6998\n",
      "Epoch 17/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5324 - accuracy: 0.7104 - val_loss: 0.5385 - val_accuracy: 0.6998\n",
      "Epoch 18/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5305 - accuracy: 0.7087 - val_loss: 0.5327 - val_accuracy: 0.7056\n",
      "Epoch 19/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5185 - accuracy: 0.7111 - val_loss: 0.5260 - val_accuracy: 0.7121\n",
      "Epoch 20/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5222 - accuracy: 0.7081 - val_loss: 0.5393 - val_accuracy: 0.6929\n",
      "Epoch 21/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5189 - accuracy: 0.7191 - val_loss: 0.5216 - val_accuracy: 0.7288\n",
      "Epoch 22/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5209 - accuracy: 0.7140 - val_loss: 0.5219 - val_accuracy: 0.7335\n",
      "Epoch 23/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5167 - accuracy: 0.7176 - val_loss: 0.5225 - val_accuracy: 0.7266\n",
      "Epoch 24/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5137 - accuracy: 0.7227 - val_loss: 0.5232 - val_accuracy: 0.7139\n",
      "Epoch 25/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5160 - accuracy: 0.7214 - val_loss: 0.5307 - val_accuracy: 0.7172\n",
      "Epoch 26/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5115 - accuracy: 0.7199 - val_loss: 0.5326 - val_accuracy: 0.7024\n",
      "Epoch 27/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5061 - accuracy: 0.7250 - val_loss: 0.5220 - val_accuracy: 0.7128\n",
      "Epoch 28/30\n",
      "269/269 [==============================] - 3s 11ms/step - loss: 0.5027 - accuracy: 0.7276 - val_loss: 0.5198 - val_accuracy: 0.7288\n",
      "Epoch 29/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.5086 - accuracy: 0.7216 - val_loss: 0.5357 - val_accuracy: 0.7157\n",
      "Epoch 30/30\n",
      "269/269 [==============================] - 3s 10ms/step - loss: 0.4963 - accuracy: 0.7314 - val_loss: 0.5415 - val_accuracy: 0.7143\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7142857142857143\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_25 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_76 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "259/259 [==============================] - 4s 11ms/step - loss: 0.7209 - accuracy: 0.5429 - val_loss: 0.6824 - val_accuracy: 0.5696\n",
      "Epoch 2/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.6661 - accuracy: 0.6011 - val_loss: 0.6356 - val_accuracy: 0.6495\n",
      "Epoch 3/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.6289 - accuracy: 0.6273 - val_loss: 0.6057 - val_accuracy: 0.6807\n",
      "Epoch 4/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.6195 - accuracy: 0.6394 - val_loss: 0.5899 - val_accuracy: 0.6835\n",
      "Epoch 5/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.6086 - accuracy: 0.6526 - val_loss: 0.5772 - val_accuracy: 0.6897\n",
      "Epoch 6/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5887 - accuracy: 0.6664 - val_loss: 0.5571 - val_accuracy: 0.7009\n",
      "Epoch 7/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5793 - accuracy: 0.6780 - val_loss: 0.5538 - val_accuracy: 0.6850\n",
      "Epoch 8/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5698 - accuracy: 0.6785 - val_loss: 0.5435 - val_accuracy: 0.6958\n",
      "Epoch 9/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5632 - accuracy: 0.6863 - val_loss: 0.5620 - val_accuracy: 0.6904\n",
      "Epoch 10/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5610 - accuracy: 0.6894 - val_loss: 0.5384 - val_accuracy: 0.7074\n",
      "Epoch 11/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5527 - accuracy: 0.6946 - val_loss: 0.5399 - val_accuracy: 0.7049\n",
      "Epoch 12/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5508 - accuracy: 0.6997 - val_loss: 0.5292 - val_accuracy: 0.7089\n",
      "Epoch 13/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5473 - accuracy: 0.6982 - val_loss: 0.5409 - val_accuracy: 0.7056\n",
      "Epoch 14/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5432 - accuracy: 0.6999 - val_loss: 0.5487 - val_accuracy: 0.6901\n",
      "Epoch 15/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5425 - accuracy: 0.7031 - val_loss: 0.5345 - val_accuracy: 0.7092\n",
      "Epoch 16/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5337 - accuracy: 0.7118 - val_loss: 0.5334 - val_accuracy: 0.6969\n",
      "Epoch 17/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5321 - accuracy: 0.7059 - val_loss: 0.5332 - val_accuracy: 0.7071\n",
      "Epoch 18/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5283 - accuracy: 0.7080 - val_loss: 0.5283 - val_accuracy: 0.6958\n",
      "Epoch 19/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5206 - accuracy: 0.7163 - val_loss: 0.5236 - val_accuracy: 0.7175\n",
      "Epoch 20/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5218 - accuracy: 0.7103 - val_loss: 0.5387 - val_accuracy: 0.6854\n",
      "Epoch 21/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5161 - accuracy: 0.7202 - val_loss: 0.5226 - val_accuracy: 0.7248\n",
      "Epoch 22/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5148 - accuracy: 0.7221 - val_loss: 0.5164 - val_accuracy: 0.7222\n",
      "Epoch 23/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5185 - accuracy: 0.7146 - val_loss: 0.5180 - val_accuracy: 0.7324\n",
      "Epoch 24/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5114 - accuracy: 0.7242 - val_loss: 0.5222 - val_accuracy: 0.7201\n",
      "Epoch 25/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5088 - accuracy: 0.7239 - val_loss: 0.5252 - val_accuracy: 0.7291\n",
      "Epoch 26/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.5031 - accuracy: 0.7230 - val_loss: 0.5270 - val_accuracy: 0.7081\n",
      "Epoch 27/30\n",
      "259/259 [==============================] - 3s 12ms/step - loss: 0.5055 - accuracy: 0.7289 - val_loss: 0.5356 - val_accuracy: 0.7161\n",
      "Epoch 28/30\n",
      "259/259 [==============================] - 3s 10ms/step - loss: 0.5049 - accuracy: 0.7278 - val_loss: 0.5269 - val_accuracy: 0.7146\n",
      "Epoch 29/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.4962 - accuracy: 0.7334 - val_loss: 0.5345 - val_accuracy: 0.7237\n",
      "Epoch 30/30\n",
      "259/259 [==============================] - 3s 11ms/step - loss: 0.4973 - accuracy: 0.7312 - val_loss: 0.5351 - val_accuracy: 0.7103\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7103074141048824\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_26 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_80 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "249/249 [==============================] - 4s 12ms/step - loss: 0.7214 - accuracy: 0.5370 - val_loss: 0.6865 - val_accuracy: 0.5570\n",
      "Epoch 2/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.6623 - accuracy: 0.5908 - val_loss: 0.6220 - val_accuracy: 0.6474\n",
      "Epoch 3/30\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.6276 - accuracy: 0.6280 - val_loss: 0.6017 - val_accuracy: 0.6517\n",
      "Epoch 4/30\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.6174 - accuracy: 0.6396 - val_loss: 0.5855 - val_accuracy: 0.6846\n",
      "Epoch 5/30\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.6037 - accuracy: 0.6545 - val_loss: 0.5727 - val_accuracy: 0.6991\n",
      "Epoch 6/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5891 - accuracy: 0.6663 - val_loss: 0.5599 - val_accuracy: 0.6937\n",
      "Epoch 7/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5740 - accuracy: 0.6813 - val_loss: 0.5546 - val_accuracy: 0.6911\n",
      "Epoch 8/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5661 - accuracy: 0.6894 - val_loss: 0.5502 - val_accuracy: 0.6933\n",
      "Epoch 9/30\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.5576 - accuracy: 0.6931 - val_loss: 0.5566 - val_accuracy: 0.6940\n",
      "Epoch 10/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5561 - accuracy: 0.6892 - val_loss: 0.5371 - val_accuracy: 0.7024\n",
      "Epoch 11/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5516 - accuracy: 0.7008 - val_loss: 0.5400 - val_accuracy: 0.7042\n",
      "Epoch 12/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5490 - accuracy: 0.6949 - val_loss: 0.5284 - val_accuracy: 0.7107\n",
      "Epoch 13/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5469 - accuracy: 0.7022 - val_loss: 0.5419 - val_accuracy: 0.7099\n",
      "Epoch 14/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5420 - accuracy: 0.6994 - val_loss: 0.5758 - val_accuracy: 0.6901\n",
      "Epoch 15/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5383 - accuracy: 0.7049 - val_loss: 0.5473 - val_accuracy: 0.6969\n",
      "Epoch 16/30\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.5276 - accuracy: 0.7052 - val_loss: 0.5392 - val_accuracy: 0.7016\n",
      "Epoch 17/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5298 - accuracy: 0.7008 - val_loss: 0.5310 - val_accuracy: 0.7031\n",
      "Epoch 18/30\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.5244 - accuracy: 0.7087 - val_loss: 0.5291 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5257 - accuracy: 0.7072 - val_loss: 0.5326 - val_accuracy: 0.7074\n",
      "Epoch 20/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5257 - accuracy: 0.7134 - val_loss: 0.5399 - val_accuracy: 0.6955\n",
      "Epoch 21/30\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.5168 - accuracy: 0.7188 - val_loss: 0.5237 - val_accuracy: 0.7251\n",
      "Epoch 22/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5143 - accuracy: 0.7174 - val_loss: 0.5323 - val_accuracy: 0.7208\n",
      "Epoch 23/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5142 - accuracy: 0.7160 - val_loss: 0.5235 - val_accuracy: 0.7222\n",
      "Epoch 24/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5148 - accuracy: 0.7196 - val_loss: 0.5382 - val_accuracy: 0.6955\n",
      "Epoch 25/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5139 - accuracy: 0.7179 - val_loss: 0.5347 - val_accuracy: 0.7132\n",
      "Epoch 26/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5038 - accuracy: 0.7289 - val_loss: 0.5268 - val_accuracy: 0.7092\n",
      "Epoch 27/30\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.5034 - accuracy: 0.7262 - val_loss: 0.5383 - val_accuracy: 0.7089\n",
      "Epoch 28/30\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.5011 - accuracy: 0.7269 - val_loss: 0.5220 - val_accuracy: 0.7204\n",
      "Epoch 29/30\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.5083 - accuracy: 0.7281 - val_loss: 0.5281 - val_accuracy: 0.7262\n",
      "Epoch 30/30\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.4978 - accuracy: 0.7289 - val_loss: 0.5299 - val_accuracy: 0.7146\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7146473779385172\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_27 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_81 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_82 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_83 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "239/239 [==============================] - 4s 11ms/step - loss: 0.7198 - accuracy: 0.5381 - val_loss: 0.6838 - val_accuracy: 0.5841\n",
      "Epoch 2/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.6610 - accuracy: 0.5940 - val_loss: 0.6237 - val_accuracy: 0.6665\n",
      "Epoch 3/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.6262 - accuracy: 0.6284 - val_loss: 0.5997 - val_accuracy: 0.6622\n",
      "Epoch 4/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.6165 - accuracy: 0.6373 - val_loss: 0.5768 - val_accuracy: 0.6908\n",
      "Epoch 5/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.6055 - accuracy: 0.6583 - val_loss: 0.5745 - val_accuracy: 0.6868\n",
      "Epoch 6/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5910 - accuracy: 0.6706 - val_loss: 0.5764 - val_accuracy: 0.6897\n",
      "Epoch 7/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5779 - accuracy: 0.6768 - val_loss: 0.5465 - val_accuracy: 0.7146\n",
      "Epoch 8/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5660 - accuracy: 0.6898 - val_loss: 0.5359 - val_accuracy: 0.7150\n",
      "Epoch 9/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5581 - accuracy: 0.6923 - val_loss: 0.5571 - val_accuracy: 0.6976\n",
      "Epoch 10/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5535 - accuracy: 0.6901 - val_loss: 0.5391 - val_accuracy: 0.7049\n",
      "Epoch 11/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5474 - accuracy: 0.6996 - val_loss: 0.5381 - val_accuracy: 0.7096\n",
      "Epoch 12/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5484 - accuracy: 0.6952 - val_loss: 0.5248 - val_accuracy: 0.7222\n",
      "Epoch 13/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5431 - accuracy: 0.7035 - val_loss: 0.5349 - val_accuracy: 0.7027\n",
      "Epoch 14/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5352 - accuracy: 0.7025 - val_loss: 0.5390 - val_accuracy: 0.6976\n",
      "Epoch 15/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5368 - accuracy: 0.7031 - val_loss: 0.5420 - val_accuracy: 0.6980\n",
      "Epoch 16/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5243 - accuracy: 0.7204 - val_loss: 0.5292 - val_accuracy: 0.7052\n",
      "Epoch 17/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5197 - accuracy: 0.7177 - val_loss: 0.5307 - val_accuracy: 0.7052\n",
      "Epoch 18/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5251 - accuracy: 0.7145 - val_loss: 0.5265 - val_accuracy: 0.6980\n",
      "Epoch 19/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5192 - accuracy: 0.7168 - val_loss: 0.5283 - val_accuracy: 0.7034\n",
      "Epoch 20/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5188 - accuracy: 0.7176 - val_loss: 0.5344 - val_accuracy: 0.6908\n",
      "Epoch 21/30\n",
      "239/239 [==============================] - 3s 12ms/step - loss: 0.5166 - accuracy: 0.7165 - val_loss: 0.5201 - val_accuracy: 0.7154\n",
      "Epoch 22/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5092 - accuracy: 0.7185 - val_loss: 0.5167 - val_accuracy: 0.7208\n",
      "Epoch 23/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5150 - accuracy: 0.7224 - val_loss: 0.5141 - val_accuracy: 0.7291\n",
      "Epoch 24/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5077 - accuracy: 0.7283 - val_loss: 0.5207 - val_accuracy: 0.7179\n",
      "Epoch 25/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5095 - accuracy: 0.7219 - val_loss: 0.5161 - val_accuracy: 0.7298\n",
      "Epoch 26/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5050 - accuracy: 0.7219 - val_loss: 0.5225 - val_accuracy: 0.7168\n",
      "Epoch 27/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5019 - accuracy: 0.7298 - val_loss: 0.5303 - val_accuracy: 0.7121\n",
      "Epoch 28/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5026 - accuracy: 0.7266 - val_loss: 0.5089 - val_accuracy: 0.7280\n",
      "Epoch 29/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.5022 - accuracy: 0.7276 - val_loss: 0.5167 - val_accuracy: 0.7233\n",
      "Epoch 30/30\n",
      "239/239 [==============================] - 3s 11ms/step - loss: 0.4961 - accuracy: 0.7298 - val_loss: 0.5210 - val_accuracy: 0.7233\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7233273056057866\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_28 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_84 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_86 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "231/231 [==============================] - 4s 12ms/step - loss: 0.7202 - accuracy: 0.5495 - val_loss: 0.6796 - val_accuracy: 0.5826\n",
      "Epoch 2/30\n",
      "231/231 [==============================] - 2s 11ms/step - loss: 0.6579 - accuracy: 0.6056 - val_loss: 0.6268 - val_accuracy: 0.6445\n",
      "Epoch 3/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.6252 - accuracy: 0.6253 - val_loss: 0.6027 - val_accuracy: 0.6723\n",
      "Epoch 4/30\n",
      "231/231 [==============================] - 2s 11ms/step - loss: 0.6155 - accuracy: 0.6449 - val_loss: 0.5855 - val_accuracy: 0.6723\n",
      "Epoch 5/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.6055 - accuracy: 0.6549 - val_loss: 0.5757 - val_accuracy: 0.6926\n",
      "Epoch 6/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5858 - accuracy: 0.6769 - val_loss: 0.5703 - val_accuracy: 0.6857\n",
      "Epoch 7/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5775 - accuracy: 0.6814 - val_loss: 0.5684 - val_accuracy: 0.6854\n",
      "Epoch 8/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5692 - accuracy: 0.6842 - val_loss: 0.5450 - val_accuracy: 0.7038\n",
      "Epoch 9/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5621 - accuracy: 0.6892 - val_loss: 0.5522 - val_accuracy: 0.7013\n",
      "Epoch 10/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5568 - accuracy: 0.6892 - val_loss: 0.5358 - val_accuracy: 0.7045\n",
      "Epoch 11/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5476 - accuracy: 0.6942 - val_loss: 0.5441 - val_accuracy: 0.7009\n",
      "Epoch 12/30\n",
      "231/231 [==============================] - 3s 12ms/step - loss: 0.5535 - accuracy: 0.6942 - val_loss: 0.5318 - val_accuracy: 0.7222\n",
      "Epoch 13/30\n",
      "231/231 [==============================] - 3s 12ms/step - loss: 0.5506 - accuracy: 0.6940 - val_loss: 0.5416 - val_accuracy: 0.7107\n",
      "Epoch 14/30\n",
      "231/231 [==============================] - 2s 11ms/step - loss: 0.5335 - accuracy: 0.7087 - val_loss: 0.5607 - val_accuracy: 0.6839\n",
      "Epoch 15/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5394 - accuracy: 0.7070 - val_loss: 0.5377 - val_accuracy: 0.7034\n",
      "Epoch 16/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5300 - accuracy: 0.7160 - val_loss: 0.5288 - val_accuracy: 0.7056\n",
      "Epoch 17/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5256 - accuracy: 0.7111 - val_loss: 0.5291 - val_accuracy: 0.7150\n",
      "Epoch 18/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5311 - accuracy: 0.7039 - val_loss: 0.5273 - val_accuracy: 0.7016\n",
      "Epoch 19/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5242 - accuracy: 0.7098 - val_loss: 0.5209 - val_accuracy: 0.7110\n",
      "Epoch 20/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5229 - accuracy: 0.7109 - val_loss: 0.5317 - val_accuracy: 0.7056\n",
      "Epoch 21/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5164 - accuracy: 0.7219 - val_loss: 0.5450 - val_accuracy: 0.7081\n",
      "Epoch 22/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5185 - accuracy: 0.7157 - val_loss: 0.5258 - val_accuracy: 0.7284\n",
      "Epoch 23/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5157 - accuracy: 0.7252 - val_loss: 0.5220 - val_accuracy: 0.7183\n",
      "Epoch 24/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5137 - accuracy: 0.7248 - val_loss: 0.5333 - val_accuracy: 0.7118\n",
      "Epoch 25/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5151 - accuracy: 0.7208 - val_loss: 0.5339 - val_accuracy: 0.7056\n",
      "Epoch 26/30\n",
      "231/231 [==============================] - 3s 12ms/step - loss: 0.5082 - accuracy: 0.7258 - val_loss: 0.5273 - val_accuracy: 0.7114\n",
      "Epoch 27/30\n",
      "231/231 [==============================] - 2s 11ms/step - loss: 0.5065 - accuracy: 0.7250 - val_loss: 0.5411 - val_accuracy: 0.7136\n",
      "Epoch 28/30\n",
      "231/231 [==============================] - 3s 11ms/step - loss: 0.5059 - accuracy: 0.7286 - val_loss: 0.5284 - val_accuracy: 0.7251\n",
      "Epoch 29/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5071 - accuracy: 0.7242 - val_loss: 0.5274 - val_accuracy: 0.7212\n",
      "Epoch 30/30\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5002 - accuracy: 0.7357 - val_loss: 0.5272 - val_accuracy: 0.7204\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7204339963833635\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_29 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_87 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_88 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_89 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "223/223 [==============================] - 4s 11ms/step - loss: 0.7271 - accuracy: 0.5334 - val_loss: 0.6834 - val_accuracy: 0.5689\n",
      "Epoch 2/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.6614 - accuracy: 0.5900 - val_loss: 0.6161 - val_accuracy: 0.6590\n",
      "Epoch 3/30\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 0.6272 - accuracy: 0.6388 - val_loss: 0.5993 - val_accuracy: 0.6651\n",
      "Epoch 4/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.6170 - accuracy: 0.6371 - val_loss: 0.5842 - val_accuracy: 0.6774\n",
      "Epoch 5/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.6039 - accuracy: 0.6498 - val_loss: 0.5816 - val_accuracy: 0.6857\n",
      "Epoch 6/30\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 0.5884 - accuracy: 0.6666 - val_loss: 0.5709 - val_accuracy: 0.6919\n",
      "Epoch 7/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5792 - accuracy: 0.6828 - val_loss: 0.5511 - val_accuracy: 0.7009\n",
      "Epoch 8/30\n",
      "223/223 [==============================] - 3s 11ms/step - loss: 0.5666 - accuracy: 0.6810 - val_loss: 0.5524 - val_accuracy: 0.7013\n",
      "Epoch 9/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5572 - accuracy: 0.6957 - val_loss: 0.5578 - val_accuracy: 0.6973\n",
      "Epoch 10/30\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 0.5548 - accuracy: 0.6934 - val_loss: 0.5331 - val_accuracy: 0.7071\n",
      "Epoch 11/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5444 - accuracy: 0.6977 - val_loss: 0.5326 - val_accuracy: 0.7049\n",
      "Epoch 12/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5469 - accuracy: 0.7007 - val_loss: 0.5270 - val_accuracy: 0.7161\n",
      "Epoch 13/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5419 - accuracy: 0.7016 - val_loss: 0.5361 - val_accuracy: 0.7150\n",
      "Epoch 14/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5355 - accuracy: 0.7100 - val_loss: 0.5406 - val_accuracy: 0.6973\n",
      "Epoch 15/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5393 - accuracy: 0.7030 - val_loss: 0.5404 - val_accuracy: 0.7049\n",
      "Epoch 16/30\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 0.5291 - accuracy: 0.7142 - val_loss: 0.5687 - val_accuracy: 0.7071\n",
      "Epoch 17/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5249 - accuracy: 0.7089 - val_loss: 0.5286 - val_accuracy: 0.7114\n",
      "Epoch 18/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5267 - accuracy: 0.7106 - val_loss: 0.5237 - val_accuracy: 0.7013\n",
      "Epoch 19/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5239 - accuracy: 0.7140 - val_loss: 0.5241 - val_accuracy: 0.7146\n",
      "Epoch 20/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5187 - accuracy: 0.7106 - val_loss: 0.5218 - val_accuracy: 0.7139\n",
      "Epoch 21/30\n",
      "223/223 [==============================] - 2s 9ms/step - loss: 0.5190 - accuracy: 0.7120 - val_loss: 0.5199 - val_accuracy: 0.7222\n",
      "Epoch 22/30\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 0.5135 - accuracy: 0.7162 - val_loss: 0.5185 - val_accuracy: 0.7241\n",
      "Epoch 23/30\n",
      "223/223 [==============================] - 3s 11ms/step - loss: 0.5150 - accuracy: 0.7176 - val_loss: 0.5211 - val_accuracy: 0.7298\n",
      "Epoch 24/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5126 - accuracy: 0.7231 - val_loss: 0.5295 - val_accuracy: 0.7139\n",
      "Epoch 25/30\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 0.5094 - accuracy: 0.7236 - val_loss: 0.5260 - val_accuracy: 0.7139\n",
      "Epoch 26/30\n",
      "223/223 [==============================] - 2s 10ms/step - loss: 0.5093 - accuracy: 0.7238 - val_loss: 0.5244 - val_accuracy: 0.7081\n",
      "Epoch 27/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5019 - accuracy: 0.7248 - val_loss: 0.5270 - val_accuracy: 0.7132\n",
      "Epoch 28/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.5016 - accuracy: 0.7283 - val_loss: 0.5192 - val_accuracy: 0.7233\n",
      "Epoch 29/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.4994 - accuracy: 0.7328 - val_loss: 0.5322 - val_accuracy: 0.7193\n",
      "Epoch 30/30\n",
      "223/223 [==============================] - 2s 11ms/step - loss: 0.4934 - accuracy: 0.7368 - val_loss: 0.5270 - val_accuracy: 0.7165\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7164556962025317\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_30 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_90 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_91 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_92 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "216/216 [==============================] - 4s 12ms/step - loss: 0.7252 - accuracy: 0.5350 - val_loss: 0.6830 - val_accuracy: 0.5689\n",
      "Epoch 2/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6612 - accuracy: 0.5920 - val_loss: 0.6250 - val_accuracy: 0.6597\n",
      "Epoch 3/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6297 - accuracy: 0.6329 - val_loss: 0.6037 - val_accuracy: 0.6691\n",
      "Epoch 4/30\n",
      "216/216 [==============================] - 2s 10ms/step - loss: 0.6149 - accuracy: 0.6410 - val_loss: 0.5992 - val_accuracy: 0.6597\n",
      "Epoch 5/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.6038 - accuracy: 0.6489 - val_loss: 0.5713 - val_accuracy: 0.6803\n",
      "Epoch 6/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5853 - accuracy: 0.6697 - val_loss: 0.5660 - val_accuracy: 0.6774\n",
      "Epoch 7/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5719 - accuracy: 0.6869 - val_loss: 0.5562 - val_accuracy: 0.6911\n",
      "Epoch 8/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5672 - accuracy: 0.6814 - val_loss: 0.5434 - val_accuracy: 0.6987\n",
      "Epoch 9/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5599 - accuracy: 0.6869 - val_loss: 0.5585 - val_accuracy: 0.7067\n",
      "Epoch 10/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5529 - accuracy: 0.6965 - val_loss: 0.5337 - val_accuracy: 0.6987\n",
      "Epoch 11/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5466 - accuracy: 0.6999 - val_loss: 0.5311 - val_accuracy: 0.7201\n",
      "Epoch 12/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5451 - accuracy: 0.6988 - val_loss: 0.5250 - val_accuracy: 0.7324\n",
      "Epoch 13/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5413 - accuracy: 0.6999 - val_loss: 0.5363 - val_accuracy: 0.7114\n",
      "Epoch 14/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5330 - accuracy: 0.7090 - val_loss: 0.5433 - val_accuracy: 0.6933\n",
      "Epoch 15/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5308 - accuracy: 0.7132 - val_loss: 0.5367 - val_accuracy: 0.7016\n",
      "Epoch 16/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5246 - accuracy: 0.7157 - val_loss: 0.5343 - val_accuracy: 0.7056\n",
      "Epoch 17/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5253 - accuracy: 0.7126 - val_loss: 0.5317 - val_accuracy: 0.7132\n",
      "Epoch 18/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5242 - accuracy: 0.7208 - val_loss: 0.5253 - val_accuracy: 0.6998\n",
      "Epoch 19/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5179 - accuracy: 0.7217 - val_loss: 0.5281 - val_accuracy: 0.7110\n",
      "Epoch 20/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5178 - accuracy: 0.7146 - val_loss: 0.5228 - val_accuracy: 0.7132\n",
      "Epoch 21/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5140 - accuracy: 0.7213 - val_loss: 0.5199 - val_accuracy: 0.7255\n",
      "Epoch 22/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5138 - accuracy: 0.7247 - val_loss: 0.5250 - val_accuracy: 0.7259\n",
      "Epoch 23/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5105 - accuracy: 0.7197 - val_loss: 0.5237 - val_accuracy: 0.7277\n",
      "Epoch 24/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5097 - accuracy: 0.7292 - val_loss: 0.5261 - val_accuracy: 0.7150\n",
      "Epoch 25/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5060 - accuracy: 0.7321 - val_loss: 0.5310 - val_accuracy: 0.7193\n",
      "Epoch 26/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.5030 - accuracy: 0.7289 - val_loss: 0.5331 - val_accuracy: 0.7099\n",
      "Epoch 27/30\n",
      "216/216 [==============================] - 3s 13ms/step - loss: 0.4983 - accuracy: 0.7225 - val_loss: 0.5417 - val_accuracy: 0.7230\n",
      "Epoch 28/30\n",
      "216/216 [==============================] - 3s 16ms/step - loss: 0.4983 - accuracy: 0.7259 - val_loss: 0.5220 - val_accuracy: 0.7172\n",
      "Epoch 29/30\n",
      "216/216 [==============================] - 3s 12ms/step - loss: 0.5012 - accuracy: 0.7320 - val_loss: 0.5451 - val_accuracy: 0.7172\n",
      "Epoch 30/30\n",
      "216/216 [==============================] - 2s 11ms/step - loss: 0.4898 - accuracy: 0.7343 - val_loss: 0.5326 - val_accuracy: 0.7172\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7171790235081374\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_31 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_93 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_94 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_95 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "209/209 [==============================] - 4s 11ms/step - loss: 0.7279 - accuracy: 0.5376 - val_loss: 0.6817 - val_accuracy: 0.5725\n",
      "Epoch 2/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.6650 - accuracy: 0.5887 - val_loss: 0.6285 - val_accuracy: 0.6383\n",
      "Epoch 3/30\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.6301 - accuracy: 0.6207 - val_loss: 0.6006 - val_accuracy: 0.6637\n",
      "Epoch 4/30\n",
      "209/209 [==============================] - 3s 12ms/step - loss: 0.6163 - accuracy: 0.6418 - val_loss: 0.5929 - val_accuracy: 0.6799\n",
      "Epoch 5/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.6036 - accuracy: 0.6477 - val_loss: 0.5873 - val_accuracy: 0.6535\n",
      "Epoch 6/30\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.5807 - accuracy: 0.6800 - val_loss: 0.5621 - val_accuracy: 0.6781\n",
      "Epoch 7/30\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.5766 - accuracy: 0.6768 - val_loss: 0.5554 - val_accuracy: 0.6814\n",
      "Epoch 8/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5649 - accuracy: 0.6783 - val_loss: 0.5464 - val_accuracy: 0.6962\n",
      "Epoch 9/30\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.5660 - accuracy: 0.6883 - val_loss: 0.5566 - val_accuracy: 0.6926\n",
      "Epoch 10/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5525 - accuracy: 0.6921 - val_loss: 0.5326 - val_accuracy: 0.7063\n",
      "Epoch 11/30\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.5446 - accuracy: 0.6979 - val_loss: 0.5470 - val_accuracy: 0.7074\n",
      "Epoch 12/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5394 - accuracy: 0.7075 - val_loss: 0.5304 - val_accuracy: 0.7143\n",
      "Epoch 13/30\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.5435 - accuracy: 0.6942 - val_loss: 0.5398 - val_accuracy: 0.7067\n",
      "Epoch 14/30\n",
      "209/209 [==============================] - 2s 12ms/step - loss: 0.5386 - accuracy: 0.7078 - val_loss: 0.5384 - val_accuracy: 0.6893\n",
      "Epoch 15/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5353 - accuracy: 0.7047 - val_loss: 0.5437 - val_accuracy: 0.6962\n",
      "Epoch 16/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5254 - accuracy: 0.7137 - val_loss: 0.5461 - val_accuracy: 0.6995\n",
      "Epoch 17/30\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.5296 - accuracy: 0.7126 - val_loss: 0.5366 - val_accuracy: 0.7168\n",
      "Epoch 18/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5210 - accuracy: 0.7135 - val_loss: 0.5369 - val_accuracy: 0.6966\n",
      "Epoch 19/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5205 - accuracy: 0.7109 - val_loss: 0.5284 - val_accuracy: 0.7172\n",
      "Epoch 20/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5207 - accuracy: 0.7073 - val_loss: 0.5269 - val_accuracy: 0.7016\n",
      "Epoch 21/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5115 - accuracy: 0.7233 - val_loss: 0.5262 - val_accuracy: 0.7208\n",
      "Epoch 22/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5114 - accuracy: 0.7217 - val_loss: 0.5226 - val_accuracy: 0.7295\n",
      "Epoch 23/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5133 - accuracy: 0.7239 - val_loss: 0.5245 - val_accuracy: 0.7280\n",
      "Epoch 24/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5101 - accuracy: 0.7202 - val_loss: 0.5431 - val_accuracy: 0.7042\n",
      "Epoch 25/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5150 - accuracy: 0.7199 - val_loss: 0.5304 - val_accuracy: 0.7172\n",
      "Epoch 26/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5019 - accuracy: 0.7276 - val_loss: 0.5389 - val_accuracy: 0.7081\n",
      "Epoch 27/30\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.4995 - accuracy: 0.7266 - val_loss: 0.5333 - val_accuracy: 0.7143\n",
      "Epoch 28/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.4969 - accuracy: 0.7250 - val_loss: 0.5220 - val_accuracy: 0.7204\n",
      "Epoch 29/30\n",
      "209/209 [==============================] - 2s 11ms/step - loss: 0.5008 - accuracy: 0.7346 - val_loss: 0.5269 - val_accuracy: 0.7280\n",
      "Epoch 30/30\n",
      "209/209 [==============================] - 2s 10ms/step - loss: 0.4961 - accuracy: 0.7326 - val_loss: 0.5275 - val_accuracy: 0.7212\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7211573236889692\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_32 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_32 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_97 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_98 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "202/202 [==============================] - 3s 11ms/step - loss: 0.7264 - accuracy: 0.5342 - val_loss: 0.6931 - val_accuracy: 0.5530\n",
      "Epoch 2/30\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.6655 - accuracy: 0.5931 - val_loss: 0.6261 - val_accuracy: 0.6452\n",
      "Epoch 3/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.6290 - accuracy: 0.6353 - val_loss: 0.5996 - val_accuracy: 0.6637\n",
      "Epoch 4/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.6172 - accuracy: 0.6376 - val_loss: 0.5841 - val_accuracy: 0.6756\n",
      "Epoch 5/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.6051 - accuracy: 0.6566 - val_loss: 0.5828 - val_accuracy: 0.6911\n",
      "Epoch 6/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5864 - accuracy: 0.6729 - val_loss: 0.5632 - val_accuracy: 0.6647\n",
      "Epoch 7/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5723 - accuracy: 0.6872 - val_loss: 0.5471 - val_accuracy: 0.7089\n",
      "Epoch 8/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5644 - accuracy: 0.6900 - val_loss: 0.5419 - val_accuracy: 0.7038\n",
      "Epoch 9/30\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.5608 - accuracy: 0.6825 - val_loss: 0.5447 - val_accuracy: 0.7027\n",
      "Epoch 10/30\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.5550 - accuracy: 0.7013 - val_loss: 0.5319 - val_accuracy: 0.7146\n",
      "Epoch 11/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5437 - accuracy: 0.7016 - val_loss: 0.5330 - val_accuracy: 0.7118\n",
      "Epoch 12/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5438 - accuracy: 0.7019 - val_loss: 0.5241 - val_accuracy: 0.7168\n",
      "Epoch 13/30\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.5345 - accuracy: 0.7087 - val_loss: 0.5315 - val_accuracy: 0.6980\n",
      "Epoch 14/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5333 - accuracy: 0.7036 - val_loss: 0.5325 - val_accuracy: 0.7067\n",
      "Epoch 15/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5354 - accuracy: 0.7050 - val_loss: 0.5314 - val_accuracy: 0.7259\n",
      "Epoch 16/30\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.5256 - accuracy: 0.7123 - val_loss: 0.5407 - val_accuracy: 0.7052\n",
      "Epoch 17/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5263 - accuracy: 0.7129 - val_loss: 0.5310 - val_accuracy: 0.7081\n",
      "Epoch 18/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5235 - accuracy: 0.7174 - val_loss: 0.5227 - val_accuracy: 0.7067\n",
      "Epoch 19/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5139 - accuracy: 0.7194 - val_loss: 0.5172 - val_accuracy: 0.7244\n",
      "Epoch 20/30\n",
      "202/202 [==============================] - 2s 12ms/step - loss: 0.5167 - accuracy: 0.7159 - val_loss: 0.5351 - val_accuracy: 0.6929\n",
      "Epoch 21/30\n",
      "202/202 [==============================] - 2s 12ms/step - loss: 0.5129 - accuracy: 0.7205 - val_loss: 0.5184 - val_accuracy: 0.7284\n",
      "Epoch 22/30\n",
      "202/202 [==============================] - 2s 12ms/step - loss: 0.5049 - accuracy: 0.7224 - val_loss: 0.5315 - val_accuracy: 0.7259\n",
      "Epoch 23/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5127 - accuracy: 0.7245 - val_loss: 0.5174 - val_accuracy: 0.7316\n",
      "Epoch 24/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5106 - accuracy: 0.7199 - val_loss: 0.5294 - val_accuracy: 0.7146\n",
      "Epoch 25/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5093 - accuracy: 0.7252 - val_loss: 0.5286 - val_accuracy: 0.7096\n",
      "Epoch 26/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.5032 - accuracy: 0.7245 - val_loss: 0.5319 - val_accuracy: 0.7067\n",
      "Epoch 27/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.4977 - accuracy: 0.7300 - val_loss: 0.5405 - val_accuracy: 0.7154\n",
      "Epoch 28/30\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.4975 - accuracy: 0.7306 - val_loss: 0.5135 - val_accuracy: 0.7255\n",
      "Epoch 29/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.4960 - accuracy: 0.7337 - val_loss: 0.5326 - val_accuracy: 0.7172\n",
      "Epoch 30/30\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.4965 - accuracy: 0.7345 - val_loss: 0.5301 - val_accuracy: 0.7201\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7200723327305606\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_33 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_99 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_100 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "196/196 [==============================] - 4s 12ms/step - loss: 0.7252 - accuracy: 0.5298 - val_loss: 0.6948 - val_accuracy: 0.5425\n",
      "Epoch 2/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.6664 - accuracy: 0.5875 - val_loss: 0.6241 - val_accuracy: 0.6604\n",
      "Epoch 3/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.6262 - accuracy: 0.6300 - val_loss: 0.6020 - val_accuracy: 0.6637\n",
      "Epoch 4/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.6148 - accuracy: 0.6362 - val_loss: 0.6082 - val_accuracy: 0.6506\n",
      "Epoch 5/30\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.6123 - accuracy: 0.6475 - val_loss: 0.5743 - val_accuracy: 0.6922\n",
      "Epoch 6/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5858 - accuracy: 0.6762 - val_loss: 0.5618 - val_accuracy: 0.6861\n",
      "Epoch 7/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5772 - accuracy: 0.6827 - val_loss: 0.5568 - val_accuracy: 0.6929\n",
      "Epoch 8/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5669 - accuracy: 0.6814 - val_loss: 0.5443 - val_accuracy: 0.7078\n",
      "Epoch 9/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5564 - accuracy: 0.6898 - val_loss: 0.5505 - val_accuracy: 0.7139\n",
      "Epoch 10/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5544 - accuracy: 0.6915 - val_loss: 0.5308 - val_accuracy: 0.7063\n",
      "Epoch 11/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5434 - accuracy: 0.7000 - val_loss: 0.5304 - val_accuracy: 0.7190\n",
      "Epoch 12/30\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.5392 - accuracy: 0.7100 - val_loss: 0.5357 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.5376 - accuracy: 0.7058 - val_loss: 0.5345 - val_accuracy: 0.7165\n",
      "Epoch 14/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5312 - accuracy: 0.7083 - val_loss: 0.5390 - val_accuracy: 0.7002\n",
      "Epoch 15/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5329 - accuracy: 0.7118 - val_loss: 0.5317 - val_accuracy: 0.7110\n",
      "Epoch 16/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5215 - accuracy: 0.7134 - val_loss: 0.5409 - val_accuracy: 0.7013\n",
      "Epoch 17/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5220 - accuracy: 0.7117 - val_loss: 0.5384 - val_accuracy: 0.7016\n",
      "Epoch 18/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5237 - accuracy: 0.7112 - val_loss: 0.5256 - val_accuracy: 0.7056\n",
      "Epoch 19/30\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5127 - accuracy: 0.7180 - val_loss: 0.5156 - val_accuracy: 0.7262\n",
      "Epoch 20/30\n",
      "196/196 [==============================] - 2s 10ms/step - loss: 0.5100 - accuracy: 0.7230 - val_loss: 0.5268 - val_accuracy: 0.7078\n",
      "Epoch 21/30\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.5083 - accuracy: 0.7245 - val_loss: 0.5204 - val_accuracy: 0.7204\n",
      "Epoch 22/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5089 - accuracy: 0.7259 - val_loss: 0.5137 - val_accuracy: 0.7230\n",
      "Epoch 23/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5084 - accuracy: 0.7247 - val_loss: 0.5226 - val_accuracy: 0.7280\n",
      "Epoch 24/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5074 - accuracy: 0.7259 - val_loss: 0.5228 - val_accuracy: 0.7237\n",
      "Epoch 25/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.5065 - accuracy: 0.7298 - val_loss: 0.5252 - val_accuracy: 0.7277\n",
      "Epoch 26/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.4968 - accuracy: 0.7292 - val_loss: 0.5325 - val_accuracy: 0.7042\n",
      "Epoch 27/30\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.4969 - accuracy: 0.7292 - val_loss: 0.5246 - val_accuracy: 0.7179\n",
      "Epoch 28/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.4956 - accuracy: 0.7314 - val_loss: 0.5205 - val_accuracy: 0.7230\n",
      "Epoch 29/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.4955 - accuracy: 0.7366 - val_loss: 0.5286 - val_accuracy: 0.7121\n",
      "Epoch 30/30\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.4968 - accuracy: 0.7346 - val_loss: 0.5318 - val_accuracy: 0.7197\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7197106690777577\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_34 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_102 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_103 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_104 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "190/190 [==============================] - 4s 14ms/step - loss: 0.7231 - accuracy: 0.5356 - val_loss: 0.6783 - val_accuracy: 0.5577\n",
      "Epoch 2/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6545 - accuracy: 0.5999 - val_loss: 0.6206 - val_accuracy: 0.6532\n",
      "Epoch 3/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.6207 - accuracy: 0.6370 - val_loss: 0.6021 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "190/190 [==============================] - 3s 14ms/step - loss: 0.6150 - accuracy: 0.6478 - val_loss: 0.5933 - val_accuracy: 0.6539\n",
      "Epoch 5/30\n",
      "190/190 [==============================] - 3s 13ms/step - loss: 0.6031 - accuracy: 0.6563 - val_loss: 0.5928 - val_accuracy: 0.6770\n",
      "Epoch 6/30\n",
      "190/190 [==============================] - 3s 13ms/step - loss: 0.5879 - accuracy: 0.6684 - val_loss: 0.5661 - val_accuracy: 0.6911\n",
      "Epoch 7/30\n",
      "190/190 [==============================] - 2s 13ms/step - loss: 0.5791 - accuracy: 0.6841 - val_loss: 0.5527 - val_accuracy: 0.7049\n",
      "Epoch 8/30\n",
      "190/190 [==============================] - 3s 13ms/step - loss: 0.5671 - accuracy: 0.6861 - val_loss: 0.5578 - val_accuracy: 0.6774\n",
      "Epoch 9/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5568 - accuracy: 0.6861 - val_loss: 0.5436 - val_accuracy: 0.7045\n",
      "Epoch 10/30\n",
      "190/190 [==============================] - 2s 13ms/step - loss: 0.5533 - accuracy: 0.6983 - val_loss: 0.5380 - val_accuracy: 0.7013\n",
      "Epoch 11/30\n",
      "190/190 [==============================] - 3s 13ms/step - loss: 0.5475 - accuracy: 0.6996 - val_loss: 0.5408 - val_accuracy: 0.7042\n",
      "Epoch 12/30\n",
      "190/190 [==============================] - 2s 13ms/step - loss: 0.5390 - accuracy: 0.7025 - val_loss: 0.5251 - val_accuracy: 0.7298\n",
      "Epoch 13/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5389 - accuracy: 0.7070 - val_loss: 0.5462 - val_accuracy: 0.7005\n",
      "Epoch 14/30\n",
      "190/190 [==============================] - 2s 13ms/step - loss: 0.5383 - accuracy: 0.7066 - val_loss: 0.5458 - val_accuracy: 0.7074\n",
      "Epoch 15/30\n",
      "190/190 [==============================] - 2s 13ms/step - loss: 0.5365 - accuracy: 0.7090 - val_loss: 0.5354 - val_accuracy: 0.7107\n",
      "Epoch 16/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5237 - accuracy: 0.7126 - val_loss: 0.5367 - val_accuracy: 0.7110\n",
      "Epoch 17/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5250 - accuracy: 0.7143 - val_loss: 0.5277 - val_accuracy: 0.7262\n",
      "Epoch 18/30\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.5179 - accuracy: 0.7200 - val_loss: 0.5357 - val_accuracy: 0.6980\n",
      "Epoch 19/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5142 - accuracy: 0.7149 - val_loss: 0.5304 - val_accuracy: 0.7136\n",
      "Epoch 20/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5212 - accuracy: 0.7174 - val_loss: 0.5223 - val_accuracy: 0.7089\n",
      "Epoch 21/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5076 - accuracy: 0.7272 - val_loss: 0.5210 - val_accuracy: 0.7331\n",
      "Epoch 22/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5095 - accuracy: 0.7202 - val_loss: 0.5194 - val_accuracy: 0.7302\n",
      "Epoch 23/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5128 - accuracy: 0.7213 - val_loss: 0.5195 - val_accuracy: 0.7349\n",
      "Epoch 24/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5069 - accuracy: 0.7275 - val_loss: 0.5265 - val_accuracy: 0.7201\n",
      "Epoch 25/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5053 - accuracy: 0.7289 - val_loss: 0.5348 - val_accuracy: 0.7197\n",
      "Epoch 26/30\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.5033 - accuracy: 0.7300 - val_loss: 0.5324 - val_accuracy: 0.7103\n",
      "Epoch 27/30\n",
      "190/190 [==============================] - 2s 13ms/step - loss: 0.5000 - accuracy: 0.7281 - val_loss: 0.5343 - val_accuracy: 0.7165\n",
      "Epoch 28/30\n",
      "190/190 [==============================] - 2s 11ms/step - loss: 0.4917 - accuracy: 0.7351 - val_loss: 0.5243 - val_accuracy: 0.7157\n",
      "Epoch 29/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.5029 - accuracy: 0.7266 - val_loss: 0.5354 - val_accuracy: 0.7136\n",
      "Epoch 30/30\n",
      "190/190 [==============================] - 2s 12ms/step - loss: 0.4885 - accuracy: 0.7328 - val_loss: 0.5338 - val_accuracy: 0.7193\n",
      "87/87 [==============================] - 0s 4ms/step\n",
      "0.7193490054249548\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_35 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_105 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_106 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_107 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "185/185 [==============================] - 3s 12ms/step - loss: 0.7225 - accuracy: 0.5291 - val_loss: 0.6767 - val_accuracy: 0.5769\n",
      "Epoch 2/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.6645 - accuracy: 0.5906 - val_loss: 0.6309 - val_accuracy: 0.6575\n",
      "Epoch 3/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.6331 - accuracy: 0.6184 - val_loss: 0.5976 - val_accuracy: 0.6756\n",
      "Epoch 4/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.6181 - accuracy: 0.6435 - val_loss: 0.5867 - val_accuracy: 0.6651\n",
      "Epoch 5/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.6031 - accuracy: 0.6594 - val_loss: 0.5727 - val_accuracy: 0.6911\n",
      "Epoch 6/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5834 - accuracy: 0.6779 - val_loss: 0.5523 - val_accuracy: 0.6933\n",
      "Epoch 7/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5678 - accuracy: 0.6942 - val_loss: 0.5433 - val_accuracy: 0.7024\n",
      "Epoch 8/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5677 - accuracy: 0.6797 - val_loss: 0.5528 - val_accuracy: 0.6969\n",
      "Epoch 9/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5565 - accuracy: 0.6945 - val_loss: 0.5523 - val_accuracy: 0.7067\n",
      "Epoch 10/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5483 - accuracy: 0.6920 - val_loss: 0.5339 - val_accuracy: 0.7049\n",
      "Epoch 11/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5455 - accuracy: 0.7007 - val_loss: 0.5318 - val_accuracy: 0.7118\n",
      "Epoch 12/30\n",
      "185/185 [==============================] - 2s 11ms/step - loss: 0.5428 - accuracy: 0.7030 - val_loss: 0.5281 - val_accuracy: 0.7183\n",
      "Epoch 13/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5381 - accuracy: 0.7027 - val_loss: 0.5328 - val_accuracy: 0.7099\n",
      "Epoch 14/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5337 - accuracy: 0.7124 - val_loss: 0.5378 - val_accuracy: 0.6966\n",
      "Epoch 15/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5335 - accuracy: 0.7101 - val_loss: 0.5416 - val_accuracy: 0.7002\n",
      "Epoch 16/30\n",
      "185/185 [==============================] - 2s 11ms/step - loss: 0.5212 - accuracy: 0.7188 - val_loss: 0.5341 - val_accuracy: 0.7081\n",
      "Epoch 17/30\n",
      "185/185 [==============================] - 2s 11ms/step - loss: 0.5226 - accuracy: 0.7173 - val_loss: 0.5311 - val_accuracy: 0.7085\n",
      "Epoch 18/30\n",
      "185/185 [==============================] - 2s 13ms/step - loss: 0.5178 - accuracy: 0.7190 - val_loss: 0.5260 - val_accuracy: 0.7089\n",
      "Epoch 19/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5123 - accuracy: 0.7152 - val_loss: 0.5183 - val_accuracy: 0.7306\n",
      "Epoch 20/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5150 - accuracy: 0.7174 - val_loss: 0.5328 - val_accuracy: 0.6948\n",
      "Epoch 21/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5087 - accuracy: 0.7252 - val_loss: 0.5180 - val_accuracy: 0.7298\n",
      "Epoch 22/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5046 - accuracy: 0.7211 - val_loss: 0.5167 - val_accuracy: 0.7309\n",
      "Epoch 23/30\n",
      "185/185 [==============================] - 2s 11ms/step - loss: 0.5099 - accuracy: 0.7211 - val_loss: 0.5197 - val_accuracy: 0.7251\n",
      "Epoch 24/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5042 - accuracy: 0.7252 - val_loss: 0.5340 - val_accuracy: 0.7168\n",
      "Epoch 25/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5072 - accuracy: 0.7244 - val_loss: 0.5229 - val_accuracy: 0.7096\n",
      "Epoch 26/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.5020 - accuracy: 0.7248 - val_loss: 0.5169 - val_accuracy: 0.7190\n",
      "Epoch 27/30\n",
      "185/185 [==============================] - 2s 11ms/step - loss: 0.4954 - accuracy: 0.7340 - val_loss: 0.5283 - val_accuracy: 0.7190\n",
      "Epoch 28/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.4943 - accuracy: 0.7373 - val_loss: 0.5229 - val_accuracy: 0.7241\n",
      "Epoch 29/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.4933 - accuracy: 0.7373 - val_loss: 0.5285 - val_accuracy: 0.7284\n",
      "Epoch 30/30\n",
      "185/185 [==============================] - 2s 12ms/step - loss: 0.4940 - accuracy: 0.7323 - val_loss: 0.5259 - val_accuracy: 0.7204\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7204339963833635\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_36 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_36 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_108 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_110 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "180/180 [==============================] - 3s 13ms/step - loss: 0.7248 - accuracy: 0.5356 - val_loss: 0.6858 - val_accuracy: 0.5548\n",
      "Epoch 2/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.6633 - accuracy: 0.6052 - val_loss: 0.6213 - val_accuracy: 0.6448\n",
      "Epoch 3/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.6287 - accuracy: 0.6253 - val_loss: 0.5970 - val_accuracy: 0.6622\n",
      "Epoch 4/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.6159 - accuracy: 0.6422 - val_loss: 0.5895 - val_accuracy: 0.6644\n",
      "Epoch 5/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.6094 - accuracy: 0.6490 - val_loss: 0.5875 - val_accuracy: 0.6861\n",
      "Epoch 6/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.5875 - accuracy: 0.6717 - val_loss: 0.5566 - val_accuracy: 0.6980\n",
      "Epoch 7/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5740 - accuracy: 0.6785 - val_loss: 0.5533 - val_accuracy: 0.6817\n",
      "Epoch 8/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5670 - accuracy: 0.6818 - val_loss: 0.5475 - val_accuracy: 0.6995\n",
      "Epoch 9/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.5591 - accuracy: 0.6869 - val_loss: 0.5759 - val_accuracy: 0.6904\n",
      "Epoch 10/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5538 - accuracy: 0.6945 - val_loss: 0.5343 - val_accuracy: 0.7067\n",
      "Epoch 11/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5459 - accuracy: 0.7025 - val_loss: 0.5420 - val_accuracy: 0.7042\n",
      "Epoch 12/30\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.5457 - accuracy: 0.7022 - val_loss: 0.5292 - val_accuracy: 0.7157\n",
      "Epoch 13/30\n",
      "180/180 [==============================] - 2s 10ms/step - loss: 0.5394 - accuracy: 0.7053 - val_loss: 0.5379 - val_accuracy: 0.7009\n",
      "Epoch 14/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5383 - accuracy: 0.7036 - val_loss: 0.5438 - val_accuracy: 0.6976\n",
      "Epoch 15/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.5323 - accuracy: 0.7118 - val_loss: 0.5436 - val_accuracy: 0.7002\n",
      "Epoch 16/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.5256 - accuracy: 0.7174 - val_loss: 0.5372 - val_accuracy: 0.7067\n",
      "Epoch 17/30\n",
      "180/180 [==============================] - 2s 10ms/step - loss: 0.5257 - accuracy: 0.7045 - val_loss: 0.5355 - val_accuracy: 0.7063\n",
      "Epoch 18/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5207 - accuracy: 0.7151 - val_loss: 0.5290 - val_accuracy: 0.7002\n",
      "Epoch 19/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5204 - accuracy: 0.7112 - val_loss: 0.5319 - val_accuracy: 0.7157\n",
      "Epoch 20/30\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.5200 - accuracy: 0.7207 - val_loss: 0.5223 - val_accuracy: 0.7107\n",
      "Epoch 21/30\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.5094 - accuracy: 0.7230 - val_loss: 0.5256 - val_accuracy: 0.7186\n",
      "Epoch 22/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5150 - accuracy: 0.7163 - val_loss: 0.5166 - val_accuracy: 0.7262\n",
      "Epoch 23/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.5087 - accuracy: 0.7225 - val_loss: 0.5184 - val_accuracy: 0.7255\n",
      "Epoch 24/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5059 - accuracy: 0.7310 - val_loss: 0.5323 - val_accuracy: 0.7128\n",
      "Epoch 25/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5054 - accuracy: 0.7276 - val_loss: 0.5251 - val_accuracy: 0.7165\n",
      "Epoch 26/30\n",
      "180/180 [==============================] - 2s 13ms/step - loss: 0.5036 - accuracy: 0.7256 - val_loss: 0.5290 - val_accuracy: 0.7143\n",
      "Epoch 27/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5013 - accuracy: 0.7300 - val_loss: 0.5343 - val_accuracy: 0.7125\n",
      "Epoch 28/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.4877 - accuracy: 0.7393 - val_loss: 0.5165 - val_accuracy: 0.7345\n",
      "Epoch 29/30\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5013 - accuracy: 0.7317 - val_loss: 0.5394 - val_accuracy: 0.7056\n",
      "Epoch 30/30\n",
      "180/180 [==============================] - 2s 11ms/step - loss: 0.4897 - accuracy: 0.7379 - val_loss: 0.5279 - val_accuracy: 0.7284\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7283905967450272\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_37 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_111 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_112 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_113 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_151 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "175/175 [==============================] - 4s 13ms/step - loss: 0.7293 - accuracy: 0.5294 - val_loss: 0.6957 - val_accuracy: 0.5407\n",
      "Epoch 2/30\n",
      "175/175 [==============================] - 2s 13ms/step - loss: 0.6644 - accuracy: 0.5918 - val_loss: 0.6194 - val_accuracy: 0.6445\n",
      "Epoch 3/30\n",
      "175/175 [==============================] - 2s 13ms/step - loss: 0.6313 - accuracy: 0.6218 - val_loss: 0.5954 - val_accuracy: 0.6644\n",
      "Epoch 4/30\n",
      "175/175 [==============================] - 2s 13ms/step - loss: 0.6117 - accuracy: 0.6408 - val_loss: 0.5788 - val_accuracy: 0.6702\n",
      "Epoch 5/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.6001 - accuracy: 0.6593 - val_loss: 0.5736 - val_accuracy: 0.6919\n",
      "Epoch 6/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5882 - accuracy: 0.6703 - val_loss: 0.5724 - val_accuracy: 0.6698\n",
      "Epoch 7/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5742 - accuracy: 0.6824 - val_loss: 0.5620 - val_accuracy: 0.6879\n",
      "Epoch 8/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5645 - accuracy: 0.6962 - val_loss: 0.5420 - val_accuracy: 0.7121\n",
      "Epoch 9/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5537 - accuracy: 0.6968 - val_loss: 0.5566 - val_accuracy: 0.7107\n",
      "Epoch 10/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5571 - accuracy: 0.6844 - val_loss: 0.5331 - val_accuracy: 0.7110\n",
      "Epoch 11/30\n",
      "175/175 [==============================] - 2s 13ms/step - loss: 0.5429 - accuracy: 0.7005 - val_loss: 0.5426 - val_accuracy: 0.7042\n",
      "Epoch 12/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5422 - accuracy: 0.7080 - val_loss: 0.5346 - val_accuracy: 0.7118\n",
      "Epoch 13/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5410 - accuracy: 0.7002 - val_loss: 0.5260 - val_accuracy: 0.7230\n",
      "Epoch 14/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5377 - accuracy: 0.7052 - val_loss: 0.5308 - val_accuracy: 0.7013\n",
      "Epoch 15/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5347 - accuracy: 0.7086 - val_loss: 0.5358 - val_accuracy: 0.7118\n",
      "Epoch 16/30\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.5242 - accuracy: 0.7208 - val_loss: 0.5411 - val_accuracy: 0.7103\n",
      "Epoch 17/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5224 - accuracy: 0.7120 - val_loss: 0.5277 - val_accuracy: 0.7208\n",
      "Epoch 18/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5233 - accuracy: 0.7104 - val_loss: 0.5306 - val_accuracy: 0.6987\n",
      "Epoch 19/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5174 - accuracy: 0.7129 - val_loss: 0.5224 - val_accuracy: 0.7219\n",
      "Epoch 20/30\n",
      "175/175 [==============================] - 2s 13ms/step - loss: 0.5160 - accuracy: 0.7138 - val_loss: 0.5338 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5088 - accuracy: 0.7293 - val_loss: 0.5182 - val_accuracy: 0.7313\n",
      "Epoch 22/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5089 - accuracy: 0.7200 - val_loss: 0.5285 - val_accuracy: 0.7320\n",
      "Epoch 23/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5095 - accuracy: 0.7225 - val_loss: 0.5140 - val_accuracy: 0.7288\n",
      "Epoch 24/30\n",
      "175/175 [==============================] - 2s 13ms/step - loss: 0.5057 - accuracy: 0.7289 - val_loss: 0.5297 - val_accuracy: 0.7132\n",
      "Epoch 25/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5030 - accuracy: 0.7310 - val_loss: 0.5283 - val_accuracy: 0.7107\n",
      "Epoch 26/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.4998 - accuracy: 0.7337 - val_loss: 0.5243 - val_accuracy: 0.7154\n",
      "Epoch 27/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.5025 - accuracy: 0.7278 - val_loss: 0.5281 - val_accuracy: 0.7186\n",
      "Epoch 28/30\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.4936 - accuracy: 0.7275 - val_loss: 0.5261 - val_accuracy: 0.7107\n",
      "Epoch 29/30\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.4998 - accuracy: 0.7304 - val_loss: 0.5293 - val_accuracy: 0.7190\n",
      "Epoch 30/30\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.4937 - accuracy: 0.7352 - val_loss: 0.5222 - val_accuracy: 0.7204\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7204339963833635\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_38 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_38 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_114 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_115 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_116 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_155 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "170/170 [==============================] - 3s 12ms/step - loss: 0.7296 - accuracy: 0.5267 - val_loss: 0.7001 - val_accuracy: 0.5410\n",
      "Epoch 2/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6680 - accuracy: 0.5836 - val_loss: 0.6294 - val_accuracy: 0.6452\n",
      "Epoch 3/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6318 - accuracy: 0.6249 - val_loss: 0.5969 - val_accuracy: 0.6698\n",
      "Epoch 4/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6195 - accuracy: 0.6297 - val_loss: 0.5824 - val_accuracy: 0.6752\n",
      "Epoch 5/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.6076 - accuracy: 0.6487 - val_loss: 0.5917 - val_accuracy: 0.6850\n",
      "Epoch 6/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5869 - accuracy: 0.6721 - val_loss: 0.5730 - val_accuracy: 0.6709\n",
      "Epoch 7/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5735 - accuracy: 0.6811 - val_loss: 0.5506 - val_accuracy: 0.6948\n",
      "Epoch 8/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5653 - accuracy: 0.6821 - val_loss: 0.5470 - val_accuracy: 0.6951\n",
      "Epoch 9/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5542 - accuracy: 0.6971 - val_loss: 0.5508 - val_accuracy: 0.6890\n",
      "Epoch 10/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5536 - accuracy: 0.6971 - val_loss: 0.5297 - val_accuracy: 0.7071\n",
      "Epoch 11/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5401 - accuracy: 0.7024 - val_loss: 0.5367 - val_accuracy: 0.7016\n",
      "Epoch 12/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5448 - accuracy: 0.7002 - val_loss: 0.5296 - val_accuracy: 0.7103\n",
      "Epoch 13/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5396 - accuracy: 0.7019 - val_loss: 0.5343 - val_accuracy: 0.7241\n",
      "Epoch 14/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5348 - accuracy: 0.7103 - val_loss: 0.5334 - val_accuracy: 0.7002\n",
      "Epoch 15/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5301 - accuracy: 0.7069 - val_loss: 0.5310 - val_accuracy: 0.7139\n",
      "Epoch 16/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5213 - accuracy: 0.7193 - val_loss: 0.5529 - val_accuracy: 0.6951\n",
      "Epoch 17/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5193 - accuracy: 0.7080 - val_loss: 0.5385 - val_accuracy: 0.7009\n",
      "Epoch 18/30\n",
      "170/170 [==============================] - 2s 10ms/step - loss: 0.5200 - accuracy: 0.7160 - val_loss: 0.5248 - val_accuracy: 0.7157\n",
      "Epoch 19/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5115 - accuracy: 0.7250 - val_loss: 0.5283 - val_accuracy: 0.7107\n",
      "Epoch 20/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5173 - accuracy: 0.7140 - val_loss: 0.5181 - val_accuracy: 0.7150\n",
      "Epoch 21/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5105 - accuracy: 0.7236 - val_loss: 0.5265 - val_accuracy: 0.7280\n",
      "Epoch 22/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5076 - accuracy: 0.7245 - val_loss: 0.5244 - val_accuracy: 0.7143\n",
      "Epoch 23/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5091 - accuracy: 0.7334 - val_loss: 0.5234 - val_accuracy: 0.7233\n",
      "Epoch 24/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5038 - accuracy: 0.7310 - val_loss: 0.5255 - val_accuracy: 0.7168\n",
      "Epoch 25/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5042 - accuracy: 0.7225 - val_loss: 0.5338 - val_accuracy: 0.7049\n",
      "Epoch 26/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5010 - accuracy: 0.7365 - val_loss: 0.5243 - val_accuracy: 0.7197\n",
      "Epoch 27/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.4957 - accuracy: 0.7323 - val_loss: 0.5337 - val_accuracy: 0.7056\n",
      "Epoch 28/30\n",
      "170/170 [==============================] - 2s 10ms/step - loss: 0.4987 - accuracy: 0.7306 - val_loss: 0.5232 - val_accuracy: 0.7327\n",
      "Epoch 29/30\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4949 - accuracy: 0.7317 - val_loss: 0.5367 - val_accuracy: 0.7136\n",
      "Epoch 30/30\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.4927 - accuracy: 0.7341 - val_loss: 0.5338 - val_accuracy: 0.7226\n",
      "87/87 [==============================] - 0s 2ms/step\n",
      "0.7226039783001809\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_39 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_39 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_117 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_118 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_119 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "166/166 [==============================] - 3s 11ms/step - loss: 0.7272 - accuracy: 0.5364 - val_loss: 0.6934 - val_accuracy: 0.5374\n",
      "Epoch 2/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.6616 - accuracy: 0.6029 - val_loss: 0.6118 - val_accuracy: 0.6553\n",
      "Epoch 3/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.6278 - accuracy: 0.6290 - val_loss: 0.5930 - val_accuracy: 0.6655\n",
      "Epoch 4/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.6171 - accuracy: 0.6404 - val_loss: 0.5795 - val_accuracy: 0.6901\n",
      "Epoch 5/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.6056 - accuracy: 0.6552 - val_loss: 0.5821 - val_accuracy: 0.6915\n",
      "Epoch 6/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5875 - accuracy: 0.6763 - val_loss: 0.5617 - val_accuracy: 0.6890\n",
      "Epoch 7/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5701 - accuracy: 0.6875 - val_loss: 0.5532 - val_accuracy: 0.6951\n",
      "Epoch 8/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5597 - accuracy: 0.6847 - val_loss: 0.5444 - val_accuracy: 0.7027\n",
      "Epoch 9/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5576 - accuracy: 0.6934 - val_loss: 0.5440 - val_accuracy: 0.7110\n",
      "Epoch 10/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5556 - accuracy: 0.6917 - val_loss: 0.5341 - val_accuracy: 0.7009\n",
      "Epoch 11/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5448 - accuracy: 0.6973 - val_loss: 0.5361 - val_accuracy: 0.7114\n",
      "Epoch 12/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5440 - accuracy: 0.7011 - val_loss: 0.5332 - val_accuracy: 0.7121\n",
      "Epoch 13/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5346 - accuracy: 0.7069 - val_loss: 0.5351 - val_accuracy: 0.7078\n",
      "Epoch 14/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5276 - accuracy: 0.7089 - val_loss: 0.5385 - val_accuracy: 0.6908\n",
      "Epoch 15/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5336 - accuracy: 0.7044 - val_loss: 0.5315 - val_accuracy: 0.7060\n",
      "Epoch 16/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5197 - accuracy: 0.7196 - val_loss: 0.5321 - val_accuracy: 0.7078\n",
      "Epoch 17/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5180 - accuracy: 0.7188 - val_loss: 0.5239 - val_accuracy: 0.7136\n",
      "Epoch 18/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5153 - accuracy: 0.7165 - val_loss: 0.5318 - val_accuracy: 0.7060\n",
      "Epoch 19/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5122 - accuracy: 0.7217 - val_loss: 0.5266 - val_accuracy: 0.7190\n",
      "Epoch 20/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5124 - accuracy: 0.7256 - val_loss: 0.5213 - val_accuracy: 0.7121\n",
      "Epoch 21/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5018 - accuracy: 0.7301 - val_loss: 0.5245 - val_accuracy: 0.7280\n",
      "Epoch 22/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5062 - accuracy: 0.7182 - val_loss: 0.5180 - val_accuracy: 0.7313\n",
      "Epoch 23/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5075 - accuracy: 0.7247 - val_loss: 0.5197 - val_accuracy: 0.7233\n",
      "Epoch 24/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5022 - accuracy: 0.7278 - val_loss: 0.5229 - val_accuracy: 0.7208\n",
      "Epoch 25/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.4979 - accuracy: 0.7293 - val_loss: 0.5229 - val_accuracy: 0.7193\n",
      "Epoch 26/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.4969 - accuracy: 0.7354 - val_loss: 0.5249 - val_accuracy: 0.7125\n",
      "Epoch 27/30\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.4967 - accuracy: 0.7275 - val_loss: 0.5369 - val_accuracy: 0.7132\n",
      "Epoch 28/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.4893 - accuracy: 0.7301 - val_loss: 0.5189 - val_accuracy: 0.7277\n",
      "Epoch 29/30\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.4966 - accuracy: 0.7369 - val_loss: 0.5207 - val_accuracy: 0.7345\n",
      "Epoch 30/30\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.4849 - accuracy: 0.7416 - val_loss: 0.5411 - val_accuracy: 0.7215\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7215189873417721\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_40 (Flatten)        (None, 1170)              0         \n",
      "                                                                 \n",
      " batch_normalization_40 (Ba  (None, 1170)              4680      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 256)               299776    \n",
      "                                                                 \n",
      " dropout_120 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_121 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_122 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354122 (1.35 MB)\n",
      "Trainable params: 351782 (1.34 MB)\n",
      "Non-trainable params: 2340 (9.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 3s 13ms/step - loss: 0.7352 - accuracy: 0.5388 - val_loss: 0.6935 - val_accuracy: 0.5544\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.6678 - accuracy: 0.5894 - val_loss: 0.6289 - val_accuracy: 0.6448\n",
      "Epoch 3/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.6306 - accuracy: 0.6297 - val_loss: 0.6052 - val_accuracy: 0.6629\n",
      "Epoch 4/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.6171 - accuracy: 0.6379 - val_loss: 0.5883 - val_accuracy: 0.6684\n",
      "Epoch 5/30\n",
      "162/162 [==============================] - 2s 13ms/step - loss: 0.6083 - accuracy: 0.6486 - val_loss: 0.5808 - val_accuracy: 0.6897\n",
      "Epoch 6/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5894 - accuracy: 0.6614 - val_loss: 0.5587 - val_accuracy: 0.6948\n",
      "Epoch 7/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5787 - accuracy: 0.6822 - val_loss: 0.5636 - val_accuracy: 0.6919\n",
      "Epoch 8/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5693 - accuracy: 0.6856 - val_loss: 0.5495 - val_accuracy: 0.7027\n",
      "Epoch 9/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.5593 - accuracy: 0.6895 - val_loss: 0.5555 - val_accuracy: 0.6948\n",
      "Epoch 10/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5532 - accuracy: 0.6959 - val_loss: 0.5337 - val_accuracy: 0.7052\n",
      "Epoch 11/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5417 - accuracy: 0.7036 - val_loss: 0.5365 - val_accuracy: 0.7161\n",
      "Epoch 12/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5433 - accuracy: 0.7033 - val_loss: 0.5306 - val_accuracy: 0.7143\n",
      "Epoch 13/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5383 - accuracy: 0.7061 - val_loss: 0.5439 - val_accuracy: 0.7078\n",
      "Epoch 14/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5341 - accuracy: 0.7093 - val_loss: 0.5601 - val_accuracy: 0.6973\n",
      "Epoch 15/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.5305 - accuracy: 0.7101 - val_loss: 0.5348 - val_accuracy: 0.7063\n",
      "Epoch 16/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.5235 - accuracy: 0.7183 - val_loss: 0.5384 - val_accuracy: 0.7049\n",
      "Epoch 17/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5190 - accuracy: 0.7197 - val_loss: 0.5366 - val_accuracy: 0.7136\n",
      "Epoch 18/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.5203 - accuracy: 0.7117 - val_loss: 0.5210 - val_accuracy: 0.7074\n",
      "Epoch 19/30\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 0.5141 - accuracy: 0.7186 - val_loss: 0.5244 - val_accuracy: 0.7179\n",
      "Epoch 20/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5160 - accuracy: 0.7174 - val_loss: 0.5252 - val_accuracy: 0.7049\n",
      "Epoch 21/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5096 - accuracy: 0.7301 - val_loss: 0.5181 - val_accuracy: 0.7219\n",
      "Epoch 22/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5062 - accuracy: 0.7255 - val_loss: 0.5193 - val_accuracy: 0.7204\n",
      "Epoch 23/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5071 - accuracy: 0.7320 - val_loss: 0.5184 - val_accuracy: 0.7295\n",
      "Epoch 24/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.5043 - accuracy: 0.7272 - val_loss: 0.5199 - val_accuracy: 0.7230\n",
      "Epoch 25/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.5040 - accuracy: 0.7273 - val_loss: 0.5245 - val_accuracy: 0.7190\n",
      "Epoch 26/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.4977 - accuracy: 0.7307 - val_loss: 0.5179 - val_accuracy: 0.7186\n",
      "Epoch 27/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.4964 - accuracy: 0.7380 - val_loss: 0.5274 - val_accuracy: 0.7168\n",
      "Epoch 28/30\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.4874 - accuracy: 0.7328 - val_loss: 0.5143 - val_accuracy: 0.7295\n",
      "Epoch 29/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.4977 - accuracy: 0.7318 - val_loss: 0.5259 - val_accuracy: 0.7248\n",
      "Epoch 30/30\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.4906 - accuracy: 0.7363 - val_loss: 0.5322 - val_accuracy: 0.7089\n",
      "87/87 [==============================] - 0s 3ms/step\n",
      "0.7088607594936709\n"
     ]
    }
   ],
   "source": [
    "#TRAINING AND TESTING - FINDING THE PERFECT BATCH SIZE  \n",
    "acc = 0\n",
    "i=0\n",
    "batch = 1 \n",
    "test_accuracy_per_epoch2 = []\n",
    "train_loss_per_epoch2 = []\n",
    "\n",
    "while batch < 41: \n",
    "    \n",
    "    train_Y_one_hot = to_categorical(y_train)\n",
    "    test_Y_one_hot = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(input_size,13)))\n",
    "    model.add(keras.layers.BatchNormalization()) #doesnt work without - why?? \n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dropout(0.2)) #vary dropout see if gets better or worse\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, kernel_regularizer=keras.regularizers.l2(0.02))) #ridge - loss = l2 * reduce_sum(square(x))\n",
    "\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    model.build()\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    model_train = model.fit(x_train, train_Y_one_hot, batch_size = batch, epochs = 30 ,verbose=1, validation_data=(x_test, test_Y_one_hot))\n",
    "    predicted_classes = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(np.round(predicted_classes), axis=1)\n",
    "    acc = accuracy_score(y_test, predicted_classes)\n",
    "    test_accuracy_per_epoch2.append(acc)\n",
    "    train_loss_per_epoch2.append(model_train.history['loss'][0])\n",
    "    print(acc)\n",
    "    batch = batch + 1 \n",
    "    \n",
    "        \n",
    "#DO THE ACCURACY PLOT FOR FINDING BATCH SIZE \n",
    "plt.plot(range(1, batch), test_accuracy_per_epoch2)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy Plot for [0.5s : 2s] Time Window')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Batch size of the highest accuracy:\", test_accuracy_per_epoch2.index(max(test_accuracy_per_epoch2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT : Accuracies for the Increasing Time Window\n",
    "\n",
    "# Sample data for line 1\n",
    "#data_line1 = [\n",
    "#    (0.5, 0.718626),\n",
    "#    (1, 0.776130),\n",
    "#    (1.5, 0.774684),\n",
    "#    (2, 0.770344),\n",
    "#    (2.5, 0.764195),\n",
    " #   (3, 0.762749)\n",
    "#]\n",
    "\n",
    "#STARTING AT 0S \n",
    "data_line1 = [\n",
    "    (0.1, 0.577577),\n",
    "    (0.2, 0.581917),\n",
    "    (0.3, 0.589512),\n",
    "    (0.4, 0.634358),\n",
    "    (0.5, 0.770344),\n",
    "    (0.6, 0.793490),\n",
    "    (0.7, 0.794937),\n",
    "    (0.8, 0.784448),\n",
    "    (0.9, 0.794213),\n",
    "    (1.0, 0.790235),\n",
    "    (1.1, 0.777577),\n",
    "    (1.2, 0.799277),\n",
    "    (1.3, 0.796745),\n",
    "    (1.4, 0.783725),\n",
    "    (1.5, 0.784810),\n",
    "    (1.6, 0.787703),\n",
    "    (1.7, 0.786980),\n",
    "    (1.8, 0.786980),\n",
    "    (1.9, 0.788427),\n",
    "    (2.0, 0.792767),\n",
    "    (2.1, 0.781555),\n",
    "    (2.2, 0.789150),\n",
    "    (2.3, 0.781555),\n",
    "    (2.4, 0.789873),\n",
    "    (2.5, 0.771067),\n",
    "    (2.6, 0.780108),\n",
    "    (2.7, 0.779747),\n",
    "    (2.8, 0.781555),\n",
    "    (2.9, 0.784087),\n",
    "    (3.0, 0.773960)\n",
    "]\n",
    "\n",
    "#STARTING AT -1S \n",
    "\n",
    "data_line2 = [\n",
    "    (0.1, 0.574322),\n",
    "    (0.2, 0.577215),\n",
    "    (0.3, 0.580832),\n",
    "    (0.4, 0.620253),\n",
    "    (0.5, 0.748282),\n",
    "    (0.6, 0.769982),\n",
    "    (0.7, 0.793852),\n",
    "    (0.8, 0.791320),\n",
    "    (0.9, 0.779747),\n",
    "    (1.0, 0.785895),\n",
    "    (1.1, 0.779024),\n",
    "    (1.2, 0.793128),\n",
    "    (1.3, 0.773237),\n",
    "    (1.4, 0.786980),\n",
    "    (1.5, 0.793490),\n",
    "    (1.6, 0.785895),\n",
    "    (1.7, 0.785533),\n",
    "    (1.8, 0.779024),\n",
    "    (1.9, 0.786980),\n",
    "    (2.0, 0.785895),\n",
    "    (2.1, 0.788065),\n",
    "    (2.2, 0.780470),\n",
    "    (2.3, 0.786980),\n",
    "    (2.4, 0.787342),\n",
    "    (2.5, 0.791682),\n",
    "    (2.6, 0.781555),\n",
    "    (2.7, 0.785172),\n",
    "    (2.8, 0.780832),\n",
    "    (2.9, 0.789512),\n",
    "    (3.0, 0.788065)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sample data for line 2\n",
    "#data_line2 = [\n",
    "#    (0.5, 0.760579),\n",
    "#    (1, 0.790235),\n",
    "#    (1.5, 0.786618),\n",
    "#    (2, 0.786618),\n",
    "#    (2.5, 0.783725),\n",
    "#    (3, 0.790235)\n",
    "#]\n",
    "\n",
    "# Extracting data for line 1\n",
    "seconds_line1 = [entry[0] for entry in data_line1]\n",
    "accuracy1 = [entry[1] for entry in data_line1]\n",
    "\n",
    "# Extracting data for line 2\n",
    "seconds_line2 = [entry[0] for entry in data_line2]\n",
    "accuracy2 = [entry[1] for entry in data_line2]\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the first line with points and label\n",
    "plt.plot(seconds_line1, accuracy1, marker='o', linestyle='-', label='Starting at 0s')\n",
    "\n",
    "# Plot the second line with points and label\n",
    "plt.plot(seconds_line2, accuracy2, marker='o', linestyle='-', label='Starting at -1s')\n",
    "\n",
    "# Adding vertical lines at 0s and 0.5s with labels\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.text(0, plt.gca().get_ylim()[0] - 0.015, 'Loom Onset', color='red', ha='center', va='top', fontsize=10)\n",
    "plt.axvline(0.5, color='red', linestyle='--')\n",
    "plt.text(0.5, plt.gca().get_ylim()[0] - 0.015, 'Loom Offset', color='red', ha='center', va='top', fontsize=10)\n",
    "\n",
    "# Labeling axes\n",
    "plt.xlabel('Time (seconds)', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "# Setting ticks for x-axis\n",
    "plt.xticks(np.arange(-0.5, 3.1, 0.5))\n",
    "\n",
    "# Adding legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.title('Accuracies for Increasing Time Windows', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT : Accuracies for the Increasing Time Window, With and Without Freezing\n",
    "\n",
    "\n",
    "data_line1 = [\n",
    "    (-1, 0.5, 3741, 0.761693),\n",
    "    (-1, 1, 3153, 0.784884),\n",
    "    (-1, 1.5, 3087, 0.778737),\n",
    "    (-1, 2, 3054, 0.790507),\n",
    "    (-1, 2.5, 3038, 0.783873),\n",
    "    (-1, 3, 3022, 0.783903)\n",
    "]\n",
    "\n",
    "data_line2 = [\n",
    "    (0.5, 3741, 0.780846),\n",
    "    (1, 3153, 0.803911),\n",
    "    (1.5, 3087, 0.805181),\n",
    "    (2, 3054, 0.796508),\n",
    "    (2.5, 3038, 0.786615),\n",
    "    (3, 3022, 0.798787)\n",
    "]\n",
    "\n",
    "# Extracting data for line 1\n",
    "seconds_line1 = [entry[1] for entry in data_line1]\n",
    "accuracy1 = [entry[3] for entry in data_line1]\n",
    "middle_values1 = [int(entry[2]) for entry in data_line1]\n",
    "\n",
    "# Extracting data for line 2\n",
    "seconds_line2 = [entry[0] for entry in data_line2]\n",
    "accuracy2 = [entry[2] for entry in data_line2]\n",
    "middle_values2 = [entry[1] for entry in data_line2]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the first line with points and label\n",
    "plt.plot(seconds_line1, accuracy1, marker='o', linestyle='-', label='Including Freezing')\n",
    "\n",
    "# Plot the second line with points and label\n",
    "plt.plot(seconds_line2, accuracy2, marker='o', linestyle='-', label='Excluding Freezing')\n",
    "\n",
    "# Adding middle values above each data point for line 1\n",
    "for x, y, middle_value in zip(seconds_line1, accuracy1, middle_values1):\n",
    "    plt.text(x, y + 0.001, f'{middle_value}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Adding vertical lines at 0s and 0.5s with labels\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.text(0, plt.gca().get_ylim()[0] - 0.005, 'Loom Onset', color='red', ha='center', va='top', fontsize=10)\n",
    "plt.axvline(0.5, color='red', linestyle='--')\n",
    "plt.text(0.5, plt.gca().get_ylim()[0] - 0.005, 'Loom Offset', color='red', ha='center', va='top', fontsize=10)\n",
    "\n",
    "# Labeling axes\n",
    "plt.xlabel('Time (seconds)', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "# Setting ticks for x-axis\n",
    "plt.xticks(np.arange(-0.5, 3.1, 0.5))\n",
    "\n",
    "# Adding legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.title('Accuracies for Increasing Time Windows', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT : Accuracies for the Sliding Time Window\n",
    "\n",
    "data1 = {\n",
    "    'Time_Window': ['-60s:0s', '-54s:6s', '-48s:12s', '-42s:18s', '-36s:24s', '-30s:30s', '-24s:36s', '-18s:42s', '-12s:48s', '-6s:54s', \n",
    "                    '-0s:60s', '6s:66s', '12s:72s', '18s:78s', '24s:84s', '30s:90s', '36s:96s', '42s:102s', '48s:108s', '54s:114s',\n",
    "                    '60s:120s', '66s:126s', '72s:132s', '78s:138s', '84s:144s', '90s:150s', '96s:156s', '102s:162s', '108s:168s', '114s:174s', '120s:180s'],\n",
    "    'Accuracy': [0.560579, 0.580470, 0.587703, 0.604340, 0.645208, 0.764557, 0.770705, 0.787703, 0.782640, 0.786618, \n",
    "                 0.789873, 0.780108, 0.785895, 0.771429, 0.764195, 0.724412, 0.662568, 0.631465, 0.625316, 0.614105,\n",
    "                 0.608318, 0.596745, 0.593128, 0.596745, 0.588788, 0.580470, 0.575407, 0.573599, 0.566365, 0.563834, 0.568897]\n",
    "}\n",
    "\n",
    "data2 = {\n",
    "    'Time_Window': ['-30s:0s', '-24s:6s', '-18s:12s', '-12s:18s', '-6s:24s', '-0s:30s', '6s:36s', '12s:42s', '18s:48s', '24s:54s', \n",
    "                    '30s:60s', '36s:66s', '42s:72s', '48s:78s', '54s:84s', '60s:90s', '66s:96s', '72s:102s', '78s:108s', '84s:114s',\n",
    "                    '90s:120s', '96s:126s', '102s:132s', '108s:138s', '114s:144s', '120s:150s', '126s:156s', '132s:162s', '138s:168s', '144s:174s', '150s:180s'],\n",
    "    'Accuracy': [0.564195, 0.576130, 0.590958, 0.598915, 0.630018, 0.764195, 0.774684, 0.783363, 0.770705, 0.754430, \n",
    "                 0.733454, 0.664376, 0.625678, 0.618445, 0.622061, 0.603617, 0.600000, 0.600362, 0.596022, 0.590958,\n",
    "                 0.581555, 0.579385, 0.568897, 0.576130, 0.560940, 0.568174, 0.562749, 0.561302, 0.556600, 0.554069, 0.558770]\n",
    "}\n",
    "\n",
    "# data3 = {\n",
    "#     'Time_Window': [\n",
    "#         '-15s:0s', '-12s:3s', '-9s:6s', '-6s:9s', '-3s:12s', '-0s:15s', '3s:18s', '6s:21s', '9s:24s', '12s:27s',\n",
    "#         '15s:30s', '18s:33s', '21s:36s', '24s:39s', '27s:42s', '30s:45s', '33s:48s', '36s:51s', '39s:54s', '42s:57s',\n",
    "#         '45s:60s', '48s:63s', '51s:66s', '54s:69s', '57s:72s', '60s:75s', '63s:78s', '66s:81s', '69s:84s', '72s:87s',\n",
    "#         '75s:90s', '78s:93s', '81s:96s', '84s:99s', '87s:102s', '90s:105s', '93s:108s', '96s:111s', '99s:114s', '102s:117s',\n",
    "#         '105s:120s', '108s:123s', '111s:126s', '114s:129s', '117s:132s', '120s:135s', '123s:138s', '126s:141s', '129s:144s', '132s:147s',\n",
    "#         '135s:150s', '138s:153s', '141s:156s', '144s:159s', '147s:162s', '150s:165s', '153s:168s', '156s:171s', '159s:174s', '162s:177s',\n",
    "#         '165s:180s'\n",
    "#     ],\n",
    "#     'Accuracy': [\n",
    "#         0.546835, 0.557324, 0.571790, 0.580108, 0.583363, 0.585895, 0.575769, 0.573237, 0.599638, 0.671971,\n",
    "#         0.749367, 0.761664, 0.769259, 0.750452, 0.751899, 0.721519, 0.677758, 0.652441, 0.619530, 0.612658,\n",
    "#         0.606510, 0.612658, 0.603255, 0.601808, 0.604340, 0.603978, 0.592767, 0.590235, 0.578662, 0.594213,\n",
    "#         0.588788, 0.581555, 0.582278, 0.573599, 0.573599, 0.569982, 0.569259, 0.576130, 0.564557, 0.567812,\n",
    "#         0.567450, 0.564919, 0.565642, 0.557324, 0.566004, 0.555877, 0.563834, 0.563834, 0.567089, 0.556962,\n",
    "#         0.555515, 0.549729, 0.561302, 0.556962, 0.555877, 0.564195, 0.547559, 0.551537, 0.541410, 0.547920,\n",
    "#         0.545027\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "data3 = {\n",
    "    'Time_Window': [\n",
    "        '-15s:0s',  '-9s:6s', '-3s:12s', '3s:18s',  '9s:24s', \n",
    "        '15s:30s',  '21s:36s',  '27s:42s',  '33s:48s',  '39s:54s', \n",
    "        '45s:60s',  '51s:66s',  '57s:72s',  '63s:78s',  '69s:84s', \n",
    "        '75s:90s',  '81s:96s',  '87s:102s',  '93s:108s',  '99s:114s', \n",
    "        '105s:120s',  '111s:126s', '117s:132s', '123s:138s',  '129s:144s', \n",
    "        '135s:150s', '141s:156s',  '147s:162s',  '153s:168s', '159s:174s', \n",
    "        '165s:180s'\n",
    "    ],\n",
    "    \n",
    "    'Accuracy': [\n",
    "         0.546835, 0.571790,  0.583363,  0.575769, 0.599638, \n",
    "         0.749367,  0.769259, 0.751899, 0.677758,  0.619530, \n",
    "         0.606510,  0.603255,  0.604340, 0.592767,  0.578662, \n",
    "         0.588788,  0.582278,  0.573599, 0.569259,  0.564557, \n",
    "        0.567450,  0.565642,  0.566004, 0.563834,  0.567089, \n",
    "        0.555515,  0.561302,  0.555877, 0.547559, 0.541410, \n",
    "        0.545027\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "# Convert time window from frames to seconds\n",
    "def convert_to_seconds(window):\n",
    "    start_str, end_str = window[:-1].split(':')\n",
    "    start_sec = int(start_str.split('s')[0]) if start_str else 0\n",
    "    end_sec = int(end_str.split('s')[0]) if end_str else 0\n",
    "    return ((start_sec + end_sec) / 2) / 60\n",
    "\n",
    "# Apply the function to both DataFrames\n",
    "df1['Time_Seconds'] = df1['Time_Window'].apply(convert_to_seconds)\n",
    "df2['Time_Seconds'] = df2['Time_Window'].apply(convert_to_seconds)\n",
    "df3['Time_Seconds'] = df3['Time_Window'].apply(convert_to_seconds)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the data for line 1\n",
    "plt.plot(df1['Time_Seconds'], df1['Accuracy'], marker='o', linestyle='-', label='1s Time Window')\n",
    "\n",
    "# Plot the data for line 2\n",
    "plt.plot(df2['Time_Seconds'], df2['Accuracy'], marker='o', linestyle='-', label='0.5s Time Window')\n",
    "\n",
    "# Plot the data for line 3\n",
    "plt.plot(df3['Time_Seconds'], df3['Accuracy'], marker='o', linestyle='-', label='0.25s Time Window')\n",
    "\n",
    "\n",
    "# Adding vertical lines at 0s and 0.5s with labels\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.text(0, plt.gca().get_ylim()[0] - 0.015, 'Loom Onset', color='red', ha='center', va='top', fontsize=12)\n",
    "plt.axvline(0.5, color='red', linestyle='--')\n",
    "plt.text(0.5, plt.gca().get_ylim()[0] - 0.015, 'Loom Offset', color='red', ha='center', va='top', fontsize=12)\n",
    "\n",
    "# Labeling axes\n",
    "plt.xlabel('Time (seconds)', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "# Adding legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.title('Accuracies for Sliding Time Windows', fontsize=16)\n",
    "plt.xlim(-0.6, 3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
